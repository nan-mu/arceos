diff --git a/Cargo.lock b/Cargo.lock
index 44fd07c..b4729f6 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -31,6 +31,7 @@ dependencies = [
  "rand",
  "rlsf",
  "slab_allocator",
+ "talc",
 ]
 
 [[package]]
@@ -471,6 +472,7 @@ dependencies = [
  "axerrno",
  "axfeat",
  "axio",
+ "hash32 0.3.1",
  "spinlock",
 ]
 
@@ -588,6 +590,12 @@ version = "3.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "703642b98a00b3b90513279a8ede3fcfa479c126c5fb46e78f3051522f021403"
 
+[[package]]
+name = "buddy-alloc"
+version = "0.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1f0d2da64a6a895d5a7e0724882825d50f83c13396b1b9f1878e19a024bab395"
+
 [[package]]
 name = "buddy_system_allocator"
 version = "0.9.0"
@@ -866,6 +874,17 @@ dependencies = [
  "thiserror",
 ]
 
+[[package]]
+name = "dlmalloc"
+version = "0.2.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "960a02b0caee913a3df97cd43fcc7370d0695c1dfcd9aca2af3bb9e96c0acd80"
+dependencies = [
+ "cfg-if",
+ "libc",
+ "windows-sys 0.52.0",
+]
+
 [[package]]
 name = "driver_block"
 version = "0.1.0"
@@ -974,7 +993,7 @@ checksum = "6b30f669a7961ef1631673d2766cc92f52d64f7ef354d4fe0ddfd30ed52f0f4f"
 dependencies = [
  "errno-dragonfly",
  "libc",
- "windows-sys",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
@@ -987,6 +1006,15 @@ dependencies = [
  "libc",
 ]
 
+[[package]]
+name = "fastrand"
+version = "1.9.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e51093e27b0797c359783294ca4f0a911c270184cb10f85783b118614a1501be"
+dependencies = [
+ "instant",
+]
+
 [[package]]
 name = "fatfs"
 version = "0.4.0"
@@ -1029,6 +1057,16 @@ version = "0.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d2fabcfbdc87f4758337ca535fb41a6d701b65693ce38287d856d1674551ec9b"
 
+[[package]]
+name = "good_memory_allocator"
+version = "0.1.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b1651659e016ea4259760966432aebcc96c81e26743fb018c59585ddd677127e"
+dependencies = [
+ "either",
+ "spin 0.9.8",
+]
+
 [[package]]
 name = "half"
 version = "1.8.2"
@@ -1048,6 +1086,13 @@ dependencies = [
  "byteorder",
 ]
 
+[[package]]
+name = "hash32"
+version = "0.3.1"
+dependencies = [
+ "byteorder",
+]
+
 [[package]]
 name = "hashbrown"
 version = "0.14.0"
@@ -1061,7 +1106,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "db04bc24a18b9ea980628ecf00e6c0264f3c1426dac36c00cb49b6fbad8b0743"
 dependencies = [
  "atomic-polyfill",
- "hash32",
+ "hash32 0.2.1",
  "rustc_version",
  "spin 0.9.8",
  "stable_deref_trait",
@@ -1106,6 +1151,15 @@ dependencies = [
  "hashbrown",
 ]
 
+[[package]]
+name = "instant"
+version = "0.1.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7a5bbe824c507c5da5956355e86a746d82e0e1464f65d862cc5e71da70e94b2c"
+dependencies = [
+ "cfg-if",
+]
+
 [[package]]
 name = "is-terminal"
 version = "0.4.9"
@@ -1114,7 +1168,7 @@ checksum = "cb0889898416213fab133e1d33a0e5858a48177452750691bde3666d0fdbaf8b"
 dependencies = [
  "hermit-abi",
  "rustix",
- "windows-sys",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
@@ -1200,6 +1254,15 @@ dependencies = [
 name = "linked_list"
 version = "0.1.0"
 
+[[package]]
+name = "linked_list_allocator"
+version = "0.10.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9afa463f5405ee81cdb9cc2baf37e08ec7e4c8209442b5d72c04cfb2cd6e6286"
+dependencies = [
+ "spinning_top",
+]
+
 [[package]]
 name = "linux-raw-sys"
 version = "0.4.5"
@@ -1603,7 +1666,7 @@ dependencies = [
  "errno",
  "libc",
  "linux-raw-sys",
- "windows-sys",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
@@ -1745,6 +1808,15 @@ dependencies = [
  "kernel_guard",
 ]
 
+[[package]]
+name = "spinning_top"
+version = "0.2.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5b9eb1a2f4c41445a3a0ff9abc5221c5fcd28e1f13cd7c0397706f9ac938ddb0"
+dependencies = [
+ "lock_api",
+]
+
 [[package]]
 name = "stable_deref_trait"
 version = "1.2.0"
@@ -1792,6 +1864,19 @@ dependencies = [
  "unicode-ident",
 ]
 
+[[package]]
+name = "talc"
+version = "4.2.0"
+dependencies = [
+ "buddy-alloc",
+ "dlmalloc",
+ "fastrand",
+ "good_memory_allocator",
+ "linked_list_allocator",
+ "lock_api",
+ "spin 0.9.8",
+]
+
 [[package]]
 name = "thiserror"
 version = "1.0.47"
@@ -2055,7 +2140,7 @@ version = "0.48.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e686886bc078bc1b0b600cac0147aadb815089b6e4da64016cbd754b6342700f"
 dependencies = [
- "windows-targets",
+ "windows-targets 0.48.5",
 ]
 
 [[package]]
@@ -2064,7 +2149,16 @@ version = "0.48.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
 dependencies = [
- "windows-targets",
+ "windows-targets 0.48.5",
+]
+
+[[package]]
+name = "windows-sys"
+version = "0.52.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
+dependencies = [
+ "windows-targets 0.52.4",
 ]
 
 [[package]]
@@ -2073,13 +2167,28 @@ version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c"
 dependencies = [
- "windows_aarch64_gnullvm",
- "windows_aarch64_msvc",
- "windows_i686_gnu",
- "windows_i686_msvc",
- "windows_x86_64_gnu",
- "windows_x86_64_gnullvm",
- "windows_x86_64_msvc",
+ "windows_aarch64_gnullvm 0.48.5",
+ "windows_aarch64_msvc 0.48.5",
+ "windows_i686_gnu 0.48.5",
+ "windows_i686_msvc 0.48.5",
+ "windows_x86_64_gnu 0.48.5",
+ "windows_x86_64_gnullvm 0.48.5",
+ "windows_x86_64_msvc 0.48.5",
+]
+
+[[package]]
+name = "windows-targets"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7dd37b7e5ab9018759f893a1952c9420d060016fc19a472b4bb20d1bdd694d1b"
+dependencies = [
+ "windows_aarch64_gnullvm 0.52.4",
+ "windows_aarch64_msvc 0.52.4",
+ "windows_i686_gnu 0.52.4",
+ "windows_i686_msvc 0.52.4",
+ "windows_x86_64_gnu 0.52.4",
+ "windows_x86_64_gnullvm 0.52.4",
+ "windows_x86_64_msvc 0.52.4",
 ]
 
 [[package]]
@@ -2088,42 +2197,84 @@ version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8"
 
+[[package]]
+name = "windows_aarch64_gnullvm"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bcf46cf4c365c6f2d1cc93ce535f2c8b244591df96ceee75d8e83deb70a9cac9"
+
 [[package]]
 name = "windows_aarch64_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc"
 
+[[package]]
+name = "windows_aarch64_msvc"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "da9f259dd3bcf6990b55bffd094c4f7235817ba4ceebde8e6d11cd0c5633b675"
+
 [[package]]
 name = "windows_i686_gnu"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e"
 
+[[package]]
+name = "windows_i686_gnu"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b474d8268f99e0995f25b9f095bc7434632601028cf86590aea5c8a5cb7801d3"
+
 [[package]]
 name = "windows_i686_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406"
 
+[[package]]
+name = "windows_i686_msvc"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1515e9a29e5bed743cb4415a9ecf5dfca648ce85ee42e15873c3cd8610ff8e02"
+
 [[package]]
 name = "windows_x86_64_gnu"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e"
 
+[[package]]
+name = "windows_x86_64_gnu"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5eee091590e89cc02ad514ffe3ead9eb6b660aedca2183455434b93546371a03"
+
 [[package]]
 name = "windows_x86_64_gnullvm"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc"
 
+[[package]]
+name = "windows_x86_64_gnullvm"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "77ca79f2451b49fa9e2af39f0747fe999fcda4f5e241b2898624dca97a1f2177"
+
 [[package]]
 name = "windows_x86_64_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"
 
+[[package]]
+name = "windows_x86_64_msvc"
+version = "0.52.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "32b752e52a2da0ddfbdbcc6fceadfeede4c939ed16d13e648833a61dfb611ed8"
+
 [[package]]
 name = "winnow"
 version = "0.5.14"
diff --git a/api/axfeat/Cargo.toml b/api/axfeat/Cargo.toml
index 2947a7e..eda3a99 100644
--- a/api/axfeat/Cargo.toml
+++ b/api/axfeat/Cargo.toml
@@ -22,6 +22,7 @@ fp_simd = ["axhal/fp_simd"]
 irq = ["axhal/irq", "axruntime/irq", "axtask?/irq"]
 
 # Memory
+alloc-new = ["axalloc/new"]
 alloc = ["axalloc", "axruntime/alloc"]
 alloc-tlsf = ["axalloc/tlsf"]
 alloc-slab = ["axalloc/slab"]
@@ -30,20 +31,37 @@ paging = ["alloc", "axhal/paging", "axruntime/paging"]
 tls = ["alloc", "axhal/tls", "axruntime/tls", "axtask?/tls"]
 
 # Multi-threading and scheduler
-multitask = ["alloc", "axtask/multitask", "axsync/multitask", "axruntime/multitask"]
+multitask = [
+    "alloc",
+    "axtask/multitask",
+    "axsync/multitask",
+    "axruntime/multitask",
+]
 sched_fifo = ["axtask/sched_fifo"]
 sched_rr = ["axtask/sched_rr", "irq"]
 sched_cfs = ["axtask/sched_cfs", "irq"]
 
 # File system
-fs = ["alloc", "paging", "axdriver/virtio-blk", "dep:axfs", "axruntime/fs"] # TODO: try to remove "paging"
+fs = [
+    "alloc",
+    "paging",
+    "axdriver/virtio-blk",
+    "dep:axfs",
+    "axruntime/fs",
+] # TODO: try to remove "paging"
 myfs = ["axfs?/myfs"]
 
 # Networking
 net = ["alloc", "paging", "axdriver/virtio-net", "dep:axnet", "axruntime/net"]
 
 # Display
-display = ["alloc", "paging", "axdriver/virtio-gpu", "dep:axdisplay", "axruntime/display"]
+display = [
+    "alloc",
+    "paging",
+    "axdriver/virtio-gpu",
+    "dep:axdisplay",
+    "axruntime/display",
+]
 
 # Device drivers
 bus-mmio = ["axdriver?/bus-mmio"]
diff --git a/apps/memtest/src/main.rs b/apps/memtest/src/main.rs
index e23e95e..19ede20 100644
--- a/apps/memtest/src/main.rs
+++ b/apps/memtest/src/main.rs
@@ -6,7 +6,7 @@
 extern crate axstd as std;
 
 use rand::{rngs::SmallRng, RngCore, SeedableRng};
-use std::collections::BTreeMap;
+use std::collections::HashMap;
 use std::vec::Vec;
 
 fn test_vec(rng: &mut impl RngCore) {
@@ -22,20 +22,21 @@ fn test_vec(rng: &mut impl RngCore) {
     println!("test_vec() OK!");
 }
 
-fn test_btree_map(rng: &mut impl RngCore) {
+fn test_hashmap(rng: &mut impl RngCore) {
     const N: usize = 50_000;
-    let mut m = BTreeMap::new();
+    let mut m = HashMap::new();
     for _ in 0..N {
         let value = rng.next_u32();
         let key = format!("key_{value}");
         m.insert(key, value);
     }
-    for (k, v) in m.iter() {
+    let a = m.iter();
+    for (k, v) in a {
         if let Some(k) = k.strip_prefix("key_") {
             assert_eq!(k.parse::<u32>().unwrap(), *v);
         }
     }
-    println!("test_btree_map() OK!");
+    println!("test_hashmap_map() OK!");
 }
 
 #[cfg_attr(feature = "axstd", no_mangle)]
@@ -44,7 +45,7 @@ fn main() {
 
     let mut rng = SmallRng::seed_from_u64(0xdead_beef);
     test_vec(&mut rng);
-    test_btree_map(&mut rng);
+    test_hashmap(&mut rng);
 
     println!("Memory tests run OK!");
 }
diff --git a/crates/allocator/Cargo.toml b/crates/allocator/Cargo.toml
index b8770f1..e69bdaa 100644
--- a/crates/allocator/Cargo.toml
+++ b/crates/allocator/Cargo.toml
@@ -11,10 +11,10 @@ documentation = "https://rcore-os.github.io/arceos/allocator/index.html"
 
 [features]
 default = []
-full = ["bitmap", "tlsf", "slab", "buddy", "allocator_api"]
+full = ["bitmap", "tlsf", "slab", "buddy", "allocator_api", "new"]
 
 bitmap = ["dep:bitmap-allocator"]
-
+new = []
 tlsf = ["dep:rlsf"]
 slab = ["dep:slab_allocator"]
 buddy = ["dep:buddy_system_allocator"]
@@ -26,6 +26,7 @@ buddy_system_allocator = { version = "0.9", default-features = false, optional =
 slab_allocator = { path = "../slab_allocator", optional = true }
 rlsf = { version = "0.2", optional = true }
 bitmap-allocator = { git = "https://github.com/rcore-os/bitmap-allocator.git", rev = "88e871a", optional = true }
+talc = { path = "../talc" }
 
 [dev-dependencies]
 allocator = { path = ".", features = ["full"] }
diff --git a/crates/allocator/bench_output.txt b/crates/allocator/bench_output.txt
new file mode 100644
index 0000000..6a07f60
--- /dev/null
+++ b/crates/allocator/bench_output.txt
@@ -0,0 +1,109 @@
+
+running 0 tests
+
+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s
+
+system/vec_push_3M      time:   [2.3521 ms 2.3641 ms 2.3780 ms]
+                        change: [-3.7892% -1.7619% +0.0102%] (p = 0.08 > 0.05)
+                        No change in performance detected.
+Found 7 outliers among 100 measurements (7.00%)
+  3 (3.00%) high mild
+  4 (4.00%) high severe
+system/vec_rand_free_25K_64
+                        time:   [2.2996 ms 2.3099 ms 2.3248 ms]
+                        change: [+0.2389% +1.5141% +3.1727%] (p = 0.04 < 0.05)
+                        Change within noise threshold.
+Found 1 outliers among 10 measurements (10.00%)
+  1 (10.00%) high severe
+system/vec_rand_free_7500_520
+                        time:   [617.51 µs 621.17 µs 627.83 µs]
+                        change: [-1.5538% -0.6460% +0.4201%] (p = 0.27 > 0.05)
+                        No change in performance detected.
+Found 1 outliers among 10 measurements (10.00%)
+  1 (10.00%) high severe
+system/btree_map_50K    time:   [11.722 ms 11.764 ms 11.837 ms]
+                        change: [-0.9239% -0.2975% +0.3860%] (p = 0.41 > 0.05)
+                        No change in performance detected.
+Found 2 outliers among 10 measurements (20.00%)
+  2 (20.00%) high mild
+
+tlsf/vec_push_3M        time:   [3.2706 ms 3.3080 ms 3.3476 ms]
+                        change: [+1.9600% +3.2667% +4.6309%] (p = 0.00 < 0.05)
+                        Performance has regressed.
+Found 20 outliers among 100 measurements (20.00%)
+  3 (3.00%) high mild
+  17 (17.00%) high severe
+tlsf/vec_rand_free_25K_64
+                        time:   [1.6600 ms 1.6675 ms 1.6757 ms]
+                        change: [-1.8418% -1.0942% -0.3209%] (p = 0.01 < 0.05)
+                        Change within noise threshold.
+Found 1 outliers among 10 measurements (10.00%)
+  1 (10.00%) high severe
+tlsf/vec_rand_free_7500_520
+                        time:   [414.01 µs 416.40 µs 420.71 µs]
+                        change: [-1.2040% +0.0606% +1.4288%] (p = 0.93 > 0.05)
+                        No change in performance detected.
+tlsf/btree_map_50K      time:   [11.279 ms 11.672 ms 12.431 ms]
+                        change: [-7.0268% -0.4045% +6.1727%] (p = 0.92 > 0.05)
+                        No change in performance detected.
+Found 2 outliers among 10 measurements (20.00%)
+  2 (20.00%) high severe
+
+slab/vec_push_3M        time:   [3.1398 ms 3.1518 ms 3.1661 ms]
+                        change: [-0.6928% -0.2456% +0.3460%] (p = 0.32 > 0.05)
+                        No change in performance detected.
+Found 5 outliers among 100 measurements (5.00%)
+  3 (3.00%) high mild
+  2 (2.00%) high severe
+slab/vec_rand_free_25K_64
+                        time:   [1.4788 ms 1.4884 ms 1.4975 ms]
+                        change: [-4.3748% -0.5909% +3.8227%] (p = 0.82 > 0.05)
+                        No change in performance detected.
+Found 1 outliers among 10 measurements (10.00%)
+  1 (10.00%) high severe
+slab/vec_rand_free_7500_520
+                        time:   [445.87 ms 454.79 ms 463.46 ms]
+                        change: [+75.176% +78.900% +82.654%] (p = 0.00 < 0.05)
+                        Performance has regressed.
+slab/btree_map_50K      time:   [10.562 ms 10.610 ms 10.693 ms]
+                        change: [-5.0589% -3.3530% -1.6638%] (p = 0.00 < 0.05)
+                        Performance has improved.
+
+buddy/vec_push_3M       time:   [3.1714 ms 3.1823 ms 3.1959 ms]
+                        change: [-0.6104% -0.0962% +0.3926%] (p = 0.73 > 0.05)
+                        No change in performance detected.
+Found 4 outliers among 100 measurements (4.00%)
+  2 (2.00%) high mild
+  2 (2.00%) high severe
+buddy/vec_rand_free_25K_64
+                        time:   [2.3548 s 2.3853 s 2.4180 s]
+                        change: [+1.1895% +2.5313% +4.0110%] (p = 0.00 < 0.05)
+                        Performance has regressed.
+buddy/vec_rand_free_7500_520
+                        time:   [417.35 ms 424.26 ms 432.18 ms]
+                        change: [+52.528% +55.466% +58.450%] (p = 0.00 < 0.05)
+                        Performance has regressed.
+buddy/btree_map_50K     time:   [1.0203 s 1.0208 s 1.0213 s]
+                        change: [-0.1134% +0.0527% +0.2227%] (p = 0.57 > 0.05)
+                        No change in performance detected.
+
+new/vec_push_3M         time:   [3.1803 ms 3.1952 ms 3.2135 ms]
+                        change: [+0.3456% +0.9114% +1.5138%] (p = 0.00 < 0.05)
+                        Change within noise threshold.
+Found 4 outliers among 100 measurements (4.00%)
+  1 (1.00%) high mild
+  3 (3.00%) high severe
+new/vec_rand_free_25K_64
+                        time:   [2.3201 s 2.3341 s 2.3481 s]
+                        change: [-0.2990% +0.5403% +1.3805%] (p = 0.23 > 0.05)
+                        No change in performance detected.
+new/vec_rand_free_7500_520
+                        time:   [413.29 ms 422.11 ms 433.04 ms]
+                        change: [+51.027% +54.317% +58.300%] (p = 0.00 < 0.05)
+                        Performance has regressed.
+new/btree_map_50K       time:   [1.0285 s 1.0372 s 1.0510 s]
+                        change: [+0.9192% +1.8185% +3.1460%] (p = 0.01 < 0.05)
+                        Change within noise threshold.
+Found 1 outliers among 10 measurements (10.00%)
+  1 (10.00%) high severe
+
diff --git a/crates/allocator/benches/collections.rs b/crates/allocator/benches/collections.rs
index c31fd84..84edbe6 100644
--- a/crates/allocator/benches/collections.rs
+++ b/crates/allocator/benches/collections.rs
@@ -95,6 +95,11 @@ fn criterion_benchmark(c: &mut Criterion) {
         "buddy",
         AllocatorRc::new(BuddyByteAllocator::new(), pool.as_slice()),
     );
+    bench(
+        c,
+        "new",
+        AllocatorRc::new(BuddyByteAllocator::new(), pool.as_slice()),
+    );
 }
 
 criterion_group!(benches, criterion_benchmark);
diff --git a/crates/allocator/src/lib.rs b/crates/allocator/src/lib.rs
index 7a79161..a732835 100644
--- a/crates/allocator/src/lib.rs
+++ b/crates/allocator/src/lib.rs
@@ -31,6 +31,11 @@ mod tlsf;
 #[cfg(feature = "tlsf")]
 pub use tlsf::TlsfByteAllocator;
 
+// #[cfg(feature = "new")]
+mod new;
+// #[cfg(feature = "new")]
+pub use new::YourNewAllocator;
+
 use core::alloc::Layout;
 use core::ptr::NonNull;
 
diff --git a/crates/allocator/src/new.rs b/crates/allocator/src/new.rs
new file mode 100644
index 0000000..82ac0a6
--- /dev/null
+++ b/crates/allocator/src/new.rs
@@ -0,0 +1,61 @@
+use crate::{AllocError, AllocResult, BaseAllocator, ByteAllocator};
+use core::{alloc::Layout, ptr::NonNull};
+use talc::{ErrOnOom, Span, Talc};
+
+pub struct YourNewAllocator {
+    talc: Talc<ErrOnOom>,
+    heap: Span,
+}
+
+impl YourNewAllocator {
+    pub const fn new() -> Self {
+        Self {
+            talc: Talc::new(ErrOnOom),
+            heap: Span::empty(),
+        }
+    }
+}
+
+impl BaseAllocator for YourNewAllocator {
+    fn init(&mut self, start: usize, size: usize) {
+        self.heap = unsafe {
+            self.talc
+                .claim(Span::new(start as *mut u8, (start + size) as *mut u8))
+                .unwrap()
+        };
+    }
+    fn add_memory(&mut self, start: usize, size: usize) -> AllocResult {
+        assert!(!self.heap.is_empty());
+        self.heap = unsafe {
+            self.talc.extend(
+                self.heap,
+                Span::new(start as *mut u8, (start + size) as *mut u8),
+            )
+        };
+        Ok(())
+    }
+}
+
+impl ByteAllocator for YourNewAllocator {
+    fn alloc(&mut self, layout: Layout) -> AllocResult<NonNull<u8>> {
+        assert!(layout.size() > 0);
+        unsafe { self.talc.malloc(layout).map_err(|_| AllocError::NoMemory) }
+    }
+
+    fn dealloc(&mut self, pos: NonNull<u8>, layout: Layout) {
+        unsafe { self.talc.free(pos, layout) };
+        //这里有歧义，talc实现了一个free函数用于释放，实现了shrink用于收缩。看起来两者都实现了dealloc。不知道用哪个
+    }
+
+    fn total_bytes(&self) -> usize {
+        self.heap.size()
+    }
+
+    fn used_bytes(&self) -> usize {
+        unsafe { self.talc.get_allocated_span(self.heap).size() }
+    }
+
+    fn available_bytes(&self) -> usize {
+        self.heap.size() - unsafe { self.talc.get_allocated_span(self.heap).size() }
+    }
+}
diff --git a/crates/hash32/Cargo.toml b/crates/hash32/Cargo.toml
new file mode 100644
index 0000000..b05df19
--- /dev/null
+++ b/crates/hash32/Cargo.toml
@@ -0,0 +1,13 @@
+[package]
+authors = ["Jorge Aparicio <jorge@japaric.io>"]
+categories = ["no-std"]
+description = "32-bit hashing algorithms"
+keywords = ["32-bit", "hash", "fnv", "murmur3"]
+license = "MIT OR Apache-2.0"
+name = "hash32"
+repository = "https://github.com/japaric/hash32"
+version = "0.3.1"
+
+[dependencies.byteorder]
+default-features = false
+version = "1.2.2"
diff --git a/crates/hash32/LICENSE-APACHE b/crates/hash32/LICENSE-APACHE
new file mode 100644
index 0000000..f47c941
--- /dev/null
+++ b/crates/hash32/LICENSE-APACHE
@@ -0,0 +1,201 @@
+                              Apache License
+                        Version 2.0, January 2004
+                     http://www.apache.org/licenses/
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+   "License" shall mean the terms and conditions for use, reproduction,
+   and distribution as defined by Sections 1 through 9 of this document.
+
+   "Licensor" shall mean the copyright owner or entity authorized by
+   the copyright owner that is granting the License.
+
+   "Legal Entity" shall mean the union of the acting entity and all
+   other entities that control, are controlled by, or are under common
+   control with that entity. For the purposes of this definition,
+   "control" means (i) the power, direct or indirect, to cause the
+   direction or management of such entity, whether by contract or
+   otherwise, or (ii) ownership of fifty percent (50%) or more of the
+   outstanding shares, or (iii) beneficial ownership of such entity.
+
+   "You" (or "Your") shall mean an individual or Legal Entity
+   exercising permissions granted by this License.
+
+   "Source" form shall mean the preferred form for making modifications,
+   including but not limited to software source code, documentation
+   source, and configuration files.
+
+   "Object" form shall mean any form resulting from mechanical
+   transformation or translation of a Source form, including but
+   not limited to compiled object code, generated documentation,
+   and conversions to other media types.
+
+   "Work" shall mean the work of authorship, whether in Source or
+   Object form, made available under the License, as indicated by a
+   copyright notice that is included in or attached to the work
+   (an example is provided in the Appendix below).
+
+   "Derivative Works" shall mean any work, whether in Source or Object
+   form, that is based on (or derived from) the Work and for which the
+   editorial revisions, annotations, elaborations, or other modifications
+   represent, as a whole, an original work of authorship. For the purposes
+   of this License, Derivative Works shall not include works that remain
+   separable from, or merely link (or bind by name) to the interfaces of,
+   the Work and Derivative Works thereof.
+
+   "Contribution" shall mean any work of authorship, including
+   the original version of the Work and any modifications or additions
+   to that Work or Derivative Works thereof, that is intentionally
+   submitted to Licensor for inclusion in the Work by the copyright owner
+   or by an individual or Legal Entity authorized to submit on behalf of
+   the copyright owner. For the purposes of this definition, "submitted"
+   means any form of electronic, verbal, or written communication sent
+   to the Licensor or its representatives, including but not limited to
+   communication on electronic mailing lists, source code control systems,
+   and issue tracking systems that are managed by, or on behalf of, the
+   Licensor for the purpose of discussing and improving the Work, but
+   excluding communication that is conspicuously marked or otherwise
+   designated in writing by the copyright owner as "Not a Contribution."
+
+   "Contributor" shall mean Licensor and any individual or Legal Entity
+   on behalf of whom a Contribution has been received by Licensor and
+   subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   copyright license to reproduce, prepare Derivative Works of,
+   publicly display, publicly perform, sublicense, and distribute the
+   Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   (except as stated in this section) patent license to make, have made,
+   use, offer to sell, sell, import, and otherwise transfer the Work,
+   where such license applies only to those patent claims licensable
+   by such Contributor that are necessarily infringed by their
+   Contribution(s) alone or by combination of their Contribution(s)
+   with the Work to which such Contribution(s) was submitted. If You
+   institute patent litigation against any entity (including a
+   cross-claim or counterclaim in a lawsuit) alleging that the Work
+   or a Contribution incorporated within the Work constitutes direct
+   or contributory patent infringement, then any patent licenses
+   granted to You under this License for that Work shall terminate
+   as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the
+   Work or Derivative Works thereof in any medium, with or without
+   modifications, and in Source or Object form, provided that You
+   meet the following conditions:
+
+   (a) You must give any other recipients of the Work or
+       Derivative Works a copy of this License; and
+
+   (b) You must cause any modified files to carry prominent notices
+       stating that You changed the files; and
+
+   (c) You must retain, in the Source form of any Derivative Works
+       that You distribute, all copyright, patent, trademark, and
+       attribution notices from the Source form of the Work,
+       excluding those notices that do not pertain to any part of
+       the Derivative Works; and
+
+   (d) If the Work includes a "NOTICE" text file as part of its
+       distribution, then any Derivative Works that You distribute must
+       include a readable copy of the attribution notices contained
+       within such NOTICE file, excluding those notices that do not
+       pertain to any part of the Derivative Works, in at least one
+       of the following places: within a NOTICE text file distributed
+       as part of the Derivative Works; within the Source form or
+       documentation, if provided along with the Derivative Works; or,
+       within a display generated by the Derivative Works, if and
+       wherever such third-party notices normally appear. The contents
+       of the NOTICE file are for informational purposes only and
+       do not modify the License. You may add Your own attribution
+       notices within Derivative Works that You distribute, alongside
+       or as an addendum to the NOTICE text from the Work, provided
+       that such additional attribution notices cannot be construed
+       as modifying the License.
+
+   You may add Your own copyright statement to Your modifications and
+   may provide additional or different license terms and conditions
+   for use, reproduction, or distribution of Your modifications, or
+   for any such Derivative Works as a whole, provided Your use,
+   reproduction, and distribution of the Work otherwise complies with
+   the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise,
+   any Contribution intentionally submitted for inclusion in the Work
+   by You to the Licensor shall be under the terms and conditions of
+   this License, without any additional terms or conditions.
+   Notwithstanding the above, nothing herein shall supersede or modify
+   the terms of any separate license agreement you may have executed
+   with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade
+   names, trademarks, service marks, or product names of the Licensor,
+   except as required for reasonable and customary use in describing the
+   origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or
+   agreed to in writing, Licensor provides the Work (and each
+   Contributor provides its Contributions) on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+   implied, including, without limitation, any warranties or conditions
+   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+   PARTICULAR PURPOSE. You are solely responsible for determining the
+   appropriateness of using or redistributing the Work and assume any
+   risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory,
+   whether in tort (including negligence), contract, or otherwise,
+   unless required by applicable law (such as deliberate and grossly
+   negligent acts) or agreed to in writing, shall any Contributor be
+   liable to You for damages, including any direct, indirect, special,
+   incidental, or consequential damages of any character arising as a
+   result of this License or out of the use or inability to use the
+   Work (including but not limited to damages for loss of goodwill,
+   work stoppage, computer failure or malfunction, or any and all
+   other commercial damages or losses), even if such Contributor
+   has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing
+   the Work or Derivative Works thereof, You may choose to offer,
+   and charge a fee for, acceptance of support, warranty, indemnity,
+   or other liability obligations and/or rights consistent with this
+   License. However, in accepting such obligations, You may act only
+   on Your own behalf and on Your sole responsibility, not on behalf
+   of any other Contributor, and only if You agree to indemnify,
+   defend, and hold each Contributor harmless for any liability
+   incurred by, or claims asserted against, such Contributor by reason
+   of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+APPENDIX: How to apply the Apache License to your work.
+
+   To apply the Apache License to your work, attach the following
+   boilerplate notice, with the fields enclosed by brackets "[]"
+   replaced with your own identifying information. (Don't include
+   the brackets!)  The text should be enclosed in the appropriate
+   comment syntax for the file format. We also recommend that a
+   file or class name and description of purpose be included on the
+   same "printed page" as the copyright notice for easier
+   identification within third-party archives.
+
+Copyright [yyyy] [name of copyright owner]
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+	http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/crates/hash32/LICENSE-MIT b/crates/hash32/LICENSE-MIT
new file mode 100644
index 0000000..6516dee
--- /dev/null
+++ b/crates/hash32/LICENSE-MIT
@@ -0,0 +1,25 @@
+Copyright (c) 2018 Jorge Aparicio
+
+Permission is hereby granted, free of charge, to any
+person obtaining a copy of this software and associated
+documentation files (the "Software"), to deal in the
+Software without restriction, including without
+limitation the rights to use, copy, modify, merge,
+publish, distribute, sublicense, and/or sell copies of
+the Software, and to permit persons to whom the Software
+is furnished to do so, subject to the following
+conditions:
+
+The above copyright notice and this permission notice
+shall be included in all copies or substantial portions
+of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF
+ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
+TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
+PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT
+SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
+IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+DEALINGS IN THE SOFTWARE.
diff --git a/crates/hash32/README.md b/crates/hash32/README.md
new file mode 100644
index 0000000..5f3832b
--- /dev/null
+++ b/crates/hash32/README.md
@@ -0,0 +1,21 @@
+# `hash32`
+
+> 32-bit hashing machinery
+
+## [Documentation](https://docs.rs/hash32)
+
+## License
+
+Licensed under either of
+
+- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or
+  http://www.apache.org/licenses/LICENSE-2.0)
+- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)
+
+at your option.
+
+### Contribution
+
+Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the
+work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any
+additional terms or conditions.
diff --git a/crates/hash32/src/fnv.rs b/crates/hash32/src/fnv.rs
new file mode 100644
index 0000000..9dbcbc7
--- /dev/null
+++ b/crates/hash32/src/fnv.rs
@@ -0,0 +1,37 @@
+use crate::Hasher as _;
+
+const BASIS: u32 = 0x811c9dc5;
+const PRIME: u32 = 0x1000193;
+
+/// 32-bit Fowler-Noll-Vo hasher
+pub struct Hasher {
+    state: u32,
+}
+
+impl Default for Hasher {
+    fn default() -> Self {
+        Hasher { state: BASIS }
+    }
+}
+
+impl crate::Hasher for Hasher {
+    #[inline]
+    fn finish32(&self) -> u32 {
+        self.state
+    }
+}
+
+impl core::hash::Hasher for Hasher {
+    #[inline]
+    fn write(&mut self, bytes: &[u8]) {
+        for byte in bytes {
+            self.state ^= u32::from(*byte);
+            self.state = self.state.wrapping_mul(PRIME);
+        }
+    }
+
+    #[inline]
+    fn finish(&self) -> u64 {
+        self.finish32().into()
+    }
+}
diff --git a/crates/hash32/src/lib.rs b/crates/hash32/src/lib.rs
new file mode 100644
index 0000000..56fea44
--- /dev/null
+++ b/crates/hash32/src/lib.rs
@@ -0,0 +1,141 @@
+//! 32-bit hashing algorithms
+//!
+//! # Why?
+//!
+//! Because 32-bit architectures are a thing (e.g. ARM Cortex-M) and you don't want your hashing
+//! function to pull in a bunch of slow 64-bit compiler intrinsics (software implementations of
+//! 64-bit operations).
+//!
+//! # Relationship to `core::hash`
+//!
+//! This crate extends [`core::hash`] with a 32-bit version of `Hasher`, which extends
+//! `core::hash::Hasher`. It requires that the hasher only performs 32-bit operations when computing
+//! the hash, and adds [`finish32`] to get the hasher's result as a `u32`. The standard `finish`
+//! method should just zero-extend this result.
+//!
+//! Since it extends `core::hash::Hasher`, `Hasher` can be used with any type which implements the
+//! standard `Hash` trait.
+//!
+//! This crate also adds a version of `BuildHasherDefault` with a const constructor, to work around
+//! the `core` version's lack of one.
+//!
+//! [`core::hash`]: https://doc.rust-lang.org/std/hash/index.html
+//! [`finish32`]: crate::Hasher::finish32
+//!
+//! # Hashers
+//!
+//! This crate provides implementations of the following 32-bit hashing algorithms:
+//!
+//! - [Fowler-Noll-Vo](struct.FnvHasher.html)
+//! - [MurmurHash3](struct.Murmur3Hasher.html)
+//!
+//! # Generic code
+//!
+//! In generic code, the trait bound `H: core::hash::Hasher` accepts *both* 64-bit hashers like
+//! `std::collections::hash_map::DefaultHasher`; and 32-bit hashers like the ones defined in this
+//! crate (`hash32::FnvHasher` and `hash32::Murmur3Hasher`)
+//!
+//! The trait bound `H: hash32::Hasher` is *more* restrictive as it only accepts 32-bit hashers.
+//!
+//! The `BuildHasherDefault<H>` type implements the `core::hash::BuildHasher` trait so it can
+//! construct both 32-bit and 64-bit hashers. To constrain the type to only produce 32-bit hasher
+//! you can add the trait bound `H::Hasher: hash32::Hasher`
+//!
+//! # MSRV
+//!
+//! This crate is guaranteed to compile on latest stable Rust. It *might* compile on older
+//! versions but that may change in any new patch release.
+
+#![deny(missing_docs)]
+#![deny(warnings)]
+#![no_std]
+
+extern crate byteorder;
+
+use core::fmt;
+use core::hash::BuildHasher;
+use core::marker::PhantomData;
+
+pub use fnv::Hasher as FnvHasher;
+pub use murmur3::Hasher as Murmur3Hasher;
+
+mod fnv;
+mod murmur3;
+
+/// A copy of [`core::hash::BuildHasherDefault`][0], but with a const constructor.
+///
+/// This will eventually be deprecated once the version in `core` becomes const-constructible
+/// (presumably using `const Default`).
+///
+/// [0]: https://doc.rust-lang.org/core/hash/struct.BuildHasherDefault.html
+pub struct BuildHasherDefault<H> {
+    _marker: PhantomData<H>,
+}
+
+impl<H> Default for BuildHasherDefault<H> {
+    fn default() -> Self {
+        BuildHasherDefault {
+            _marker: PhantomData,
+        }
+    }
+}
+
+impl<H> Clone for BuildHasherDefault<H> {
+    fn clone(&self) -> Self {
+        BuildHasherDefault::default()
+    }
+}
+
+impl<H> PartialEq for BuildHasherDefault<H> {
+    fn eq(&self, _other: &BuildHasherDefault<H>) -> bool {
+        true
+    }
+}
+
+impl<H> Eq for BuildHasherDefault<H> {}
+
+impl<H> fmt::Debug for BuildHasherDefault<H> {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        f.pad("BuildHasherDefault")
+    }
+}
+
+impl<H> BuildHasherDefault<H> {
+    /// `const` constructor
+    pub const fn new() -> Self {
+        BuildHasherDefault {
+            _marker: PhantomData,
+        }
+    }
+}
+
+impl<H> BuildHasher for BuildHasherDefault<H>
+where
+    H: Default + core::hash::Hasher,
+{
+    type Hasher = H;
+
+    fn build_hasher(&self) -> Self::Hasher {
+        H::default()
+    }
+}
+
+/// An extension of [core::hash::Hasher][0] for hashers which use 32 bits.
+///
+/// For hashers which implement this trait, the standard `finish` method should just return a
+/// zero-extended version of the result of `finish32`.
+///
+/// [0]: https://doc.rust-lang.org/core/hash/trait.Hasher.html
+///
+/// # Contract
+///
+/// Implementers of this trait must *not* perform any 64-bit (or 128-bit) operation while computing
+/// the hash.
+pub trait Hasher: core::hash::Hasher {
+    /// The equivalent of [`core::hash::Hasher.finish`][0] for 32-bit hashers.
+    ///
+    /// This returns the hash directly; `finish` zero-extends it to 64 bits for compatibility.
+    ///
+    /// [0]: https://doc.rust-lang.org/std/hash/trait.Hasher.html#tymethod.finish
+    fn finish32(&self) -> u32;
+}
diff --git a/crates/hash32/src/murmur3.rs b/crates/hash32/src/murmur3.rs
new file mode 100644
index 0000000..4fb63f1
--- /dev/null
+++ b/crates/hash32/src/murmur3.rs
@@ -0,0 +1,206 @@
+use core::slice;
+use core::mem::MaybeUninit;
+
+use byteorder::{ByteOrder, LE};
+
+use crate::Hasher as _;
+
+/// 32-bit MurmurHash3 hasher
+pub struct Hasher {
+    buf: Buffer,
+    index: Index,
+    processed: u32,
+    state: State,
+}
+
+struct State(u32);
+
+#[derive(Clone, Copy)]
+#[repr(align(4))]
+struct Buffer {
+    bytes: MaybeUninit<[u8; 4]>,
+}
+
+#[derive(Clone, Copy, PartialEq)]
+enum Index {
+    _0,
+    _1,
+    _2,
+    _3,
+}
+
+impl Index {
+    fn usize(&self) -> usize {
+        match *self {
+            Index::_0 => 0,
+            Index::_1 => 1,
+            Index::_2 => 2,
+            Index::_3 => 3,
+        }
+    }
+}
+
+impl From<usize> for Index {
+    fn from(x: usize) -> Self {
+        match x % 4 {
+            0 => Index::_0,
+            1 => Index::_1,
+            2 => Index::_2,
+            3 => Index::_3,
+            _ => unreachable!(),
+        }
+    }
+}
+
+impl Hasher {
+    fn push(&mut self, buf: &[u8]) {
+        let start = self.index.usize();
+        let len = buf.len();
+        // NOTE(unsafe) avoid calling `memcpy` on a 0-3 byte copy
+        // self.buf.bytes[start..start+len].copy_from(buf);
+        for i in 0..len {
+            unsafe {
+                *self.buf.bytes.assume_init_mut().get_unchecked_mut(start + i) = *buf.get_unchecked(i);
+            }
+        }
+        self.index = Index::from(start + len);
+    }
+}
+
+impl Default for Hasher {
+    #[allow(deprecated)]
+    fn default() -> Self {
+        Hasher {
+            buf: Buffer { bytes: MaybeUninit::uninit() },
+            index: Index::_0,
+            processed: 0,
+            state: State(0),
+        }
+    }
+}
+
+impl crate::Hasher for Hasher {
+    fn finish32(&self) -> u32 {
+        // tail
+        let mut state = match self.index {
+            Index::_3 => {
+                let mut block = 0;
+                unsafe {
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[2]) << 16;
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[1]) << 8;
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[0]);
+                }
+                self.state.0 ^ pre_mix(block)
+            }
+            Index::_2 => {
+                let mut block = 0;
+                unsafe {
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[1]) << 8;
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[0]);
+                }
+                self.state.0 ^ pre_mix(block)
+            }
+            Index::_1 => {
+                let mut block = 0;
+                unsafe {
+                    block ^= u32::from(self.buf.bytes.assume_init_ref()[0]);
+                }
+                self.state.0 ^ pre_mix(block)
+            }
+            Index::_0 => self.state.0,
+        };
+
+        // finalization mix
+        state ^= self.processed;
+        state ^= state >> 16;
+        state = state.wrapping_mul(0x85ebca6b);
+        state ^= state >> 13;
+        state = state.wrapping_mul(0xc2b2ae35);
+        state ^= state >> 16;
+
+        state
+    }
+}
+
+impl core::hash::Hasher for Hasher {
+    #[inline]
+    fn write(&mut self, bytes: &[u8]) {
+        let len = bytes.len();
+        self.processed += len as u32;
+
+        let body = if self.index == Index::_0 {
+            bytes
+        } else {
+            let index = self.index.usize();
+            if len + index >= 4 {
+                // we can complete a block using the data left in the buffer
+                // NOTE(unsafe) avoid panicking branch (`slice_index_len_fail`)
+                // let (head, body) = bytes.split_at(4 - index);
+                let mid = 4 - index;
+                let head = unsafe { slice::from_raw_parts(bytes.as_ptr(), mid) };
+                let body = unsafe {
+                    slice::from_raw_parts(bytes.as_ptr().offset(mid as isize), len - mid)
+                };
+
+                // NOTE(unsafe) avoid calling `memcpy` on a 0-3 byte copy
+                // self.buf.bytes[index..].copy_from_slice(head);
+                for i in 0..4 - index {
+                    unsafe {
+                        *self.buf.bytes.assume_init_mut().get_unchecked_mut(index + i) = *head.get_unchecked(i);
+                    }
+                }
+
+                self.index = Index::_0;
+
+                self.state.process_block(&self.buf.bytes);
+
+                body
+            } else {
+                bytes
+            }
+        };
+
+        for block in body.chunks(4) {
+            if block.len() == 4 {
+                self.state
+                    .process_block(unsafe { &*(block.as_ptr() as *const _) });
+            } else {
+                self.push(block);
+            }
+        }
+
+        // XXX is this faster?
+        // for block in body.exact_chunks(4) {
+        //     self.state
+        //         .process_block(unsafe { &*(block.as_ptr() as *const _) });
+        // }
+
+        // let tail = body.split_at(body.len() / 4 * 4).1;
+
+        // self.push(tail);
+    }
+
+    #[inline]
+    fn finish(&self) -> u64 {
+        self.finish32().into()
+    }
+}
+
+const C1: u32 = 0xcc9e2d51;
+const C2: u32 = 0x1b873593;
+const R1: u32 = 15;
+
+impl State {
+    fn process_block(&mut self, block: &MaybeUninit<[u8; 4]>) {
+        self.0 ^= pre_mix(LE::read_u32(unsafe { block.assume_init_ref() }));
+        self.0 = self.0.rotate_left(13);
+        self.0 = 5u32.wrapping_mul(self.0).wrapping_add(0xe6546b64);
+    }
+}
+
+fn pre_mix(mut block: u32) -> u32 {
+    block = block.wrapping_mul(C1);
+    block = block.rotate_left(R1);
+    block = block.wrapping_mul(C2);
+    block
+}
diff --git a/crates/talc/Cargo.toml b/crates/talc/Cargo.toml
new file mode 100644
index 0000000..6c1a8c7
--- /dev/null
+++ b/crates/talc/Cargo.toml
@@ -0,0 +1,45 @@
+[package]
+name = "talc"
+version = "4.2.0"
+rust-version = "1.67.1"
+edition = "2021"
+readme = "README.md"
+authors = ["Shaun Beautement"]
+description = "A fast and flexible allocator for no_std and WebAssembly"
+repository = "https://github.com/SFBdragon/talc"
+keywords = ["allocator", "no_std", "memory", "heap", "wasm"]
+categories = ["memory-management", "no-std", "embedded", "wasm"]
+license = "MIT"
+exclude = [
+    "benchmark_graphs",
+    "benchmark_results",
+    "wasm-size",
+    "wasm-size.sh",
+    "wasm-perf",
+    "wasm-perf.sh",
+]
+
+
+[features]
+fuzzing = []
+counters = []
+nightly_api = []
+allocator = ["lock_api"]
+default = ["lock_api", "allocator", "nightly_api"]
+
+
+[dependencies]
+lock_api = { version = "0.4", optional = true, default-features = false }
+
+[dev-dependencies]
+fastrand = "1.9"
+spin = { version = "0.9.8", default-features = false, features = ["lock_api", "spin_mutex"] }
+linked_list_allocator = { version = "0.10", features =  ["use_spin_nightly", "const_mut_refs", "alloc_ref"] }
+good_memory_allocator = { version = "0.1", features = ["spin", "allocator"] }
+buddy-alloc = "0.5"
+dlmalloc = { version =  "0.2.4", default-features = false, features = ["global"] }
+
+[profile.release]
+lto = true
+codegen-units = 1
+panic = "abort"
diff --git a/crates/talc/LICENSE.md b/crates/talc/LICENSE.md
new file mode 100644
index 0000000..bce13f1
--- /dev/null
+++ b/crates/talc/LICENSE.md
@@ -0,0 +1,21 @@
+The MIT License (MIT)
+
+Copyright © 2023 Shaun Beautement
+
+Permission is hereby granted, free of charge, to any person obtaining a 
+copy of this software and associated documentation files (the “Software”), 
+to deal in the Software without restriction, including without limitation 
+the rights to use, copy, modify, merge, publish, distribute, sublicense, 
+and/or sell copies of the Software, and to permit persons to whom the 
+Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included 
+in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS 
+OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE 
+SOFTWARE.
diff --git a/crates/talc/README.md b/crates/talc/README.md
new file mode 100644
index 0000000..ff9b8d8
--- /dev/null
+++ b/crates/talc/README.md
@@ -0,0 +1,310 @@
+# Talc Allocator [![Crates.io](https://img.shields.io/crates/v/talc?style=flat-square&color=orange)](https://crates.io/crates/talc) ![Downloads](https://img.shields.io/crates/d/talc?style=flat-square) [![docs.rs](https://img.shields.io/docsrs/talc?style=flat-square)](https://docs.rs/talc/latest/talc/) [![License](https://img.shields.io/crates/l/talc?style=flat-square)](https://github.com/SFBdragon/talc/blob/master/LICENSE.md)
+
+<sep>
+
+<sub><i>If you'd like to support my work, a tip would be greatly appreciated via [Paypal](https://www.paypal.com/donate/?hosted_button_id=8CSQ92VV58VPQ). Thanks!</i></sub>
+
+#### What is this for?
+- Embedded systems, OS kernels, and other `no_std` environments
+- WebAssembly apps, as a drop-in replacement for the default allocator
+- Subsystems in normal programs that need especially quick arena allocation
+
+#### Why Talc?
+- Generally faster and/or more memory efficient than alternatives \*
+- Scales better to multiple cores for some workloads than alternatives \*
+- Custom Out-Of-Memory handlers for just-in-time heap management and recovery
+- Supports creating and resizing arbitrarily many heaps
+- Optional allocation statistics
+- Partial validation in debug mode
+
+_\* Of those I know of, at time of writing, depending on workload. See [benchmarks](#benchmarks) below._
+
+#### Why not Talc?
+- Doesn't integrate with operating systems' dynamic memory facilities out-of-the-box
+- Doesn't scale well to allocation-heavy concurrent processing
+
+## Table of Contents
+
+Targeting WebAssembly? You can find WASM-specific usage and benchmarks [here](./README_WASM.md).
+
+- [Setup](#setup)
+- [Benchmarks](#benchmarks)
+- [General Usage](#general-usage)
+- [Advanced Usage](#advanced-usage)
+- [Conditional Features](#conditional-features)
+- [Stable Rust and MSRV](#stable-rust-and-msrv)
+- [Algorithm](#algorithm)
+- [Changelog](#changelog)
+
+
+## Setup
+
+As a global allocator:
+```rust
+use talc::*;
+
+static mut ARENA: [u8; 10000] = [0; 10000];
+
+#[global_allocator]
+static ALLOCATOR: Talck<spin::Mutex<()>, ClaimOnOom> = Talc::new(unsafe {
+    // if we're in a hosted environment, the Rust runtime may allocate before
+    // main() is called, so we need to initialize the arena automatically
+    ClaimOnOom::new(Span::from_const_array(core::ptr::addr_of!(ARENA)))
+}).lock();
+
+fn main() {
+    let mut vec = Vec::with_capacity(100);
+    vec.extend(0..300usize);
+}
+```
+
+Or use it as an arena allocator via the `Allocator` API with `spin` as follows:
+```rust
+#![feature(allocator_api)]
+use talc::*;
+use core::alloc::{Allocator, Layout};
+
+static mut ARENA: [u8; 10000] = [0; 10000];
+
+fn main () {
+    let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();
+    unsafe { talck.lock().claim(ARENA.as_mut().into()); }
+    
+    talck.allocate(Layout::new::<[u32; 16]>());
+}
+```
+
+Note that while the `spin` crate's mutexes are used here, any lock implementing `lock_api` works.
+
+See [General Usage](#general-usage) and [Advanced Usage](#advanced-usage) for more details.
+
+## Benchmarks
+
+### Heap Efficiency Benchmark Results
+
+The average occupied capacity upon first allocation failure when randomly allocating/deallocating/reallocating.
+
+|             Allocator | Average Random Actions Heap Efficiency |
+| --------------------- | -------------------------------------- |
+|              dlmalloc |                                 99.07% |
+|              **talc** |                                 98.87% |
+| linked_list_allocator |                                 98.28% |
+|                galloc |                                 95.86% |
+|           buddy_alloc |                                 58.75% |
+
+### Random Actions Benchmark
+
+The number of successful allocations, deallocations, and reallocations within the allotted time.
+
+#### Single Threaded
+
+![Random Actions Benchmark Results](/benchmark_graphs/random_actions.png)
+
+#### 4 Threads, Increased Allocation Sizes
+
+![Random Actions Multi Benchmark Results](/benchmark_graphs/random_actions_multi.png)
+
+## Allocations & Deallocations Microbenchmark
+
+![Microbenchmark Results](/benchmark_graphs/microbench.png)
+
+Whiskers represent the interval from the 5th to 95th percentile.
+
+## General Usage
+
+Here is the list of important `Talc` methods:
+* Constructors:
+    * `new`
+* Information:
+    * `get_allocated_span` - returns the minimum heap span containing all allocated memory in an established heap
+* Management:
+    * `claim` - claim memory to establishing a new heap
+    * `extend` - extend an established heap
+    * `truncate` - reduce the extent of an established heap
+    * `lock` - wraps the `Talc` in a `Talck`, which supports the `GlobalAlloc` and `Allocator` APIs
+* Allocation:
+    * `malloc`
+    * `free`
+    * `grow`
+    * `shrink`
+
+Read their [documentation](https://docs.rs/talc/latest/talc/struct.Talc.html) for more info.
+
+[`Span`](https://docs.rs/talc/latest/talc/struct.Span.html) is a handy little type for describing memory regions, as trying to manipulate `Range<*mut u8>` or `*mut [u8]` or `base_ptr`-`size` pairs tends to be inconvenient or annoying.
+
+## Advanced Usage
+
+The most powerful feature of the allocator is that it has a modular OOM handling system, allowing you to fail out of or recover from allocation failure easily. 
+
+Provided `OomHandler` implementations include:
+- `ErrOnOom`: allocations fail on OOM
+- `ClaimOnOom`: claims a heap upon first OOM, useful for initialization
+- `WasmHandler`: itegrate with WebAssembly's `memory` module for automatic memory heap management
+
+As an example of a custom implementation, recovering by extending the heap is implemented below.
+
+```rust
+use talc::*;
+
+struct MyOomHandler {
+    heap: Span,
+}
+
+impl OomHandler for MyOomHandler {
+    fn handle_oom(talc: &mut Talc<Self>, layout: core::alloc::Layout) -> Result<(), ()> {
+        // Talc doesn't have enough memory, and we just got called!
+        // We'll go through an example of how to handle this situation.
+    
+        // We can inspect `layout` to estimate how much we should free up for this allocation
+        // or we can extend by any amount (increasing powers of two has good time complexity).
+        // (Creating another heap with `claim` will also work.)
+    
+        // This function will be repeatedly called until we free up enough memory or 
+        // we return Err(()) causing allocation failure. Be careful to avoid conditions where 
+        // the heap isn't sufficiently extended indefinitely, causing an infinite loop.
+    
+        // an arbitrary address limit for the sake of example
+        const HEAP_TOP_LIMIT: *mut u8 = 0x80000000 as *mut u8;
+    
+        let old_heap: Span = talc.oom_handler.heap;
+    
+        // we're going to extend the heap upward, doubling its size
+        // but we'll be sure not to extend past the limit
+        let new_heap: Span = old_heap.extend(0, old_heap.size()).below(HEAP_TOP_LIMIT);
+    
+        if new_heap == old_heap {
+            // we won't be extending the heap, so we should return Err
+            return Err(());
+        }
+    
+        unsafe {
+            // we're assuming the new memory up to HEAP_TOP_LIMIT is unused and allocatable
+            talc.oom_handler.heap = talc.extend(old_heap, new_heap);
+        }
+    
+        Ok(())
+    }
+}
+```
+
+## Conditional Features
+* `"lock_api"` (default): Provides the `Talck` locking wrapper type that implements `GlobalAlloc`.
+* `"allocator"` (default, requires nightly): Provides an `Allocator` trait implementation via `Talck`.
+* `"nightly_api"` (default, requires nightly): Provides the `Span::from(*mut [T])` and `Span::from_slice` functions.
+* `"counters"`: `Talc` will track heap and allocation metrics. Use `Talc::get_counters` to access them.
+
+## Stable Rust and MSRV
+Talc can be built on stable Rust by disabling `"allocator"` and `"nightly_api"`. The MSRV is 1.67.1.
+
+Disabling `"nightly_api"` disables `Span::from(*mut [T])`, `Span::from(*const [T])`, `Span::from_const_slice` and `Span::from_slice`.
+
+## Algorithm
+This is a dlmalloc-style linked list allocator with boundary tagging and bucketing, aimed at general-purpose use cases. Allocation is O(n) worst case (but in practice its near-constant time, see microbenchmarks), while in-place reallocations and deallocations are O(1).
+
+Additionally, the layout of chunk metadata is rearranged to allow for smaller minimum-size chunks to reduce memory overhead of small allocations. The minimum chunk size is `3 * usize`, with a single `usize` being reserved per allocation. This is more efficient than `dlmalloc` and `galloc`, despite using a similar algorithm.
+
+## Changelog
+
+#### v4.2.0
+
+- Optimized reallocation to allows other allocation operations to occur while memcopy-ing if an in-place reallocation failed.
+    - As a side effect Talc now has a `grow_in_place` function that returns `Err` if growing the memory in-place isn't possible.
+    - A graph of the random actions benchmark with a workload that benefits from this has been included in the [benchmarks](#benchmarks) section.
+
+- Added `Span::from_*` and `From<>` functions for const pointers and shared references.
+    - This makes creating a span in static contexts on stable much easier: `Span::from_const_array(addr_of!(MEMORY))`
+- Fix: Made `Talck` derive `Debug` again.
+
+- Contribution by [Ken Hoover](https://github.com/khoover): add Talc arena-style allocation size and perf WASM benchmarks
+    - This might be a great option if you have a known dynamic memory requirement and would like to reduce your WASM size a little more.
+
+- `wasm-size` now uses _wasm-opt_, giving more realistic size differences for users of _wasm-pack_
+- Improved shell scripts
+- Overhauled microbenchmarks
+    - No longer simulates high-heap pressure as tolerating allocation failure is rare
+    - Data is now displayed using box-and-whisker plots
+
+#### v4.1.1
+
+- Fix: Reset MSRV to 1.67.1 and added a check to `test.sh` for it
+
+#### v4.1.0 (yanked, use 4.1.1)
+
+- Added optional tracking of allocation metrics. Thanks [Ken Hoover](https://github.com/khoover) for the suggestion!
+    - Enable the `"counters"` feature. Access the data via `talc.get_counters()`
+    - Metrics include allocation count, bytes available, fragmentation, overhead, and more.
+- Improvements to documentation
+- Improved and updated benchmarks
+- Integrated the WASM performance benchmark into the project. Use `wasm-bench.sh` to run (requires _wasm-pack_ and _deno_)
+- Improved `wasm-size` and `wasm-size.sh`
+
+#### v4.0.0
+- Changed `Talck`'s API to be more inline with Rust norms. 
+    - `Talck` now hides its internal structure (no more `.0`).
+    - `Talck::talc()` has been replaced by `Talck::lock()`. 
+    - `Talck::new()` and `Talck::into_inner(self)` have been added.
+    - Removed `TalckRef` and implemented the `Allocator` trait on `Talck` directly. No need to call `talck.allocator()` anymore.
+- Changed API for provided locking mechanism
+    - Moved `AssumeUnlockable` into `talc::locking::AssumeUnlockable`
+    - Removed `Talc::lock_assume_single_threaded`, use `.lock::<talc::locking::AssumeUnlockable>()` if necessary.
+- Improvements to documentation here and there. Thanks [polarathene](https://github.com/polarathene) for the contribution!
+
+#### v3.1.2
+- Some improvements to documentation.
+
+#### v3.1.1
+- Changed the WASM OOM handler's behavior to be more robust if other code calls `memory.grow` during the allocator's use.
+
+#### v3.1.0
+- Reduced use of nightly-only features, and feature-gated the remainder (`Span::from(*mut [T])` and `Span::from_slice`) behind `nightly_api`.
+- `nightly_api` feature is default-enabled
+    - *WARNING:* use of `default-features = false` may cause unexpected errors if the gated functions are used. Consider adding `nightly_api` or using another function.
+
+#### v3.0.1
+- Improved documentation
+- Improved and updated benchmarks
+    - Increased the range of allocation sizes on Random Actions. (sorry Buddy Allocator!)
+    - Increased the number of iterations the Heap Efficiency benchmark does to produce more accurate and stable values.
+
+#### v3.0.0
+- Added support for multiple discontinuous heaps! This required some major API changes
+    - `new_arena` no longer exists (use `new` and then `claim`)
+    - `init` has been replaced with `claim`
+    - `claim`, `extend` and `truncate` now return the new heap extent 
+    - `InitOnOom` is now `ClaimOnOom`. 
+    - All of the above now have different behavior and documentation.
+- Each heap now has a fixed overhead of one `usize` at the bottom.
+
+To migrate from v2 to v3, keep in mind that you must keep track of the heaps if you want to resize them, by storing the returned `Span`s. Read [`claim`](https://docs.rs/talc/latest/talc/struct.Talc.html#method.claim), [`extend`](https://docs.rs/talc/latest/talc/struct.Talc.html#method.extend) and [`truncate`](https://docs.rs/talc/latest/talc/struct.Talc.html#method.truncate)'s documentation for all the details.
+
+#### v2.2.1
+- Rewrote the allocator internals to place allocation metadata above the allocation.
+    - This will have the largest impact on avoiding false sharing, where previously, the allocation metadata for one allocation would infringe on the cache-line of the allocation before it, even if a sufficiently high alignment was demanded. Single-threaded performance marginally increased, too.
+- Removed heap_exhaustion and replaced heap_efficiency benchmarks.
+- Improved documentation and other resources.
+- Changed the WASM size measurement to include slightly less overhead.
+
+#### v2.2.0
+- Added `dlmalloc` to the benchmarks.
+- WASM should now be fully supported via `TalckWasm`. Let me know what breaks ;)
+    - Find more details [here](./README_WASM.md).
+
+
+#### v2.1.0
+- Tests are now passing on 32 bit targets.
+- Documentation fixes and improvements for various items.
+- Fixed using `lock_api` without `allocator`.
+- Experimental WASM support has been added via `TalckWasm` on WASM targets.
+
+
+#### v2.0.0
+- Removed dependency on `spin` and switched to using `lock_api` (thanks [Stefan Lankes](https://github.com/stlankes))
+    - You can specify the lock you want to use with `talc.lock::<spin::Mutex<()>>()` for example.
+- Removed the requirement that the `Talc` struct must not be moved, and removed the `mov` function.
+    - The arena is now used to store metadata, so extremely small arenas will result in allocation failure.
+- Made the OOM handling system use generics and traits instead of a function pointer.
+    - Use `ErrOnOom` to do what it says on the tin. `InitOnOom` is similar but inits to the given span if completely uninitialized. Implement `OomHandler` on any struct to implement your own behaviour (the OOM handler state can be accessed from `handle_oom` via `talc.oom_handler`).
+- Changed the API and internals of `Span` and other changes to pass `miri`'s Stacked Borrows checks.
+    - Span now uses pointers exclusively and carries provenance.
+- Updated the benchmarks in a number of ways, notably adding `buddy_alloc` and removing `simple_chunk_allocator`.
+
diff --git a/crates/talc/README_WASM.md b/crates/talc/README_WASM.md
new file mode 100644
index 0000000..244beae
--- /dev/null
+++ b/crates/talc/README_WASM.md
@@ -0,0 +1,59 @@
+# Talc for WebAssembly
+
+Talc is also a drop-in replacement for the default Rust WebAssembly allocator, dlmalloc. The two main configurations's usage and benchmarks are below. Both provide a decent middleground by being faster than `lol_alloc` and `dlmalloc` while inbetweening them in size.
+
+## Usage
+Set the global allocator in your project after running `cargo add talc` as follows:
+
+```rust
+/// SAFETY: The runtime environment must be single-threaded WASM.
+#[global_allocator]
+static ALLOCATOR: talc::TalckWasm = unsafe { talc::TalckWasm::new_global() };
+```
+
+Or if your arena size is statically known, for example 16 MiB, `0x1000000`:
+
+```rust
+#[global_allocator]
+static ALLOCATOR: talc::Talck<talc::locking::AssumeUnlockable, talc::ClaimOnOom> = {
+    static mut MEMORY: [u8; 0x1000000] = [0; 0x1000000];
+    let span = talc::Span::from_const_array(std::ptr::addr_of!(MEMORY));
+    talc::Talc::new(unsafe { talc::ClaimOnOom::new(span) }).lock()
+};
+```
+
+## Configuration features for WebAssembly:
+- If default features are disabled, make sure to enable `"lock_api"`.
+- Turn on `"counters"` for allocation statistics accessible via `ALLOCATOR.lock().get_counters()`
+- You can turn off default features to remove `"nightly_api"`, allowing stable Rust builds.
+
+    e.g. `default-features = false, features = ["lock_api", "counters"]`
+
+## Relative WASM Binary Size
+
+Rough measurements of allocator size for relative comparison using `/wasm-size`.
+
+| Allocator | WASM Size/bytes |
+| --------- | ----- |
+| lol_alloc | 11662 |
+| **talc** (arena\*) | 13546 |
+| **talc** | 14470 |
+| dlmalloc (default) | 16079 |
+
+\* uses a static arena instead of dynamically managing the heap
+
+## WASM Benchmarks
+
+Rough measurements of allocator speed for relative comparison using `/wasm-bench`.
+
+| Allocator | Average Actions/s |
+|-----------|-----|
+| **talc** | 6.3 |
+| **talc** (arena\*) | 6.2 |
+| dlmalloc (default) | 5.8 |
+| lol_alloc | 2.9 |
+
+\* uses a static arena instead of dynamically managing the heap
+
+
+If you'd like to see comparisons to other allocators in this space, consider creating a pull request or opening an issue.
diff --git a/crates/talc/benchmark_graphs/microbench.png b/crates/talc/benchmark_graphs/microbench.png
new file mode 100644
index 0000000..41c315c
Binary files /dev/null and b/crates/talc/benchmark_graphs/microbench.png differ
diff --git a/crates/talc/benchmark_graphs/random_actions.png b/crates/talc/benchmark_graphs/random_actions.png
new file mode 100644
index 0000000..f8a026e
Binary files /dev/null and b/crates/talc/benchmark_graphs/random_actions.png differ
diff --git a/crates/talc/benchmark_graphs/random_actions_multi.png b/crates/talc/benchmark_graphs/random_actions_multi.png
new file mode 100644
index 0000000..a8c2c16
Binary files /dev/null and b/crates/talc/benchmark_graphs/random_actions_multi.png differ
diff --git a/crates/talc/buckets.py b/crates/talc/buckets.py
new file mode 100644
index 0000000..f3b577a
--- /dev/null
+++ b/crates/talc/buckets.py
@@ -0,0 +1,55 @@
+import math
+
+# modify these parameters to determine the bucketing strategy
+# the main things we want are 
+# - coverage up to the 100MiB-1GiB area
+# - minimize number of allocation sizes per bucket
+# - facilitate particularly quick allocation of small sizes
+# - don't sacrifice the speed of large allocations much
+
+## for 32-bit machines:
+#word_size = 4
+#word_buckets_limit = 64
+#double_buckets_limit = 128
+#exp_fractions = 2
+
+# for 64-bit machines:
+word_size = 8
+word_buckets_limit = 256
+double_buckets_limit = 512
+exp_fractions = 4
+
+
+# the rest of this 
+
+min_chunk_size = word_size * 3
+
+word_bins_count = (word_buckets_limit - min_chunk_size) // word_size
+print("word bins count:", word_bins_count)
+
+for i, sb in enumerate(range(min_chunk_size, word_buckets_limit, word_size)):
+    print("{1:>3}: {0:>8} {0:>20b} | ".format(sb, i), end='\n')
+
+double_bins_count = (double_buckets_limit - word_buckets_limit) // (2*word_size)
+print("double bins count:", double_bins_count)
+
+for i, bsb in enumerate(range(word_buckets_limit, double_buckets_limit, 2*word_size)):
+    print("{1:>3}: {0:>8} {0:>20b} | ".format(bsb, i), end='\n')
+
+print("pseudo log-spaced bins")
+
+b_ofst = int(math.log2(double_buckets_limit)) # log2_start_pow | 16
+b_p2dv = int(math.log2(exp_fractions)) # log2_div_count | 4
+
+for b in range(0, (word_size * 8 * 2) - word_bins_count - double_bins_count):
+    # calculation for size from b
+    size = ((1 << b_p2dv) + (b & ((1<<b_p2dv)-1))) << ((b >> b_p2dv) + (b_ofst-b_p2dv))
+
+    # calculation of b from size
+    size_log2 = math.floor(math.log2(size))
+    b_calc = ((size >> size_log2 - b_p2dv) ^ (1<<b_p2dv)) + ((size_log2-b_ofst) << b_p2dv)
+
+    # check that they match
+    assert b == b_calc
+
+    print("{1:>3}: {0:>8} {0:>20b} | ".format(size, b + word_bins_count + double_bins_count), end='\n')
diff --git a/crates/talc/check.sh b/crates/talc/check.sh
new file mode 100755
index 0000000..810adb7
--- /dev/null
+++ b/crates/talc/check.sh
@@ -0,0 +1,37 @@
+#!/bin/bash
+
+set -euxo pipefail
+
+# This is the whole kitchen sink to help ensure builds are ready to be published.
+
+rustup run stable cargo check --no-default-features
+rustup run stable cargo check --no-default-features --features=lock_api
+rustup run stable cargo check --no-default-features --features=lock_api,counters
+
+rustup run nightly cargo check
+
+rustup run nightly cargo test --features=counters
+rustup run nightly cargo test --tests --no-default-features
+rustup run nightly cargo test --tests --no-default-features --features=lock_api,counters
+
+rustup run nightly cargo miri test --tests
+rustup run nightly cargo miri test --tests --target i686-unknown-linux-gnu
+
+rustup run stable cargo check --no-default-features --target wasm32-unknown-unknown
+rustup run stable cargo check --no-default-features --features=lock_api,counters --target wasm32-unknown-unknown
+
+# check whether MSRV has been broken
+rustup run 1.67.1 cargo check --no-default-features --features lock_api,counters
+
+
+# check that the wasm benches haven't been broken
+
+# check wasm size benches
+./wasm-size.sh check
+# check wasm size MSRV
+cd wasm-size && rustup run 1.68 cargo check --target wasm32-unknown-unknown && cd -
+
+# check wasm perf benches
+./wasm-perf.sh check
+# check wasm perf MSRV
+cd wasm-perf && rustup run 1.67.1 wasm-pack --log-level warn build --dev --target web && cd -
diff --git a/crates/talc/examples/microbench.rs b/crates/talc/examples/microbench.rs
new file mode 100644
index 0000000..7be9d7b
--- /dev/null
+++ b/crates/talc/examples/microbench.rs
@@ -0,0 +1,300 @@
+/*
+MIT License
+
+Copyright (c) 2022 Philipp Schuster
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+// Heavily modified by Shaun Beautement. All errors are probably my own.
+
+#![feature(allocator_api)]
+#![feature(iter_intersperse)]
+#![feature(slice_ptr_get)]
+
+use buddy_alloc::{BuddyAllocParam, FastAllocParam, NonThreadsafeAlloc};
+use good_memory_allocator::DEFAULT_SMALLBINS_AMOUNT;
+use talc::{ErrOnOom, Talc};
+
+use std::alloc::{AllocError, Allocator, GlobalAlloc, Layout};
+use std::fs::File;
+use std::time::Instant;
+
+const BENCH_DURATION: f64 = 1.0;
+
+const HEAP_SIZE: usize = 0x10000000;
+static mut HEAP_MEMORY: [u8; HEAP_SIZE] = [0u8; HEAP_SIZE];
+
+// NonThreadsafeAlloc doesn't implement Allocator: wrap it
+struct BuddyAllocator(NonThreadsafeAlloc);
+
+unsafe impl Allocator for BuddyAllocator {
+    fn allocate(&self, layout: Layout) -> Result<std::ptr::NonNull<[u8]>, AllocError> {
+        let ptr = unsafe { self.0.alloc(layout) };
+
+        match std::ptr::NonNull::new(ptr) {
+            Some(nn) => Ok(std::ptr::NonNull::slice_from_raw_parts(nn, layout.size())),
+            None => Err(AllocError),
+        }
+    }
+
+    unsafe fn deallocate(&self, ptr: std::ptr::NonNull<u8>, layout: Layout) {
+        self.0.dealloc(ptr.as_ptr(), layout);
+    }
+}
+
+struct DlMallocator(spin::Mutex<dlmalloc::Dlmalloc<DlmallocArena>>);
+
+unsafe impl Allocator for DlMallocator {
+    fn allocate(&self, layout: Layout) -> Result<std::ptr::NonNull<[u8]>, AllocError> {
+        let ptr = unsafe { self.0.lock().malloc(layout.size(), layout.align()) };
+
+        match std::ptr::NonNull::new(ptr) {
+            Some(nn) => Ok(std::ptr::NonNull::slice_from_raw_parts(nn, layout.size())),
+            None => Err(AllocError),
+        }
+    }
+
+    unsafe fn deallocate(&self, ptr: std::ptr::NonNull<u8>, layout: Layout) {
+        self.0.lock().free(ptr.as_ptr(), layout.size(), layout.align());
+    }
+}
+
+// Turn DlMalloc into an arena allocator
+struct DlmallocArena(std::sync::atomic::AtomicBool);
+
+unsafe impl dlmalloc::Allocator for DlmallocArena {
+    fn alloc(&self, _size: usize) -> (*mut u8, usize, u32) {
+        let has_data = self.0.fetch_and(false, core::sync::atomic::Ordering::SeqCst);
+
+        if has_data {
+            let align = std::mem::align_of::<usize>();
+            let heap_align_offset = unsafe { HEAP_MEMORY.as_mut_ptr() }.align_offset(align);
+            unsafe { (HEAP_MEMORY.as_mut_ptr().add(heap_align_offset), (HEAP_SIZE - heap_align_offset) / align * align, 1) }
+        } else {
+            (core::ptr::null_mut(), 0, 0)
+        }
+    }
+
+    fn remap(&self, _ptr: *mut u8, _oldsize: usize, _newsize: usize, _can_move: bool) -> *mut u8 {
+        unimplemented!()
+    }
+
+    fn free_part(&self, _ptr: *mut u8, _oldsize: usize, _newsize: usize) -> bool {
+        unimplemented!()
+    }
+
+    fn free(&self, _ptr: *mut u8, _size: usize) -> bool {
+        unimplemented!()
+    }
+
+    fn can_release_part(&self, _flags: u32) -> bool {
+        false
+    }
+
+    fn allocates_zeros(&self) -> bool {
+        false
+    }
+
+    fn page_size(&self) -> usize {
+        4 * 1024
+    }
+}
+
+fn main() {
+    const BENCHMARK_RESULTS_DIR: &str = "./benchmark_results/micro/";
+    // create a directory for the benchmark results.
+    let _ = std::fs::create_dir(BENCHMARK_RESULTS_DIR);
+
+    let deallocs_file = File::create(BENCHMARK_RESULTS_DIR.to_owned() + "deallocs.csv").unwrap();
+    //let reallocs_file = File::create(BENCHMARK_RESULTS_DIR.to_owned() + "reallocs.csv").unwrap();
+    let allocs_file = File::create(BENCHMARK_RESULTS_DIR.to_owned() + "allocs.csv").unwrap();
+    let mut csvs = Csvs { allocs_file, deallocs_file };
+
+    // warm up the memory caches, avoid demand paging issues, etc.
+    for i in 0..HEAP_SIZE {
+        unsafe {
+            HEAP_MEMORY.as_mut_ptr().add(i).write(0xAE);
+        }
+    }
+
+    /* let linked_list_allocator =ptr.read_volatile()
+        unsafe { linked_list_allocator::LockedHeap::new(HEAP_MEMORY.as_mut_ptr() as _, HEAP_SIZE) };
+    
+    benchmark_allocator(&linked_list_allocator, "Linked List Allocator", &mut csvs); */
+
+    let mut galloc_allocator =
+        good_memory_allocator::SpinLockedAllocator::<DEFAULT_SMALLBINS_AMOUNT>::empty();
+    unsafe {
+        galloc_allocator.init(HEAP_MEMORY.as_ptr() as usize, HEAP_SIZE);
+    }
+
+    benchmark_allocator(&mut galloc_allocator, "Galloc", &mut csvs);
+
+    let buddy_alloc = unsafe {
+        buddy_alloc::NonThreadsafeAlloc::new(
+            FastAllocParam::new(HEAP_MEMORY.as_ptr(), HEAP_SIZE / 8),
+            BuddyAllocParam::new(HEAP_MEMORY.as_ptr().add(HEAP_SIZE / 8), HEAP_SIZE / 8 * 7, 64),
+        )
+    };
+    benchmark_allocator(&BuddyAllocator(buddy_alloc), "Buddy Allocator", &mut csvs);
+
+    let dlmalloc = dlmalloc::Dlmalloc::new_with_allocator(DlmallocArena(true.into()));
+    
+    benchmark_allocator(&DlMallocator(spin::Mutex::new(dlmalloc)), "Dlmalloc", &mut csvs);
+
+    let talc = Talc::new(ErrOnOom).lock::<talc::locking::AssumeUnlockable/* spin::Mutex<()> */>();
+    unsafe { talc.lock().claim(HEAP_MEMORY.as_mut().into()) }.unwrap();
+    
+    benchmark_allocator(&talc, "Talc", &mut csvs);
+}
+
+fn now() -> u64 {
+    #[cfg(target_arch = "x86_64")]
+    {
+        let mut x = 0u32;
+        unsafe { std::arch::x86_64::__rdtscp(&mut x) }
+    }
+
+    #[cfg(target_arch = "aarch64")]
+    {
+        let mut timer: u64;
+        unsafe { std::arch::asm!("mrs {0}, cntvct_el0", out(reg) timer, options(nomem, nostack)); }
+        return timer;
+    }
+
+    #[cfg(not(any(target_arch = "aarch64", target_arch = "x86_64")))]
+    compile_error!(
+        "Hardware-based counter is not implemented for this architecture. Supported: x86_64, aarch64"
+    );
+}
+
+struct Csvs {
+    pub allocs_file: File, 
+    //pub reallocs_file: File,
+    pub deallocs_file: File,
+}
+
+fn benchmark_allocator(allocator: &dyn Allocator, name: &str, csvs: &mut Csvs) {
+    eprintln!("Benchmarking: {name}...");
+
+    let mut active_allocations = Vec::new();
+
+    let mut alloc_ticks_vec = Vec::new();
+    // let mut realloc_ticks_vec = Vec::new();
+    let mut dealloc_ticks_vec = Vec::new();
+
+    // warm up
+    for i in 1..10000 {
+        let layout = Layout::from_size_align(i * 8, 8).unwrap();
+        let ptr = allocator.allocate(layout).unwrap().as_non_null_ptr();
+        unsafe { let _ = ptr.as_ptr().read_volatile(); }
+        unsafe { allocator.deallocate(ptr, layout); }
+    }
+
+    let bench_timer = Instant::now();
+    for i in 0.. {
+        if i % 0x10000 == 0 && (Instant::now() - bench_timer).as_secs_f64() > BENCH_DURATION { break; }
+
+        let size = fastrand::usize((1 << 6)..(1 << 18));
+        let align = 8 << fastrand::u16(..).trailing_zeros() / 2;
+        let layout = Layout::from_size_align(size, align).unwrap();
+
+        let alloc_begin = now();
+        let alloc_res = allocator.allocate(layout);
+        let alloc_ticks = now().wrapping_sub(alloc_begin);
+
+        if let Ok(ptr) = alloc_res {
+            alloc_ticks_vec.push(alloc_ticks);
+            active_allocations.push((ptr.as_non_null_ptr(), layout));
+        } else {
+            for (ptr, layout) in active_allocations.drain(..) {
+                let dealloc_begin = now();
+                unsafe { allocator.deallocate(ptr, layout); }
+                let dealloc_ticks = now().wrapping_sub(dealloc_begin);
+                dealloc_ticks_vec.push(dealloc_ticks);
+            }
+            continue;
+        }
+
+        if active_allocations.len() > 10 && fastrand::usize(..10) == 0 {
+            for _ in 0..8 {
+                let index = fastrand::usize(..active_allocations.len());
+                let allocation = active_allocations.swap_remove(index);
+
+                let dealloc_begin = now();
+                unsafe {
+                    allocator.deallocate(allocation.0, allocation.1);
+                }
+                let dealloc_ticks = now().wrapping_sub(dealloc_begin);
+                dealloc_ticks_vec.push(dealloc_ticks);
+            }
+        }
+    }
+
+    let data_to_string = |data: &[u64]|
+        String::from_iter(data.into_iter().map(|x| x.to_string()).intersperse(",".to_owned()));
+
+    use std::io::Write;
+    writeln!(csvs.allocs_file, "{name},{}", data_to_string(&alloc_ticks_vec)).unwrap();
+    writeln!(csvs.deallocs_file, "{name},{}", data_to_string(&dealloc_ticks_vec)).unwrap();
+}
+
+/* fn print_bench_results(bench_name: &str, res: &BenchRunResults) {
+    println!("RESULTS OF BENCHMARK: {bench_name}");
+    println!(
+        " {:7} allocation attempts, {:7} successful allocations, {:7} pre-fail allocations, {:7} deallocations",
+        res.allocation_attempts,
+        res.successful_allocations,
+        res.pre_fail_allocations,
+        res.deallocations
+    );
+
+    println!(
+        "| {:>20} | Average | Minimum | 1st Quartile | Median | 3rd Quartile | ", "CATEGORY"
+    );
+    println!("|-|-|{}", "-|".repeat(4));
+    print_measurement_set(&res.nofail_alloc_measurements, "Normal Allocs");
+    print_measurement_set(&res.high_pressure_alloc_measurements, "High-Pressure Allocs");
+    print_measurement_set(&res.dealloc_measurements, "Deallocs");
+}
+
+fn print_measurement_set(measurements: &Vec<u64>, set_name: &str) {
+    print!("| {:>20} | {:>7} | ", set_name, measurements.iter().sum::<u64>() / measurements.len() as u64);
+    for i in 0..=8 {
+        print!("{:>8}", measurements[(measurements.len() / 8 * i).min(measurements.len() - 1)]);
+    }
+    print!("  (ticks)\n", );
+}
+
+/// Result of a bench run.
+struct BenchRunResults {
+    allocation_attempts: usize,
+    successful_allocations: usize,
+    pre_fail_allocations: usize,
+    deallocations: usize,
+
+    /// Sorted vector of the amount of clock ticks per successful allocation under heap pressure.
+    high_pressure_alloc_measurements: Vec<u64>,
+    /// Sorted vector of the amount of clock ticks per successful allocation.
+    nofail_alloc_measurements: Vec<u64>,
+    /// Sorted vector of the amount of clock ticks per deallocation.
+    dealloc_measurements: Vec<u64>,
+}
+ */
diff --git a/crates/talc/examples/random_actions.rs b/crates/talc/examples/random_actions.rs
new file mode 100644
index 0000000..888ce47
--- /dev/null
+++ b/crates/talc/examples/random_actions.rs
@@ -0,0 +1,428 @@
+/* The MIT License (MIT)
+
+Copyright © 2023 Roee Shoshani, Guy Nir
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the “Software”), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included
+in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+*/
+
+// Modified by Shaun Beautement
+
+#![feature(allocator_api)]
+#![feature(slice_ptr_get)]
+#![feature(iter_intersperse)]
+#![feature(const_mut_refs)]
+
+use std::{
+    alloc::{GlobalAlloc, Layout}, sync::{Arc, Barrier}, time::{Duration, Instant}
+};
+
+use buddy_alloc::{BuddyAllocParam, FastAllocParam, NonThreadsafeAlloc};
+
+const THREAD_COUNT: usize = 1;
+const RA_MAX_ALLOC_SIZE: usize = 2000;
+const RA_MAX_REALLOC_SIZE: usize = 20000;
+
+const HEAP_SIZE: usize = 1 << 29;
+static mut HEAP: [u8; HEAP_SIZE] = [0u8; HEAP_SIZE];
+
+const TIME_STEPS_AMOUNT: usize = 5;
+const TIME_STEP_MILLIS: usize = 200;
+
+const MIN_MILLIS_AMOUNT: usize = TIME_STEP_MILLIS;
+const MAX_MILLIS_AMOUNT: usize = TIME_STEP_MILLIS * TIME_STEPS_AMOUNT;
+
+const BENCHMARK_RESULTS_DIR: &str = "./benchmark_results";
+const TRIALS_AMOUNT: usize = 15;
+
+struct NamedBenchmark {
+    benchmark_fn: fn(Duration, &dyn GlobalAlloc, Arc<Barrier>) -> usize,
+    name: &'static str,
+}
+
+macro_rules! benchmark_list {
+    ($($benchmark_fn: path),+) => {
+        &[
+            $(
+                NamedBenchmark {
+                    benchmark_fn: $benchmark_fn,
+                    name: stringify!($benchmark_fn),
+                }
+            ),+
+        ]
+    }
+}
+
+struct NamedAllocator {
+    name: &'static str,
+    init_fn: unsafe fn() -> Box<dyn GlobalAlloc + Sync>,
+}
+
+macro_rules! allocator_list {
+    ($($init_fn: path),+) => {
+        &[
+            $(
+                NamedAllocator {
+                    init_fn: $init_fn,
+                    name: {
+                        const INIT_FN_NAME: &'static str = stringify!($init_fn);
+                        &INIT_FN_NAME["init_".len()..]
+                    },
+                }
+            ),+
+        ]
+    }
+}
+
+fn main() {
+    // create a directory for the benchmark results.
+    let _ = std::fs::create_dir(BENCHMARK_RESULTS_DIR);
+
+    let benchmarks = benchmark_list!(
+        random_actions
+    );
+
+    let allocators = allocator_list!(
+        init_talc,
+        init_dlmalloc,
+        init_buddy_alloc,
+        init_galloc,
+        init_linked_list_allocator
+    );
+
+    print!("Run heap efficiency microbenchmarks? y/N: ");
+    std::io::Write::flush(&mut std::io::stdout()).unwrap();
+    let mut input = String::new();
+    std::io::stdin().read_line(&mut input).unwrap();
+    if input.trim() == "y" {
+        // heap efficiency benchmark
+
+        println!("|             Allocator | Average Random Actions Heap Efficiency |");
+        println!("| --------------------- | -------------------------------------- |");
+
+        for allocator in allocators {
+            let efficiency = heap_efficiency(unsafe {(allocator.init_fn)() }.as_ref());
+
+            println!("|{:>22} | {:>38} |", allocator.name, format!("{:2.2}%", efficiency));
+        }
+    }
+
+    for benchmark in benchmarks {
+        let mut csv = String::new();
+        for allocator in allocators {
+            let scores_as_strings = (MIN_MILLIS_AMOUNT..=MAX_MILLIS_AMOUNT)
+                .step_by(TIME_STEP_MILLIS)
+                .map(|i| {
+                    eprintln!("benchmarking...");
+
+                    let duration = Duration::from_millis(i as _);
+
+                    (0..TRIALS_AMOUNT)
+                        .map(|_| {
+                            let allocator = unsafe { (allocator.init_fn)() };
+                            let allocator_ref = allocator.as_ref();
+
+                            std::thread::scope(|scope| {
+                                let barrier = Arc::new(Barrier::new(THREAD_COUNT));
+                                let mut handles = vec![];
+
+                                for _ in 0..THREAD_COUNT {
+                                    let bi = barrier.clone();
+                                    handles.push(scope.spawn(move || (benchmark.benchmark_fn)(duration, allocator_ref, bi)));
+                                }
+
+                                handles.into_iter().map(|h| h.join().unwrap()).sum::<usize>()
+                            })
+                        }).sum::<usize>() / TRIALS_AMOUNT
+                })
+                .map(|score| score.to_string());
+
+            let csv_line = std::iter::once(allocator.name.to_owned())
+                .chain(scores_as_strings)
+                .intersperse(",".to_owned())
+                .chain(std::iter::once("\n".to_owned()));
+            csv.extend(csv_line);
+        }
+        // remove the last newline.
+        csv.pop();
+
+        std::fs::write(format!("{}/{}.csv", BENCHMARK_RESULTS_DIR, benchmark.name), csv).unwrap();
+    }
+}
+
+
+pub fn random_actions(duration: Duration, allocator: &dyn GlobalAlloc, barrier: Arc<Barrier>) -> usize {
+    barrier.wait();
+
+    let mut score = 0;
+    let mut v = Vec::with_capacity(100000);
+
+    let rng = fastrand::Rng::new();
+
+    let start = Instant::now();
+    while start.elapsed() < duration {
+        for _ in 0..100 {
+            let action = rng.usize(0..3);
+
+            match action {
+                0 => {
+                    let size = rng.usize(1..RA_MAX_ALLOC_SIZE);
+                    let alignment =  std::mem::align_of::<usize>() << rng.u16(..).trailing_zeros() / 2;
+                    if let Some(allocation) = AllocationWrapper::new(size, alignment, allocator) {
+                        v.push(allocation);
+                        score += 1;
+                    }
+                }
+                1 => {
+                    if !v.is_empty() {
+                        let index = rng.usize(0..v.len());
+                        v.swap_remove(index);
+                        score += 1;
+                    }
+                }
+                2 => {
+                    if !v.is_empty() {
+                        let index = rng.usize(0..v.len());
+                        if let Some(random_allocation) = v.get_mut(index) {
+                            let size = rng.usize(1..RA_MAX_REALLOC_SIZE);
+                            random_allocation.realloc(size);
+                        }
+                        score += 1;
+                    }
+                }
+                _ => unreachable!(),
+            }
+        }
+    }
+
+    score
+}
+
+pub fn heap_efficiency(allocator: &dyn GlobalAlloc) -> f64 {
+    let mut v = Vec::with_capacity(100000);
+    let mut used = 0;
+    let mut total = HEAP_SIZE;
+
+    for _ in 0..500 {
+        loop {
+            let action = fastrand::usize(0..10);
+
+            match action {
+                0..=4 => {
+                    let size = fastrand::usize(1..(RA_MAX_ALLOC_SIZE*10));
+                    let align = std::mem::align_of::<usize>() << fastrand::u16(..).trailing_zeros() / 2;
+
+                    if let Some(allocation) = AllocationWrapper::new(size, align, allocator) {
+                        //used += allocation.layout.size();
+                        v.push(allocation);
+                    } else {
+                        used += v.iter().map(|a| a.layout.size()).sum::<usize>();
+                        total += HEAP_SIZE;
+                        v.clear();
+                        break;
+                    }
+                }
+                5 => {
+                    if !v.is_empty() {
+                        let index = fastrand::usize(0..v.len());
+                        //used -= v[index].layout.size();
+                        v.swap_remove(index);
+                    }
+                }
+                6..=9 => {
+                    if !v.is_empty() {
+                        let index = fastrand::usize(0..v.len());
+
+                        if let Some(random_allocation) = v.get_mut(index) {
+                            //let old_size = random_allocation.layout.size();
+                            let new_size = fastrand::usize(1..(RA_MAX_REALLOC_SIZE*10));
+                            random_allocation.realloc(new_size);
+                            //used = used + new_size - old_size;
+                        } else {
+                            used += v.iter().map(|a| a.layout.size()).sum::<usize>();
+                            total += HEAP_SIZE;
+                            v.clear();
+                            break;
+                        }
+                    }
+                }
+                _ => unreachable!(),
+            }
+        }
+    }
+
+    used as f64 / total as f64 * 100.0
+}
+
+struct AllocationWrapper<'a> {
+    ptr: *mut u8,
+    layout: Layout,
+    allocator: &'a dyn GlobalAlloc,
+}
+impl<'a> AllocationWrapper<'a> {
+    fn new(size: usize, align: usize, allocator: &'a dyn GlobalAlloc) -> Option<Self> {
+        let layout = Layout::from_size_align(size, align).unwrap();
+
+        let ptr = unsafe { (*allocator).alloc(layout) };
+
+        if ptr.is_null() {
+            return None;
+        }
+
+        Some(Self { ptr, layout, allocator })
+    }
+
+    fn realloc(&mut self, new_size: usize) {
+        let new_ptr = unsafe { (*self.allocator).realloc(self.ptr, self.layout, new_size) };
+        if new_ptr.is_null() {
+            return;
+        }
+        self.ptr = new_ptr;
+        self.layout = Layout::from_size_align(new_size, self.layout.align()).unwrap();
+    }
+}
+
+impl<'a> Drop for AllocationWrapper<'a> {
+    fn drop(&mut self) {
+        unsafe { (*self.allocator).dealloc(self.ptr, self.layout) }
+    }
+}
+
+
+
+/// Memory must be available.
+unsafe fn init_talc() -> Box<dyn GlobalAlloc + Sync> {
+    unsafe {
+        let talck: _ = talc::Talc::new(talc::ErrOnOom).lock::<spin::Mutex<()>>();
+        talck.lock().claim(HEAP.as_mut_slice().into()).unwrap();
+        Box::new(talck)
+    }
+}
+
+unsafe fn init_linked_list_allocator() -> Box<dyn GlobalAlloc + Sync> {
+    let lla = linked_list_allocator::LockedHeap::new(HEAP.as_mut_ptr(), HEAP_SIZE);
+    lla.lock().init(HEAP.as_mut_ptr().cast(), HEAP_SIZE);
+    Box::new(lla)
+}
+
+#[allow(unused)]
+unsafe fn init_galloc() -> Box<dyn GlobalAlloc + Sync> {
+    let galloc = good_memory_allocator::SpinLockedAllocator
+        ::<{good_memory_allocator::DEFAULT_SMALLBINS_AMOUNT}, {good_memory_allocator::DEFAULT_ALIGNMENT_SUB_BINS_AMOUNT}>
+        ::empty();
+    galloc.init(HEAP.as_ptr() as usize, HEAP_SIZE);
+    Box::new(galloc)
+}
+
+struct BuddyAllocWrapper(pub spin::Mutex<NonThreadsafeAlloc>);
+
+unsafe impl Send for BuddyAllocWrapper {}
+unsafe impl Sync for BuddyAllocWrapper {}
+
+unsafe impl GlobalAlloc for BuddyAllocWrapper {
+    unsafe fn alloc(&self, layout: Layout) -> *mut u8  { self.0.lock().alloc(layout) }
+
+    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout)  { self.0.lock().dealloc(ptr, layout) }
+
+    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 { self.0.lock().alloc_zeroed(layout) }
+
+    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+        self.0.lock().realloc(ptr, layout, new_size)
+    }
+}
+
+unsafe fn init_buddy_alloc() -> Box<dyn GlobalAlloc + Sync> {
+    let ba = BuddyAllocWrapper(spin::Mutex::new(NonThreadsafeAlloc::new(
+        FastAllocParam::new(HEAP.as_ptr().cast(), HEAP.len() / 8),
+        BuddyAllocParam::new(
+            HEAP.as_ptr().cast::<u8>().add(HEAP.len() / 8),
+            HEAP.len() / 8 * 7,
+            64,
+        ),
+    )));
+    
+    Box::new(ba)
+}
+
+struct DlMallocator(spin::Mutex<dlmalloc::Dlmalloc<DlmallocArena>>);
+
+unsafe impl GlobalAlloc for DlMallocator {
+    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
+        self.0.lock().malloc(layout.size(), layout.align())
+    }
+
+    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
+        self.0.lock().free(ptr, layout.size(), layout.align());
+    }
+
+    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+        self.0.lock().realloc(ptr, layout.size(), layout.align(), new_size)
+    }
+
+    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 {
+        self.0.lock().calloc(layout.size(), layout.align())
+    }
+}
+
+// Turn DlMalloc into an arena allocator
+struct DlmallocArena(spin::Mutex<bool>);
+
+unsafe impl dlmalloc::Allocator for DlmallocArena {
+    fn alloc(&self, _: usize) -> (*mut u8, usize, u32) {
+        let mut lock = self.0.lock();
+
+        if *lock {
+            (core::ptr::null_mut(), 0, 0)
+        } else {
+            *lock = true;
+            let align = std::mem::align_of::<usize>();
+            let heap_align_offset = unsafe { HEAP.as_mut_ptr() }.align_offset(align);
+            (unsafe { HEAP.as_mut_ptr().add(heap_align_offset) }, (HEAP_SIZE - heap_align_offset) / align * align, 1)
+        }
+    }
+
+    fn remap(&self, _ptr: *mut u8, _oldsize: usize, _newsize: usize, _can_move: bool) -> *mut u8 {
+        unimplemented!()
+    }
+
+    fn free_part(&self, _ptr: *mut u8, _oldsize: usize, _newsize: usize) -> bool {
+        unimplemented!()
+    }
+
+    fn free(&self, _ptr: *mut u8, _size: usize) -> bool {
+        true
+    }
+
+    fn can_release_part(&self, _flags: u32) -> bool {
+        false
+    }
+
+    fn allocates_zeros(&self) -> bool {
+        false
+    }
+
+    fn page_size(&self) -> usize {
+        4 * 1024
+    }
+}
+
+unsafe fn init_dlmalloc() -> Box<dyn GlobalAlloc + Sync> {
+    let dl = DlMallocator(spin::Mutex::new(
+        dlmalloc::Dlmalloc::new_with_allocator(DlmallocArena(spin::Mutex::new(false))),
+    ));
+    Box::new(dl)
+}
diff --git a/crates/talc/examples/std_global_allocator.rs b/crates/talc/examples/std_global_allocator.rs
new file mode 100644
index 0000000..64d4260
--- /dev/null
+++ b/crates/talc/examples/std_global_allocator.rs
@@ -0,0 +1,24 @@
+use talc::*;
+
+// note: miri thinks this violates stacked borrows upon program termination.
+// This only occurs if #[global_allocator] is used.
+// Use the allocator API if you can't have that.
+
+static mut START_ARENA: [u8; 10000] = [0; 10000];
+
+// The mutex provided by the `spin` crate is used here as it's a sensible choice
+
+// Allocations may occur prior to the execution of `main()`, thus support for
+// claiming memory on-demand is required, such as the ClaimOnOom OOM handler.
+
+#[global_allocator]
+static ALLOCATOR: Talck<spin::Mutex<()>, ClaimOnOom> = Talc::new(unsafe {
+        ClaimOnOom::new(Span::from_const_array(std::ptr::addr_of!(START_ARENA)))
+    }).lock();
+
+fn main() {
+    let mut vec = Vec::with_capacity(100);
+    vec.extend(0..300usize);
+    vec.truncate(100);
+    vec.shrink_to_fit();
+}
diff --git a/crates/talc/fuzz/.gitignore b/crates/talc/fuzz/.gitignore
new file mode 100644
index 0000000..1a45eee
--- /dev/null
+++ b/crates/talc/fuzz/.gitignore
@@ -0,0 +1,4 @@
+target
+corpus
+artifacts
+coverage
diff --git a/crates/talc/fuzz/Cargo.toml b/crates/talc/fuzz/Cargo.toml
new file mode 100644
index 0000000..9c1894c
--- /dev/null
+++ b/crates/talc/fuzz/Cargo.toml
@@ -0,0 +1,31 @@
+[package]
+name = "talc-fuzz"
+version = "0.0.0"
+publish = false
+edition = "2021"
+
+[package.metadata]
+cargo-fuzz = true
+
+[dependencies]
+libfuzzer-sys = "0.4"
+arbitrary = { version = "1", features = ["derive"] }
+spin = "0.9.8"
+rand = "0.8.5"
+
+[dependencies.talc]
+path = ".."
+features = ["fuzzing", "counters"]
+
+# Prevent this from interfering with workspaces
+[workspace]
+members = ["."]
+
+[profile.release]
+debug = 1
+
+[[bin]]
+name = "fuzz_talc"
+path = "fuzz_targets/fuzz_talc.rs"
+test = false
+doc = false
diff --git a/crates/talc/fuzz/fuzz_targets/fuzz_talc.rs b/crates/talc/fuzz/fuzz_targets/fuzz_talc.rs
new file mode 100644
index 0000000..eaf5aa1
--- /dev/null
+++ b/crates/talc/fuzz/fuzz_targets/fuzz_talc.rs
@@ -0,0 +1,151 @@
+#![no_main]
+
+#![feature(allocator_api)]
+#![feature(slice_ptr_get)]
+
+use std::alloc::{alloc, dealloc, GlobalAlloc, Layout};
+use std::ptr;
+
+use talc::*;
+
+use libfuzzer_sys::fuzz_target;
+
+use libfuzzer_sys::arbitrary::Arbitrary;
+
+#[derive(Arbitrary, Debug)]
+enum Actions {
+    /// Allocate memory with the given size and align of 1 << (align % 12)
+    Alloc { size: u16, align_bit: u8 },
+    /// Dealloc the ith allocation
+    Dealloc { index: u8 },
+    /// Realloc the ith allocation
+    Realloc { index: u8, new_size: u16 },
+    /// Claim a new segment of memory
+    Claim { offset: u16, size: u16, capacity: u16 },
+    // Extend the ith heap by the additional amount specified on the low and high side
+    Extend { index: u8, low: u16, high: u16 },
+    // Truncate the ith heap by the additional amount specified on the low and high side
+    Truncate { index: u8, low: u16, high: u16 },
+}
+use Actions::*;
+
+fuzz_target!(|actions: Vec<Actions>| {
+    let allocator = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();
+
+    let mut allocations: Vec<(*mut u8, Layout)> = vec![];
+    let mut heaps: Vec<(*mut u8, Layout, Span)> = vec![];
+
+    for action in actions {
+        match action {
+            Alloc { size, align_bit } => {
+                if size == 0 || align_bit > 12 { continue; }
+                //eprintln!("ALLOC | size: {:x} align: {:x}", size as usize, 1 << align_bit % 12);
+
+                let layout = Layout::from_size_align(size as usize, 1 << align_bit).unwrap();
+                let ptr = unsafe { allocator.alloc(layout) };
+
+                if ptr::null_mut() != ptr {
+                    allocations.push((ptr, layout));
+                    unsafe { ptr.write_bytes(0xab, layout.size()); }
+                }
+            }
+            Dealloc { index } => {
+                if index as usize >= allocations.len() { continue; }
+                
+                let (ptr, layout) = allocations[index as usize];
+                
+                //eprintln!("DEALLOC | ptr: {:p} size: {:x} align: {:x}", ptr, layout.size(), layout.align());
+                unsafe { allocator.dealloc(ptr, layout); }
+                allocations.swap_remove(index as usize);
+            }
+            Realloc { index, new_size } => {
+                if index as usize >= allocations.len() { continue; }
+                if new_size == 0 { continue; }
+
+                let (ptr, old_layout) = allocations[index as usize];
+                
+                //eprintln!("REALLOC | ptr: {:p} old size: {:x} old align: {:x} new_size: {:x}", ptr, old_layout.size(), old_layout.align(), new_size as usize);
+                
+                let new_layout = Layout::from_size_align(new_size as usize, old_layout.align()).unwrap();
+
+                let ptr = unsafe { allocator.realloc(ptr, old_layout, new_size as usize) };
+
+                if !ptr.is_null() {
+                    allocations[index as usize] = (ptr, new_layout);
+                    if old_layout.size() < new_size as usize {
+                        unsafe { ptr.add(old_layout.size()).write_bytes(0xcd, new_size as usize - old_layout.size()); }
+                    }
+                }
+            },
+            Claim { offset, size, capacity } => {
+                if capacity == 0 { continue; }
+
+                let capacity = capacity as usize;
+
+                let mem_layout = Layout::from_size_align(capacity, 1).unwrap();
+                let mem = unsafe { alloc(mem_layout) };
+                assert!(!mem.is_null());
+
+                let size = size as usize % capacity;
+                let offset = if size == capacity { 0 } else { offset as usize % (capacity - size) };
+
+                let heap = Span::from_base_size(mem, mem_layout.size())
+                    .truncate(offset, capacity - size + offset);
+                let heap = unsafe { allocator.lock().claim(heap) };
+
+                if let Ok(heap) = heap {
+                    heaps.push((mem, mem_layout, heap));
+                } else {
+                    unsafe { dealloc(mem, mem_layout); }
+                }
+            },
+            Extend { index, low, high } => {
+                //eprintln!("EXTEND | low: {} high: {} old arena {}", low, high, allocator.talc().get_arena());
+
+                let index = index as usize;
+                if index >= heaps.len() { continue; }
+
+                let (mem, mem_layout, old_heap) = heaps[index];
+
+                let new_heap = old_heap.extend(low as usize, high as usize)
+                    .fit_within(Span::from_base_size(mem, mem_layout.size()));
+                let new_heap = unsafe { allocator.lock().extend(old_heap, new_heap) };
+
+                heaps[index].2 = new_heap;
+            },
+            Truncate { index, low, high } => {
+                //eprintln!("TRUNCATE | low: {} high: {} old arena {}", low, high, allocator.talc().get_arena());
+
+                let index = index as usize;
+                if index >= heaps.len() { continue; }
+
+                let old_heap = heaps[index].2;
+
+                let mut talc = allocator.lock();
+
+                let new_heap = old_heap
+                    .truncate(low as usize, high as usize)
+                    .fit_over(unsafe { talc.get_allocated_span(old_heap) });
+                let new_heap = unsafe { talc.truncate(old_heap, new_heap) };
+
+                if new_heap.is_empty() {
+                    let (mem, mem_layout, _) = heaps.swap_remove(index);
+                    unsafe { dealloc(mem, mem_layout); }
+                } else {
+                    heaps[index].2 = new_heap;
+                }
+            }
+        }
+    }
+
+    // Free any remaining allocations.
+    for (ptr, layout) in allocations {
+        //eprintln!("DEALLOC FINAL | ptr: {:p} size: {:x} align: {:x}", ptr, layout.size(), layout.align());
+        unsafe { allocator.dealloc(ptr, layout); }
+    }
+
+    // drop the remaining heaps
+    for (mem, mem_layout, _) in heaps {
+        unsafe { dealloc(mem, mem_layout); }
+    }
+});
diff --git a/crates/talc/graph_microbench.py b/crates/talc/graph_microbench.py
new file mode 100644
index 0000000..541eec9
--- /dev/null
+++ b/crates/talc/graph_microbench.py
@@ -0,0 +1,68 @@
+import matplotlib.pyplot as plt
+from matplotlib.lines import Line2D
+import os
+
+BENCHMARK_RESULTS_DIR = 'benchmark_results/micro/'
+BENCHMARK_RESULT_GRAPHS_DIR = 'benchmark_result_graphs/'
+
+def get_benchmark_data(filename):
+    print("reading", filename)
+    with open(filename, 'r') as f:
+        rows = f.readlines()
+    
+    allocators = {}
+    for row in rows:
+        lst = row.strip().split(',')
+        if lst[1] != "":
+            allocators[lst[0]] = [float(i) for i in lst[1:]]
+    return allocators
+
+def plot_data():
+    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))
+
+    plot_count = 0
+    for filename in os.listdir(BENCHMARK_RESULTS_DIR):
+        filepath = BENCHMARK_RESULTS_DIR + filename
+        if not os.path.isfile(filepath):
+            continue
+
+        allocators = get_benchmark_data(filepath)
+        if len(allocators) == 0:
+            continue
+
+        print("plotting", filename)
+
+        ax = axs[plot_count]
+
+        ax.boxplot([allocators[x] for x in allocators], 
+            sym="", vert=False, showmeans=True, meanline=False, whis=(5, 95), 
+            meanprops=dict(marker="D", markerfacecolor="black", markeredgecolor="black"))
+
+        ax.set_title(filename.split(".")[0].title())
+        ax.set_xlim(left=0)
+        ax.set_yticks(range(1, len(allocators) + 1))
+        ax.set_yticklabels([x for x in allocators], rotation=45, ha="right")
+        ax.set_xlabel("Ticks")
+
+        legend_elements = [
+            Line2D([0], [0], color="orange", lw=1, label="median"),
+            Line2D([0], [0], marker="D", color="white", markerfacecolor="black", markeredgecolor="black", label="average")
+        ]
+        ax.legend(handles=legend_elements, loc="upper right")
+
+        plot_count += 1
+
+    fig.tight_layout()
+    plt.show()
+
+
+def main():
+    if not os.path.exists(BENCHMARK_RESULTS_DIR):
+        os.mkdir(BENCHMARK_RESULTS_DIR)
+    plot_data()
+
+    print("complete")
+
+if __name__ == '__main__':
+    main()
+
diff --git a/crates/talc/graph_random_actions.py b/crates/talc/graph_random_actions.py
new file mode 100644
index 0000000..f3550a4
--- /dev/null
+++ b/crates/talc/graph_random_actions.py
@@ -0,0 +1,40 @@
+import matplotlib.pyplot as plt
+import os
+
+BENCHMARK_RESULTS_DIR = 'benchmark_results/'
+BENCHMARK_RESULT_GRAPHS_DIR = 'benchmark_graphs/'
+
+def get_benchmark_data(filename):
+    with open(filename, 'r') as f:
+        rows = f.readlines()
+    allocators = {}
+    for row in rows:
+        lst = row.split(',')
+        allocators[lst[0]] = [float(i) for i in lst[1:]]
+    return allocators
+
+def main():
+    if not os.path.exists(BENCHMARK_RESULTS_DIR):
+        os.mkdir(BENCHMARK_RESULTS_DIR)
+
+    filename = "random_actions.csv"
+
+    xaxis = [i/10 for i in range(2, 10+1, 2)]
+    data = get_benchmark_data(BENCHMARK_RESULTS_DIR + filename)
+    yvalues = []
+    for k,v in data.items():
+        plt.plot(xaxis, v, label=k)
+        yvalues.append(v)
+
+    plt.legend()
+    test_name = filename[len(BENCHMARK_RESULTS_DIR): filename.find('.csv')]
+
+    plt.title("Random Actions Benchmark")
+    plt.xlabel('time (seconds)\n')
+    plt.ylabel('score')
+    plt.gca().set_ylim(bottom=0)
+    
+    plt.show()
+
+if __name__ == '__main__':
+    main()
diff --git a/crates/talc/rustfmt.toml b/crates/talc/rustfmt.toml
new file mode 100644
index 0000000..6e218dd
--- /dev/null
+++ b/crates/talc/rustfmt.toml
@@ -0,0 +1,2 @@
+use_small_heuristics = "Max"
+version = "Two"
diff --git a/crates/talc/src/lib.rs b/crates/talc/src/lib.rs
new file mode 100644
index 0000000..3d881b9
--- /dev/null
+++ b/crates/talc/src/lib.rs
@@ -0,0 +1,37 @@
+//! The Talc allocator crate.
+//!
+//! For getting started:
+//! - Check out the crate's [README](https://github.com/SFBdragon/talc)
+//! - Read check out the `Talc` and `Talck` structures.
+//!
+//! Your first step will be `Talc::new(...)`, then `claim`.
+//! Calling `Talc::lock()` on it will yield a `Talck` which implements
+//! [`GlobalAlloc`] and [`Allocator`] (if the appropriate feature flags are set).
+
+#![cfg_attr(not(any(test, fuzzing)), no_std)]
+#![cfg_attr(feature = "allocator", feature(allocator_api))]
+#![cfg_attr(feature = "nightly_api", feature(slice_ptr_len))]
+#![cfg_attr(feature = "nightly_api", feature(const_slice_ptr_len))]
+
+mod talc;
+mod span;
+mod oom_handler;
+mod ptr_utils;
+
+#[cfg(feature = "lock_api")]
+mod talck;
+#[cfg(feature = "lock_api")]
+pub mod locking;
+
+
+pub use oom_handler::{ClaimOnOom, ErrOnOom, OomHandler};
+pub use span::Span;
+pub use talc::Talc;
+
+#[cfg(feature = "lock_api")]
+pub use talck::Talck;
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+pub use talck::TalckWasm;
+
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+pub use oom_handler::WasmHandler;
diff --git a/crates/talc/src/locking.rs b/crates/talc/src/locking.rs
new file mode 100644
index 0000000..1e9488d
--- /dev/null
+++ b/crates/talc/src/locking.rs
@@ -0,0 +1,33 @@
+//! Note this only contains [`AssumeUnlockable`] which is not generally recommended.
+//! Use of the `spin` crate's mutex with [`Talck`](crate::Talc) is a good default.
+
+/// #### WARNING: [`AssumeUnlockable`] may cause undefined behaviour without `unsafe` code!
+/// 
+/// A dummy [`RawMutex`](lock_api::RawMutex) implementation to skip synchronization on single threaded systems.
+///
+/// # Safety
+/// [`AssumeUnlockable`] is highly unsafe and may cause undefined behaviour if multiple 
+/// threads enter a critical section it guards, even without explicit unsafe code.
+/// 
+/// Note that uncontended spin locks are cheap. Usage is only recommended on 
+/// platforms that don't have atomics or are exclusively single threaded.
+/// 
+/// Through no fault of its own, `lock_api`'s API does not allow for safe 
+/// encapsulation of this functionality. This is a hack for backwards compatibility.
+pub struct AssumeUnlockable;
+
+// SAFETY: nope
+unsafe impl lock_api::RawMutex for AssumeUnlockable {
+    const INIT: AssumeUnlockable = AssumeUnlockable;
+
+    // A spinlock guard can be sent to another thread and unlocked there
+    type GuardMarker = lock_api::GuardSend;
+
+    fn lock(&self) {}
+
+    fn try_lock(&self) -> bool {
+        true
+    }
+
+    unsafe fn unlock(&self) {}
+}
diff --git a/crates/talc/src/oom_handler.rs b/crates/talc/src/oom_handler.rs
new file mode 100644
index 0000000..f80d39f
--- /dev/null
+++ b/crates/talc/src/oom_handler.rs
@@ -0,0 +1,136 @@
+use core::alloc::Layout;
+
+use crate::{Span, Talc};
+
+pub trait OomHandler: Sized {
+    /// Given the allocator and the `layout` of the allocation that caused
+    /// OOM, resize or claim and return `Ok(())` or fail by returning `Err(())`.
+    ///
+    /// This function is called repeatedly if the allocator is still out of memory.
+    /// Therefore an infinite loop will occur if `Ok(())` is repeatedly returned
+    /// without extending or claiming new memory.
+    fn handle_oom(talc: &mut Talc<Self>, layout: Layout) -> Result<(), ()>;
+}
+
+/// Doesn't handle out-of-memory conditions, immediate allocation error occurs.
+pub struct ErrOnOom;
+
+impl OomHandler for ErrOnOom {
+    fn handle_oom(_: &mut Talc<Self>, _: Layout) -> Result<(), ()> {
+        Err(())
+    }
+}
+
+/// An out-of-memory handler that attempts to claim the
+/// memory within the given [`Span`] upon OOM.
+///
+/// The contained span is then overwritten with an empty span.
+///
+/// If the span is empty or `claim` fails, allocation failure occurs.
+pub struct ClaimOnOom(Span);
+
+impl ClaimOnOom {
+    /// # Safety
+    /// The memory within the given [`Span`] must conform to
+    /// the requirements laid out by [`claim`](Talc::claim).
+    pub const unsafe fn new(span: Span) -> Self {
+        ClaimOnOom(span)
+    }
+}
+
+impl OomHandler for ClaimOnOom {
+    fn handle_oom(talc: &mut Talc<Self>, _: Layout) -> Result<(), ()> {
+        if !talc.oom_handler.0.is_empty() {
+            unsafe {
+                talc.claim(talc.oom_handler.0)?;
+            }
+
+            talc.oom_handler.0 = Span::empty();
+
+            Ok(())
+        } else {
+            Err(())
+        }
+    }
+}
+
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+pub struct WasmHandler {
+    prev_heap: Span,
+}
+
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+unsafe impl Send for WasmHandler {}
+
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+impl WasmHandler {
+    /// Create a new WASM handler.
+    /// # Safety
+    /// [`WasmHandler`] expects to have full control over WASM memory
+    /// and be running in a single-threaded environment.
+    pub const unsafe fn new() -> Self {
+        Self { prev_heap: Span::empty() }
+    }
+}
+
+#[cfg(all(target_family = "wasm", feature = "lock_api"))]
+impl OomHandler for WasmHandler {
+    fn handle_oom(talc: &mut Talc<Self>, layout: Layout) -> Result<(), ()> {
+        /// WASM page size is 64KiB
+        const PAGE_SIZE: usize = 1024 * 64;
+
+        // growth strategy: just try to grow enough to avoid OOM again on this allocation
+        let required = (layout.size() + 8).max(layout.align() * 2);
+        let mut delta_pages = (required + (PAGE_SIZE - 1)) / PAGE_SIZE;
+
+        let prev = 'prev: {
+            // This performs a scan, trying to find a smaller possible
+            // growth if the previous one was unsuccessful. Return
+            // any successful allocated to memory.
+            // If not quite enough, talc will invoke handle_oom again.
+
+            // if we're about to fail because of allocation failure
+            // we may as well try as hard as we can to probe what's permissable
+            // which can be done with a log2(n)-ish algorithm
+            // (factoring in repeated called to handle_oom)
+            while delta_pages != 0 {
+                // use `core::arch::wasm` instead once it doesn't
+                // require the unstable feature wasm_simd64?
+                let result = core::arch::wasm32::memory_grow::<0>(delta_pages);
+
+                if result != usize::MAX {
+                    break 'prev result;
+                } else {
+                    delta_pages >>= 1;
+                    continue;
+                }
+            }
+
+            return Err(());
+        };
+
+        let prev_heap_acme = (prev * PAGE_SIZE) as *mut u8;
+        let new_heap_acme = prev_heap_acme.wrapping_add(delta_pages * PAGE_SIZE);
+
+        // try to get base & acme, which will fail if prev_heap is empty
+        // otherwise the allocator has been initialized previously
+        if let Some((prev_base, prev_acme)) = talc.oom_handler.prev_heap.get_base_acme() {
+            if prev_acme == prev_heap_acme {
+                talc.oom_handler.prev_heap = unsafe {
+                    talc.extend(talc.oom_handler.prev_heap, Span::new(prev_base, new_heap_acme))
+                };
+
+                return Ok(());
+            }
+        }
+
+        talc.oom_handler.prev_heap = unsafe {
+            // delta_pages is always greater than zero
+            // thus one page is enough space for metadata
+            // therefore we can unwrap the result
+            talc.claim(Span::new(prev_heap_acme, new_heap_acme)).unwrap()
+        };
+
+        Ok(())
+    }
+}
diff --git a/crates/talc/src/ptr_utils.rs b/crates/talc/src/ptr_utils.rs
new file mode 100644
index 0000000..6e953a1
--- /dev/null
+++ b/crates/talc/src/ptr_utils.rs
@@ -0,0 +1,67 @@
+//! Generic utilities for pointer handling and sizing.
+
+pub const WORD_SIZE: usize = core::mem::size_of::<usize>();
+pub const WORD_BITS: usize = usize::BITS as usize;
+pub const ALIGN: usize = core::mem::align_of::<usize>();
+
+/// Aligns `ptr` up to the next `align_mask + 1`.
+///
+/// `align_mask` must be a power of two minus one.
+#[inline]
+pub fn align_up_by(ptr: *mut u8, align_mask: usize) -> *mut u8 {
+    debug_assert!((align_mask + 1).is_power_of_two());
+
+    // this incantation maintains provenance of ptr
+    // while allowing the compiler to see through the wrapping_add and optimize it
+    ptr.wrapping_add(((ptr as usize + align_mask) & !align_mask) - ptr as usize)
+    // equivalent to the following:
+    // ((ptr as usize + align_mask) & !align_mask) as *mut u8
+    // i.e. just align up to the next align_mask + 1
+}
+
+pub fn align_down(ptr: *mut u8) -> *mut u8 {
+    ptr.wrapping_sub(ptr as usize % ALIGN)
+}
+pub fn align_up_overflows(ptr: *mut u8) -> bool {
+    ALIGN - 1 > usize::MAX - ptr as usize
+}
+pub fn align_up(ptr: *mut u8) -> *mut u8 {
+    debug_assert!(!align_up_overflows(ptr));
+
+    let offset_ptr = ptr.wrapping_add(ALIGN - 1);
+    offset_ptr.wrapping_sub(offset_ptr as usize % ALIGN)
+}
+
+#[cfg(test)]
+mod tests {
+    use core::ptr::null_mut;
+
+    use super::*;
+
+    #[test]
+    fn align_ptr_test() {
+        assert!(!align_up_overflows(null_mut()));
+        assert!(!align_up_overflows(null_mut::<u8>().wrapping_sub(ALIGN)));
+        assert!(align_up_overflows(null_mut::<u8>().wrapping_sub(ALIGN - 1)));
+        assert!(align_up_overflows(null_mut::<u8>().wrapping_sub(ALIGN - 2)));
+        assert!(align_up_overflows(null_mut::<u8>().wrapping_sub(ALIGN - 3)));
+
+        assert!(align_up(null_mut()) == null_mut());
+        assert!(align_down(null_mut()) == null_mut());
+
+        assert!(align_up(null_mut::<u8>().wrapping_add(1)) == null_mut::<u8>().wrapping_add(ALIGN));
+        assert!(align_up(null_mut::<u8>().wrapping_add(2)) == null_mut::<u8>().wrapping_add(ALIGN));
+        assert!(align_up(null_mut::<u8>().wrapping_add(3)) == null_mut::<u8>().wrapping_add(ALIGN));
+        assert!(
+            align_up(null_mut::<u8>().wrapping_add(ALIGN)) == null_mut::<u8>().wrapping_add(ALIGN)
+        );
+
+        assert!(align_down(null_mut::<u8>().wrapping_add(1)) == null_mut::<u8>());
+        assert!(align_down(null_mut::<u8>().wrapping_add(2)) == null_mut::<u8>());
+        assert!(align_down(null_mut::<u8>().wrapping_add(3)) == null_mut::<u8>());
+        assert!(
+            align_down(null_mut::<u8>().wrapping_add(ALIGN))
+                == null_mut::<u8>().wrapping_add(ALIGN)
+        );
+    }
+}
diff --git a/crates/talc/src/span.rs b/crates/talc/src/span.rs
new file mode 100644
index 0000000..c1e209d
--- /dev/null
+++ b/crates/talc/src/span.rs
@@ -0,0 +1,394 @@
+use core::ops::Range;
+
+use crate::ptr_utils::*;
+
+/// Represents an interval of memory `[base, acme)`
+///
+/// Use `get_base_acme` to retrieve `base` and `acme` directly.
+///
+/// # Empty Spans
+/// Note that where `base >= acme`, the [`Span`] is considered empty, in which case
+/// the specific values of `base` and `acme` are considered meaningless.
+/// * Empty spans contain nothing and overlap with nothing.
+/// * Empty spans are contained by any sized span.
+#[derive(Clone, Copy, Hash)]
+pub struct Span {
+    base: *mut u8,
+    acme: *mut u8,
+}
+
+unsafe impl Send for Span {}
+
+impl Default for Span {
+    fn default() -> Self {
+        Self::empty()
+    }
+}
+
+impl core::fmt::Debug for Span {
+    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
+        f.write_fmt(format_args!("{:p}..[{}]..{:p}", self.base, self.size(), self.acme))
+    }
+}
+
+impl core::fmt::Display for Span {
+    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
+        match self.get_base_acme() {
+            Some((base, acme)) => f.write_fmt(format_args!("{:p}..{:p}", base, acme)),
+            None => f.write_str("Empty Span"),
+        }
+    }
+}
+
+impl<T> From<Range<*mut T>> for Span {
+    fn from(value: Range<*mut T>) -> Self {
+        Self { base: value.start.cast(), acme: value.end.cast() }
+    }
+}
+
+impl<T> From<Range<*const T>> for Span {
+    fn from(value: Range<*const T>) -> Self {
+        Self { base: value.start.cast_mut().cast(), acme: value.end.cast_mut().cast() }
+    }
+}
+
+impl<T> From<&mut [T]> for Span {
+    fn from(value: &mut [T]) -> Self {
+        Self::from(value.as_mut_ptr_range())
+    }
+}
+
+impl<T> From<&[T]> for Span {
+    fn from(value: &[T]) -> Self {
+        Self::from(value.as_ptr_range())
+    }
+}
+
+impl<T, const N: usize> From<&mut [T; N]> for Span {
+    fn from(value: &mut [T; N]) -> Self {
+        Self::from(value as *mut [T; N])
+    }
+}
+
+impl<T, const N: usize> From<&[T; N]> for Span {
+    fn from(value: &[T; N]) -> Self {
+        Self::from(value as *const [T; N])
+    }
+}
+
+#[cfg(feature = "nightly_api")]
+impl<T> From<*mut [T]> for Span {
+    fn from(value: *mut [T]) -> Self {
+        Self::from_slice(value)
+    }
+}
+
+#[cfg(feature = "nightly_api")]
+impl<T> From<*const [T]> for Span {
+    fn from(value: *const [T]) -> Self {
+        Self::from_const_slice(value)
+    }
+}
+
+impl<T, const N: usize> From<*mut [T; N]> for Span {
+    fn from(value: *mut [T; N]) -> Self {
+        Self::from_array(value)
+    }
+}
+
+impl<T, const N: usize> From<*const [T; N]> for Span {
+    fn from(value: *const [T; N]) -> Self {
+        Self::from_array(value.cast_mut())
+    }
+}
+
+impl PartialEq for Span {
+    fn eq(&self, other: &Self) -> bool {
+        self.is_empty() && other.is_empty() || self.base == other.base && self.acme == other.acme
+    }
+}
+impl Eq for Span {}
+
+impl Span {
+    /// Returns whether `base >= acme`.
+    #[inline]
+    pub fn is_empty(self) -> bool {
+        self.acme <= self.base
+    }
+
+    /// Returns whether `base < acme`.
+    #[inline]
+    pub fn is_sized(self) -> bool {
+        !self.is_empty()
+    }
+
+    /// Returns the size of the span, else zero if `base >= span`.
+    #[inline]
+    pub fn size(self) -> usize {
+        if self.is_empty() { 0 } else { self.acme as usize - self.base as usize }
+    }
+
+    /// If `self` isn't empty, returns `(base, acme)`
+    #[inline]
+    pub fn get_base_acme(self) -> Option<(*mut u8, *mut u8)> {
+        if self.is_empty() { None } else { Some((self.base, self.acme)) }
+    }
+
+    /// Create an empty span.
+    #[inline]
+    pub const fn empty() -> Self {
+        Self { base: core::ptr::null_mut(), acme: core::ptr::null_mut() }
+    }
+
+    /// Create a new span.
+    #[inline]
+    pub const fn new(base: *mut u8, acme: *mut u8) -> Self {
+        Self { base, acme }
+    }
+
+    /// Creates a [`Span`] given a `base` and a `size`.
+    ///
+    /// If `base + size` overflows, the result is empty.
+    #[inline]
+    pub const fn from_base_size(base: *mut u8, size: usize) -> Self {
+        Self { base, acme: base.wrapping_add(size) }
+    }
+
+    #[cfg(feature = "nightly_api")]
+    #[inline]
+    pub const fn from_slice<T>(slice: *mut [T]) -> Self {
+        Self {
+            base: slice as *mut T as *mut u8,
+            // SAFETY: pointing directly after an object is considered
+            // within the same object
+            acme: unsafe { (slice as *mut T).add(slice.len()).cast() },
+        }
+    }
+
+    #[cfg(feature = "nightly_api")]
+    #[inline]
+    pub const fn from_const_slice<T>(slice: *const [T]) -> Self {
+        Self {
+            base: slice as *mut T as *mut u8,
+            // SAFETY: pointing directly after an object is considered
+            // within the same object
+            acme: unsafe { (slice as *mut T).add(slice.len()).cast() },
+        }
+    }
+
+    #[inline]
+    pub const fn from_array<T, const N: usize>(array: *mut [T; N]) -> Self {
+        Self {
+            base: array as *mut T as *mut u8,
+            // SAFETY: pointing directly after an object is considered
+            // within the same object
+            acme: unsafe { (array as *mut T).add(N).cast() },
+        }
+    }
+
+    #[inline]
+    pub const fn from_const_array<T, const N: usize>(array: *const [T; N]) -> Self {
+        Self {
+            base: array as *mut T as *mut u8,
+            // SAFETY: pointing directly after an object is considered
+            // within the same object
+            acme: unsafe { (array as *mut T).add(N).cast() },
+        }
+    }
+
+    /// Returns `None` if `self` is empty.
+    #[inline]
+    pub fn to_ptr_range(self) -> Option<Range<*mut u8>> {
+        if self.is_empty() { None } else { Some(self.base..self.acme) }
+    }
+
+    /// Returns `None` if `self` is empty.
+    #[inline]
+    pub fn to_slice(self) -> Option<*mut [u8]> {
+        if self.is_empty() {
+            None
+        } else {
+            Some(core::ptr::slice_from_raw_parts_mut(self.base, self.size()))
+        }
+    }
+
+    /// Returns whether `self` contains `addr`.
+    ///
+    /// Empty spans contain nothing.
+    #[inline]
+    pub fn contains(self, ptr: *mut u8) -> bool {
+        // if self is empty, this always evaluates to false
+        self.base <= ptr && ptr < self.acme
+    }
+
+    /// Returns whether `self` contains `other`.
+    ///
+    /// Empty spans are contained by any span, even empty ones.
+    #[inline]
+    pub fn contains_span(self, other: Span) -> bool {
+        other.is_empty() || self.base <= other.base && other.acme <= self.acme
+    }
+
+    /// Returns whether some of `self` overlaps with `other`.
+    ///
+    /// Empty spans don't overlap with anything.
+    #[inline]
+    pub fn overlaps(self, other: Span) -> bool {
+        self.is_sized() && other.is_sized() && !(other.base >= self.acme || self.base >= other.acme)
+    }
+
+    /// Aligns `base` upward and `acme` downward by `align_of::<usize>()`.
+    #[inline]
+    pub fn word_align_inward(self) -> Self {
+        if ALIGN > usize::MAX - self.base as usize {
+            Self::empty()
+        } else {
+            Self { base: align_up(self.base), acme: align_down(self.acme) }
+        }
+    }
+    /// Aligns `base` downward and `acme` upward by `align_of::<usize>()`.
+    #[inline]
+    pub fn word_align_outward(self) -> Self {
+        if ALIGN > usize::MAX - self.acme as usize {
+            panic!("aligning acme upward would overflow!");
+        }
+
+        Self { base: align_down(self.base), acme: align_up(self.acme) }
+    }
+
+    /// Raises `base` if `base` is smaller than `min`.
+    #[inline]
+    pub fn above(self, min: *mut u8) -> Self {
+        Self { base: if min > self.base { min } else { self.base }, acme: self.acme }
+    }
+    /// Lowers `acme` if `acme` is greater than `max`.
+    #[inline]
+    pub fn below(self, max: *mut u8) -> Self {
+        Self { base: self.base, acme: if max < self.acme { max } else { self.acme } }
+    }
+    /// Returns a span that `other` contains by raising `base` or lowering `acme`.
+    ///
+    /// If `other` is empty, returns `other`.
+    #[inline]
+    pub fn fit_within(self, other: Span) -> Self {
+        if other.is_empty() {
+            other
+        } else {
+            Self {
+                base: if other.base > self.base { other.base } else { self.base },
+                acme: if other.acme < self.acme { other.acme } else { self.acme },
+            }
+        }
+    }
+    /// Returns a span that contains `other` by extending `self`.
+    ///
+    /// If `other` is empty, returns `self`, as all spans contain any empty span.
+    #[inline]
+    pub fn fit_over(self, other: Self) -> Self {
+        if other.is_empty() {
+            self
+        } else {
+            Self {
+                base: if other.base < self.base { other.base } else { self.base },
+                acme: if other.acme > self.acme { other.acme } else { self.acme },
+            }
+        }
+    }
+
+    /// Lower `base` by `low` and raise `acme` by `high`.
+    ///
+    /// Does nothing if `self` is empty.
+    ///
+    /// # Panics
+    /// Panics if lowering `base` by `low` or raising `acme` by `high` under/overflows.
+    #[inline]
+    pub fn extend(self, low: usize, high: usize) -> Self {
+        if self.is_empty() {
+            self
+        } else {
+            assert!((self.base as usize).checked_sub(low).is_some());
+            assert!((self.acme as usize).checked_add(high).is_some());
+
+            Self { base: self.base.wrapping_sub(low), acme: self.acme.wrapping_add(high) }
+        }
+    }
+
+    /// Raise `base` by `low` and lower `acme` by `high`.
+    ///
+    /// If `self` is empty, `self` is returned.
+    ///
+    /// If either operation would wrap around the address space, an empty span is returned.
+    #[inline]
+    pub fn truncate(self, low: usize, high: usize) -> Span {
+        if self.is_empty() {
+            self
+        } else if (self.base as usize).checked_add(low).is_none()
+            || (self.acme as usize).checked_sub(high).is_none()
+        {
+            Span::empty()
+        } else {
+            Self {
+                // if either boundary saturates, the span will be empty thereafter, as expected
+                base: self.base.wrapping_add(low),
+                acme: self.acme.wrapping_sub(high),
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+
+    fn ptr(addr: usize) -> *mut u8 {
+        // don't ` as usize` to avoid upsetting miri too much
+        core::ptr::null_mut::<u8>().wrapping_add(addr)
+    }
+
+    #[test]
+    fn test_span() {
+        let base = 1234usize;
+        let acme = 5678usize;
+
+        let bptr = ptr(base);
+        let aptr = ptr(acme);
+
+        let span = Span::from(bptr..aptr);
+        assert!(!span.is_empty());
+        assert!(span.size() == acme - base);
+
+        assert!(
+            span.word_align_inward()
+                == Span::new(
+                    bptr.wrapping_add(ALIGN - 1)
+                        .wrapping_sub(bptr.wrapping_add(ALIGN - 1) as usize & (ALIGN - 1)),
+                    aptr.wrapping_sub(acme & (ALIGN - 1))
+                )
+        );
+        assert!(
+            span.word_align_outward()
+                == Span::new(
+                    bptr.wrapping_sub(base & (ALIGN - 1)),
+                    aptr.wrapping_add(ALIGN - 1)
+                        .wrapping_sub(aptr.wrapping_add(ALIGN - 1) as usize & (ALIGN - 1))
+                )
+        );
+
+        assert!(span.above(ptr(2345)) == Span::new(ptr(2345), aptr));
+        assert!(span.below(ptr(7890)) == Span::new(bptr, aptr));
+        assert!(span.below(ptr(3456)) == Span::new(bptr, ptr(3456)));
+        assert!(span.below(ptr(0123)).is_empty());
+        assert!(span.above(ptr(7890)).is_empty());
+
+        assert!(span.fit_over(Span::empty()) == span);
+        assert!(span.fit_within(Span::empty()).is_empty());
+        assert!(span.fit_within(Span::new(ptr(0), ptr(10000))) == span);
+        assert!(span.fit_over(Span::new(ptr(0), ptr(10000))) == Span::new(ptr(0), ptr(10000)));
+        assert!(span.fit_within(Span::new(ptr(4000), ptr(10000))) == Span::new(ptr(4000), aptr));
+        assert!(span.fit_over(Span::new(ptr(4000), ptr(10000))) == Span::new(bptr, ptr(10000)));
+
+        assert!(span.extend(1234, 1010) == Span::new(ptr(0), ptr(5678 + 1010)));
+        assert!(span.truncate(1234, 1010) == Span::new(ptr(1234 + 1234), ptr(5678 - 1010)));
+        assert!(span.truncate(235623, 45235772).is_empty());
+    }
+}
diff --git a/crates/talc/src/talc.rs b/crates/talc/src/talc.rs
new file mode 100644
index 0000000..80a013c
--- /dev/null
+++ b/crates/talc/src/talc.rs
@@ -0,0 +1,1184 @@
+mod llist;
+mod tag;
+
+#[cfg(feature = "counters")]
+pub mod counters;
+
+use crate::{ptr_utils::*, OomHandler, Span};
+use core::{
+    alloc::Layout,
+    ptr::{null_mut, NonNull},
+};
+use llist::LlistNode;
+use tag::Tag;
+
+const NODE_SIZE: usize = core::mem::size_of::<LlistNode>();
+const TAG_SIZE: usize = core::mem::size_of::<Tag>();
+
+const MIN_TAG_OFFSET: usize = NODE_SIZE;
+const MIN_CHUNK_SIZE: usize = MIN_TAG_OFFSET + TAG_SIZE;
+const MIN_HEAP_SIZE: usize = MIN_CHUNK_SIZE + TAG_SIZE;
+
+const BIN_COUNT: usize = usize::BITS as usize * 2;
+
+type Bin = Option<NonNull<LlistNode>>;
+
+// Free chunk (3x ptr size minimum):
+//   ?? | NODE: LlistNode (2 * ptr), SIZE: usize, ..???.., SIZE: usize | ??
+// Reserved chunk (1x ptr size of overhead):
+//   ?? |       ???????         , TAG: Tag (ptr) | ??
+
+// TAG contains a pointer to the bottom of the reserved chunk,
+// a is_allocated (set) bit flag differentiating itself from a free chunk
+// (the LlistNode contains well-aligned pointers, thus does not have that bit set),
+// as well as a is_low_free bit flag which does what is says on the tin
+
+const GAP_NODE_OFFSET: usize = 0;
+const GAP_LOW_SIZE_OFFSET: usize = NODE_SIZE;
+const GAP_HIGH_SIZE_OFFSET: usize = WORD_SIZE;
+
+// WASM perf tanks if these #[inline]'s are not present
+#[inline]
+unsafe fn gap_base_to_node(base: *mut u8) -> *mut LlistNode {
+    base.add(GAP_NODE_OFFSET).cast()
+}
+#[inline]
+unsafe fn gap_base_to_size(base: *mut u8) -> *mut usize {
+    base.add(GAP_LOW_SIZE_OFFSET).cast()
+}
+#[inline]
+unsafe fn gap_base_to_acme(base: *mut u8) -> *mut u8 {
+    gap_base_to_acme_size(base).0
+}
+#[inline]
+unsafe fn gap_base_to_acme_size(base: *mut u8) -> (*mut u8, usize) {
+    let size = gap_base_to_size(base).read();
+    (base.add(size), size)
+}
+#[inline]
+unsafe fn gap_acme_to_size(acme: *mut u8) -> *mut usize {
+    acme.sub(GAP_HIGH_SIZE_OFFSET).cast()
+}
+#[inline]
+unsafe fn gap_acme_to_base(acme: *mut u8) -> *mut u8 {
+    gap_acme_to_base_size(acme).0
+}
+#[inline]
+unsafe fn gap_acme_to_base_size(acme: *mut u8) -> (*mut u8, usize) {
+    let size = gap_acme_to_size(acme).read();
+    (acme.sub(size), size)
+}
+#[inline]
+unsafe fn gap_node_to_base(node: NonNull<LlistNode>) -> *mut u8 {
+    node.as_ptr().cast::<u8>().sub(GAP_NODE_OFFSET).cast()
+}
+#[inline]
+unsafe fn gap_node_to_size(node: NonNull<LlistNode>) -> *mut usize {
+    node.as_ptr().cast::<u8>().sub(GAP_NODE_OFFSET).add(GAP_LOW_SIZE_OFFSET).cast()
+}
+#[inline]
+unsafe fn is_gap_below(acme: *mut u8) -> bool {
+    // gap size will never have bit 1 set, but a tag will
+    gap_acme_to_size(acme).read() & Tag::ALLOCATED_FLAG == 0
+}
+#[inline]
+unsafe fn is_gap_above_heap_base(heap_base: *mut u8) -> bool {
+    // there's a tag at every heap base
+    heap_base.cast::<Tag>().read().is_above_free()
+}
+
+/// Determines the tag pointer and retrieves the tag, given the allocated pointer.
+#[inline]
+unsafe fn tag_from_alloc_ptr(ptr: *mut u8, size: usize) -> (*mut u8, Tag) {
+    let post_alloc_ptr = align_up(ptr.add(size));
+    // we're either reading a tag_ptr or a Tag with the base pointer + metadata in the low bits
+    let tag_or_tag_ptr = post_alloc_ptr.cast::<*mut u8>().read();
+
+    // if the pointer is greater, it's a tag_ptr
+    // if it's less, it's a tag, effectively a base pointer
+    // (the low bits of metadata in a tag don't effect the inequality)
+    if tag_or_tag_ptr > post_alloc_ptr {
+        (tag_or_tag_ptr, tag_or_tag_ptr.cast::<Tag>().read())
+    } else {
+        (post_alloc_ptr, Tag(tag_or_tag_ptr))
+    }
+}
+
+/// Returns whether the two pointers are greater than `MIN_CHUNK_SIZE` apart.
+#[inline]
+fn is_chunk_size(base: *mut u8, acme: *mut u8) -> bool {
+    debug_assert!(acme >= base, "!(acme {:p} >= base {:p})", acme, base);
+    acme as usize - base as usize >= MIN_CHUNK_SIZE
+}
+
+/// `size` should be larger or equal to MIN_CHUNK_SIZE
+#[inline]
+unsafe fn bin_of_size(size: usize) -> usize {
+    // this mess determines the bucketing strategy used by the allocator
+    // the default is to have a bucket per multiple of word size from the minimum
+    // chunk size up to WORD_BUCKETED_SIZE and double word gap (sharing two sizes)
+    // up to DOUBLE_BUCKETED_SIZE, and from there on use pseudo-logarithmic sizes.
+
+    // such sizes are as follows: begin at some power of two (DOUBLE_BUCKETED_SIZE)
+    // and increase by some power of two fraction (quarters, on 64 bit machines)
+    // until reaching the next power of two, and repeat:
+    // e.g. begin at 32, increase by quarters: 32, 40, 48, 56, 64, 80, 96, 112, 128, ...
+
+    // note to anyone adding support for another word size: use buckets.py to figure it out
+    const ERRMSG: &str = "Unsupported system word size, open an issue/create a PR!";
+
+    /// up to what size do we use a bin for every multiple of a word
+    const WORD_BIN_LIMIT: usize = match WORD_SIZE {
+        8 => 256,
+        4 => 64,
+        _ => panic!("{}", ERRMSG),
+    };
+    /// up to what size beyond that do we use a bin for every multiple of a doubleword
+    const DOUBLE_BIN_LIMIT: usize = match WORD_SIZE {
+        8 => 512,
+        4 => 128,
+        _ => panic!("{}", ERRMSG),
+    };
+    /// how many buckets are linearly spaced among each power of two magnitude (how many divisions)
+    const DIVS_PER_POW2: usize = match WORD_SIZE {
+        8 => 4,
+        4 => 2,
+        _ => panic!("{}", ERRMSG),
+    };
+    /// how many bits are used to determine the division
+    const DIV_BITS: usize = DIVS_PER_POW2.ilog2() as usize;
+
+    /// the bucket index at which the doubleword separated buckets start
+    const DBL_BUCKET: usize = (WORD_BIN_LIMIT - MIN_CHUNK_SIZE) / WORD_SIZE;
+    /// the bucket index at which the peudo-exponentially separated buckets start
+    const EXP_BUCKET: usize = DBL_BUCKET + (DOUBLE_BIN_LIMIT - WORD_BIN_LIMIT) / WORD_SIZE / 2;
+    /// Log 2 of (minimum pseudo-exponential chunk size)
+    const MIN_EXP_BITS_LESS_ONE: usize = DOUBLE_BIN_LIMIT.ilog2() as usize;
+
+    debug_assert!(size >= MIN_CHUNK_SIZE);
+
+    if size < WORD_BIN_LIMIT {
+        // single word separated bucket
+
+        (size - MIN_CHUNK_SIZE) / WORD_SIZE
+    } else if size < DOUBLE_BIN_LIMIT {
+        // double word separated bucket
+
+        // equiv to (size - WORD_BIN_LIMIT) / 2WORD_SIZE + DBL_BUCKET
+        // but saves an instruction
+        size / (2 * WORD_SIZE) - WORD_BIN_LIMIT / (2 * WORD_SIZE) + DBL_BUCKET
+    } else {
+        // pseudo-exponentially separated bucket
+
+        // here's what a size is, bit by bit: 1_div_extra
+        // e.g. with four divisions 1_01_00010011000
+        // the bucket is determined by the magnitude and the division
+        // mag 0 div 0, mag 0 div 1, mag 0 div 2, mag 0 div 3, mag 1 div 0, ...
+
+        let bits_less_one = size.ilog2() as usize;
+
+        // the magnitude the size belongs to.
+        // calculate the difference in bit count i.e. difference in power
+        let magnitude = bits_less_one - MIN_EXP_BITS_LESS_ONE;
+        // the division of the magnitude the size belongs to.
+        // slide the size to get the division bits at the bottom and remove the top bit
+        let division = (size >> (bits_less_one - DIV_BITS)) - DIVS_PER_POW2;
+        // the index into the pseudo-exponential buckets.
+        let bucket_offset = magnitude * DIVS_PER_POW2 + division;
+
+        // cap the max bucket at the last bucket
+        (bucket_offset + EXP_BUCKET).min(BIN_COUNT - 1)
+    }
+}
+
+/// The Talc Allocator!
+///
+/// One way to get started:
+/// 1. Construct with [`new`](Talc::new) (supply [`ErrOnOom`] to ignore OOM handling).
+/// 2. Establish any number of heaps with [`claim`](Talc::claim).
+/// 3. Call [`lock`](Talc::lock) to get a [`Talck`] which supports the
+/// [`GlobalAlloc`](core::alloc::GlobalAlloc) and [`Allocator`](core::alloc::Allocator) traits.
+///
+/// Check out the associated functions `new`, `claim`, `lock`, `extend`, and `truncate`.
+pub struct Talc<O: OomHandler> {
+    /// The low bits of the availability flags.
+    availability_low: usize,
+    /// The high bits of the availability flags.
+    availability_high: usize,
+    /// Linked list heads.
+    bins: *mut Bin,
+
+    /// The user-specified OOM handler.
+    ///
+    /// Its state is entirely maintained by the user.
+    pub oom_handler: O,
+
+    #[cfg(feature = "counters")]
+    /// Allocation stats.
+    counters: counters::Counters,
+}
+
+unsafe impl<O: Send + OomHandler> Send for Talc<O> {}
+
+impl<O: OomHandler> core::fmt::Debug for Talc<O> {
+    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
+        f.debug_struct("Talc")
+            .field("availability_low", &format_args!("{:x}", self.availability_low))
+            .field("availability_high", &format_args!("{:x}", self.availability_high))
+            .field("metadata_ptr", &self.bins)
+            .finish()
+    }
+}
+
+impl<O: OomHandler> Talc<O> {
+    #[inline]
+    const fn required_chunk_size(size: usize) -> usize {
+        if size <= MIN_CHUNK_SIZE - TAG_SIZE {
+            MIN_CHUNK_SIZE
+        } else {
+            (size + TAG_SIZE + (ALIGN - 1)) & !(ALIGN - 1)
+        }
+    }
+
+    /// Get the pointer to the `bin`th bin.
+    /// # Safety
+    /// `bin` must be smaller than `BIN_COUNT`.
+    #[inline]
+    unsafe fn get_bin_ptr(&self, bin: usize) -> *mut Bin {
+        debug_assert!(bin < BIN_COUNT);
+
+        self.bins.add(bin)
+    }
+
+    /// Sets the availability flag for bin `b`.
+    ///
+    /// This is done when a chunk is added to an empty bin.
+    #[inline]
+    fn set_avails(&mut self, b: usize) {
+        debug_assert!(b < BIN_COUNT);
+
+        if b < WORD_BITS {
+            debug_assert!(self.availability_low & 1 << b == 0);
+            self.availability_low ^= 1 << b;
+        } else {
+            debug_assert!(self.availability_high & 1 << (b - WORD_BITS) == 0);
+            self.availability_high ^= 1 << (b - WORD_BITS);
+        }
+    }
+    /// Clears the availability flag for bin `b`.
+    ///
+    /// This is done when a bin becomes empty.
+    #[inline]
+    fn clear_avails(&mut self, b: usize) {
+        debug_assert!(b < BIN_COUNT);
+
+        // if head is the last node
+        if b < WORD_BITS {
+            self.availability_low ^= 1 << b;
+            debug_assert!(self.availability_low & 1 << b == 0);
+        } else {
+            self.availability_high ^= 1 << (b - WORD_BITS);
+            debug_assert!(self.availability_high & 1 << (b - WORD_BITS) == 0);
+        }
+    }
+
+    /// Registers a gap in memory which is allocatable.
+    #[inline]
+    unsafe fn register_gap(&mut self, base: *mut u8, acme: *mut u8) {
+        debug_assert!(is_chunk_size(base, acme));
+
+        let size = acme as usize - base as usize;
+        let bin = bin_of_size(size);
+
+        let bin_ptr = self.get_bin_ptr(bin);
+
+        if (*bin_ptr).is_none() {
+            self.set_avails(bin);
+        }
+
+        LlistNode::insert(gap_base_to_node(base), bin_ptr, *bin_ptr);
+
+        debug_assert!((*bin_ptr).is_some());
+
+        gap_base_to_size(base).write(size);
+        gap_acme_to_size(acme).write(size);
+
+        #[cfg(feature = "counters")]
+        self.counters.account_register_gap(size);
+    }
+
+    /// Deregisters memory, not allowing it to be allocated.
+    #[inline]
+    unsafe fn deregister_gap(&mut self, base: *mut u8, bin: usize) {
+        debug_assert!((*self.get_bin_ptr(bin)).is_some());
+        #[cfg(feature = "counters")]
+        self.counters.account_deregister_gap(gap_base_to_size(base).read());
+
+        LlistNode::remove(gap_base_to_node(base));
+
+        if (*self.get_bin_ptr(bin)).is_none() {
+            self.clear_avails(bin);
+        }
+    }
+
+    /// Allocate a contiguous region of memory according to `layout`, if possible.
+    /// # Safety
+    /// `layout.size()` must be nonzero.
+    pub unsafe fn malloc(&mut self, layout: Layout) -> Result<NonNull<u8>, ()> {
+        debug_assert!(layout.size() != 0);
+        self.scan_for_errors();
+
+        let (mut free_base, free_acme, alloc_base) = loop {
+            // this returns None if there are no heaps or allocatable memory
+            match self.get_sufficient_chunk(layout) {
+                Some(payload) => break payload,
+                None => _ = O::handle_oom(self, layout)?,
+            }
+        };
+
+        // determine the base of the allocated chunk
+        // if the amount of memory below the chunk is too small, subsume it, else free it
+        let chunk_base_ceil = alloc_base.min(free_acme.sub(MIN_CHUNK_SIZE));
+        if is_chunk_size(free_base, chunk_base_ceil) {
+            self.register_gap(free_base, chunk_base_ceil);
+            free_base = chunk_base_ceil;
+        } else {
+            Tag::clear_above_free(free_base.sub(TAG_SIZE).cast());
+        }
+
+        // the word immediately after the allocation
+        let post_alloc_ptr = align_up(alloc_base.add(layout.size()));
+        // the tag position, accounting for the minimum size of a chunk
+        let mut tag_ptr = free_base.add(MIN_TAG_OFFSET).max(post_alloc_ptr);
+        // the pointer after the lowest possible tag pointer
+        let min_alloc_chunk_acme = tag_ptr.add(TAG_SIZE);
+
+        // handle the space above the required allocation span
+        if is_chunk_size(min_alloc_chunk_acme, free_acme) {
+            self.register_gap(min_alloc_chunk_acme, free_acme);
+            Tag::write(tag_ptr.cast(), free_base, true);
+        } else {
+            tag_ptr = free_acme.sub(TAG_SIZE);
+            Tag::write(tag_ptr.cast(), free_base, false);
+        }
+
+        if tag_ptr != post_alloc_ptr {
+            // write the real tag ptr where the tag is expected to be
+            post_alloc_ptr.cast::<*mut u8>().write(tag_ptr);
+        }
+
+        #[cfg(feature = "counters")]
+        self.counters.account_alloc(layout.size());
+
+        Ok(NonNull::new_unchecked(alloc_base))
+    }
+
+    /// Returns `(chunk_base, chunk_acme, alloc_base)`
+    unsafe fn get_sufficient_chunk(
+        &mut self,
+        layout: Layout,
+    ) -> Option<(*mut u8, *mut u8, *mut u8)> {
+        let required_chunk_size = Self::required_chunk_size(layout.size());
+
+        // if there are no valid heaps, availability is zero, and next_available_bin returns None
+        let mut bin = self.next_available_bin(bin_of_size(required_chunk_size))?;
+
+        if layout.align() <= ALIGN {
+            // the required alignment is most often the machine word size (or less)
+            // a faster loop without alignment checking is used in this case
+            loop {
+                for node_ptr in LlistNode::iter_mut(*self.get_bin_ptr(bin)) {
+                    let size = gap_node_to_size(node_ptr).read();
+
+                    // if the chunk size is sufficient, remove from bookkeeping data structures and return
+                    if size >= required_chunk_size {
+                        let base = gap_node_to_base(node_ptr);
+                        self.deregister_gap(base, bin);
+                        return Some((base, base.add(size), base));
+                    }
+                }
+
+                bin = self.next_available_bin(bin + 1)?;
+            }
+        } else {
+            // a larger than word-size alignment is demanded
+            // therefore each chunk is manually checked to be sufficient accordingly
+            let align_mask = layout.align() - 1;
+            let required_size = layout.size() + TAG_SIZE;
+
+            loop {
+                for node_ptr in LlistNode::iter_mut(*self.get_bin_ptr(bin)) {
+                    let size = gap_node_to_size(node_ptr).read();
+
+                    if size >= required_chunk_size {
+                        let base = gap_node_to_base(node_ptr);
+                        let acme = base.add(size);
+                        // calculate the lowest aligned pointer above the tag-offset free chunk pointer
+                        let aligned_ptr = align_up_by(base, align_mask);
+
+                        // if the remaining size is sufficient, remove the chunk from the books and return
+                        if aligned_ptr.add(required_size) <= acme {
+                            self.deregister_gap(base, bin);
+                            return Some((base, acme, aligned_ptr));
+                        }
+                    }
+                }
+
+                bin = self.next_available_bin(bin + 1)?;
+            }
+        }
+    }
+
+    #[inline(always)]
+    fn next_available_bin(&self, next_bin: usize) -> Option<usize> {
+        if next_bin < usize::BITS as usize {
+            // shift flags such that only flags for larger buckets are kept
+            let shifted_avails = self.availability_low >> next_bin;
+
+            // find the next up, grab from the high flags, or quit
+            if shifted_avails != 0 {
+                Some(next_bin + shifted_avails.trailing_zeros() as usize)
+            } else if self.availability_high != 0 {
+                Some(self.availability_high.trailing_zeros() as usize + WORD_BITS)
+            } else {
+                None
+            }
+        } else if next_bin < BIN_COUNT {
+            // similar process to the above, but the low flags are irrelevant
+            let shifted_avails = self.availability_high >> (next_bin - WORD_BITS);
+
+            if shifted_avails != 0 {
+                Some(next_bin + shifted_avails.trailing_zeros() as usize)
+            } else {
+                return None;
+            }
+        } else {
+            None
+        }
+    }
+
+    /// Free previously allocated/reallocated memory.
+    /// # Safety
+    /// `ptr` must have been previously allocated given `layout`.
+    pub unsafe fn free(&mut self, ptr: NonNull<u8>, layout: Layout) {
+        self.scan_for_errors();
+        #[cfg(feature = "counters")]
+        self.counters.account_dealloc(layout.size());
+
+        let (tag_ptr, tag) = tag_from_alloc_ptr(ptr.as_ptr(), layout.size());
+        let mut chunk_base = tag.chunk_base();
+        let mut chunk_acme = tag_ptr.add(TAG_SIZE);
+
+        debug_assert!(tag.is_allocated());
+        debug_assert!(is_chunk_size(chunk_base, chunk_acme));
+
+        // try recombine below
+        if is_gap_below(chunk_base) {
+            let (below_base, below_size) = gap_acme_to_base_size(chunk_base);
+            self.deregister_gap(below_base, bin_of_size(below_size));
+
+            chunk_base = below_base;
+        } else {
+            Tag::set_above_free(chunk_base.sub(TAG_SIZE).cast());
+        }
+
+        // try recombine above
+        if tag.is_above_free() {
+            let above_size = gap_base_to_size(chunk_acme).read();
+            self.deregister_gap(chunk_acme, bin_of_size(above_size));
+
+            chunk_acme = chunk_acme.add(above_size);
+        }
+
+        // add the full recombined free chunk back into the books
+        self.register_gap(chunk_base, chunk_acme);
+    }
+
+    /// Grow a previously allocated/reallocated region of memory to `new_size`.
+    /// # Safety
+    /// `ptr` must have been previously allocated or reallocated given `layout`.
+    /// `new_size` must be larger or equal to `layout.size()`.
+    pub unsafe fn grow(
+        &mut self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_size: usize,
+    ) -> Result<NonNull<u8>, ()> {
+
+        match self.grow_in_place(ptr, old_layout, new_size) {
+            Err(_) => {
+                // grow in-place failed; reallocate the slow way
+                let new_layout = Layout::from_size_align_unchecked(new_size, old_layout.align());
+                let allocation = self.malloc(new_layout)?;
+                allocation.as_ptr().copy_from_nonoverlapping(ptr.as_ptr(), old_layout.size());
+                self.free(ptr, old_layout);
+    
+                Ok(allocation)
+            }
+            res => res,
+        }
+    }
+
+    /// Attempt to grow a previously allocated/reallocated region of memory to `new_size`.
+    /// 
+    /// Returns `Err` if reallocation could not occur in-place. 
+    /// Ownership of the memory remains with the caller.
+    /// # Safety
+    /// `ptr` must have been previously allocated or reallocated given `layout`.
+    /// `new_size` must be larger or equal to `layout.size()`.
+    pub unsafe fn grow_in_place(
+        &mut self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_size: usize,
+    ) -> Result<NonNull<u8>, ()> {
+        debug_assert!(new_size >= old_layout.size());
+        self.scan_for_errors();
+
+        let old_post_alloc_ptr = align_up(ptr.as_ptr().add(old_layout.size()));
+        let new_post_alloc_ptr = align_up(ptr.as_ptr().add(new_size));
+
+        if old_post_alloc_ptr == new_post_alloc_ptr {
+            // this handles a rare short-circuit, but more helpfully
+            // also guarantees that we'll never need to add padding to
+            // reach minimum chunk size with new_tag_ptr later as
+            // min alloc size (1) rounded up to (WORD) + post_alloc_ptr (WORD) + new_tag_ptr (WORD) >= MIN_CHUNK_SIZE
+
+            #[cfg(feature = "counters")]
+            self.counters.account_grow_in_place(old_layout.size(), new_size);
+
+            return Ok(ptr);
+        }
+
+        let (tag_ptr, tag) = tag_from_alloc_ptr(ptr.as_ptr(), old_layout.size());
+
+        // tag_ptr may be greater where extra free space needed to be reserved
+        if new_post_alloc_ptr <= tag_ptr {
+            if new_post_alloc_ptr < tag_ptr {
+                new_post_alloc_ptr.cast::<*mut u8>().write(tag_ptr);
+            }
+
+            #[cfg(feature = "counters")]
+            self.counters.account_grow_in_place(old_layout.size(), new_size);
+
+            return Ok(ptr);
+        }
+
+        let new_tag_ptr = new_post_alloc_ptr;
+
+        let base = tag.chunk_base();
+        let acme = tag_ptr.add(TAG_SIZE);
+
+        debug_assert!(tag.is_allocated());
+        debug_assert!(is_chunk_size(base, acme));
+
+        // otherwise, check if 1) is free 2) is large enough
+        // because free chunks don't border free chunks, this needn't be recursive
+        if tag.is_above_free() {
+            let above_size = gap_base_to_size(acme).read();
+            let above_tag_ptr = tag_ptr.add(above_size);
+
+            if new_tag_ptr <= above_tag_ptr {
+                self.deregister_gap(acme, bin_of_size(above_size));
+
+                // finally, determine if the remainder of the free block is big enough
+                // to be freed again, or if the entire region should be allocated
+                if is_chunk_size(new_tag_ptr, above_tag_ptr) {
+                    self.register_gap(new_tag_ptr.add(TAG_SIZE), above_tag_ptr.add(TAG_SIZE));
+                    Tag::write(new_tag_ptr.cast(), base, true);
+                } else {
+                    Tag::write(above_tag_ptr.cast(), base, false);
+
+                    if new_post_alloc_ptr != above_tag_ptr {
+                        new_post_alloc_ptr.cast::<*mut u8>().write(above_tag_ptr);
+                    }
+                }
+
+                #[cfg(feature = "counters")]
+                self.counters.account_grow_in_place(old_layout.size(), new_size);
+
+                return Ok(ptr);
+            }
+        }
+
+        Err(())
+    }
+
+    /// Shrink a previously allocated/reallocated region of memory to `new_size`.
+    ///
+    /// This function is infallible given valid inputs, and the reallocation will always be
+    /// done in-place, maintaining the validity of the pointer.
+    ///
+    /// # Safety
+    /// - `ptr` must have been previously allocated or reallocated given `layout`.
+    /// - `new_size` must be smaller or equal to `layout.size()`.
+    /// - `new_size` should be nonzero.
+    pub unsafe fn shrink(&mut self, ptr: NonNull<u8>, layout: Layout, new_size: usize) {
+        debug_assert!(new_size != 0);
+        debug_assert!(new_size <= layout.size());
+        self.scan_for_errors();
+
+        let (tag_ptr, tag) = tag_from_alloc_ptr(ptr.as_ptr(), layout.size());
+        let chunk_base = tag.chunk_base();
+
+        debug_assert!(tag.is_allocated());
+        debug_assert!(is_chunk_size(chunk_base, tag_ptr.add(TAG_SIZE)));
+
+        // the word immediately after the allocation
+        let new_post_alloc_ptr = align_up(ptr.as_ptr().add(new_size));
+        // the tag position, accounting for the minimum size of a chunk
+        let mut new_tag_ptr = chunk_base.add(MIN_TAG_OFFSET).max(new_post_alloc_ptr);
+
+        // if the remainder between the new required size and the originally allocated
+        // size is large enough, free the remainder, otherwise leave it
+        if is_chunk_size(new_tag_ptr, tag_ptr) {
+            let mut acme = tag_ptr.add(TAG_SIZE);
+            let new_acme = new_tag_ptr.add(TAG_SIZE);
+
+            if tag.is_above_free() {
+                let above_size = gap_base_to_size(acme).read();
+                self.deregister_gap(acme, bin_of_size(above_size));
+
+                acme = acme.add(above_size);
+            }
+
+            self.register_gap(new_acme, acme);
+            Tag::write(new_tag_ptr.cast(), chunk_base, true);
+        } else {
+            new_tag_ptr = tag_ptr;
+        }
+
+        if new_tag_ptr != new_post_alloc_ptr {
+            new_post_alloc_ptr.cast::<*mut u8>().write(new_tag_ptr);
+        }
+
+        #[cfg(feature = "counters")]
+        self.counters.account_shrink_in_place(layout.size(), new_size);
+    }
+
+    /// Returns an uninitialized [`Talc`].
+    ///
+    /// If you don't want to handle OOM, use [`ErrOnOom`].
+    ///
+    /// In order to make this allocator useful, `claim` some memory.
+    pub const fn new(oom_handler: O) -> Self {
+        Self {
+            oom_handler,
+            availability_low: 0,
+            availability_high: 0,
+            bins: null_mut(),
+
+            #[cfg(feature = "counters")]
+            counters: counters::Counters::new(),
+        }
+    }
+
+    /// Returns the minimum [`Span`] containing this heap's allocated memory.
+    /// # Safety
+    /// `heap` must be the return value of a heap manipulation function.
+    pub unsafe fn get_allocated_span(&self, heap: Span) -> Span {
+        assert!(heap.size() >= MIN_HEAP_SIZE);
+
+        let (mut base, mut acme) = heap.get_base_acme().unwrap();
+
+        // check for free space at the heap's top
+        if is_gap_below(acme) {
+            acme = gap_acme_to_base(acme);
+        }
+
+        // check for free memory at the bottom of the heap using the base tag
+        if is_gap_above_heap_base(base) {
+            base = gap_base_to_acme(base.add(TAG_SIZE)).sub(TAG_SIZE);
+        }
+
+        // base might be greater that acme for an empty heap
+        // but that's fine, this'll just become an empty span
+        Span::new(base, acme)
+    }
+
+    /// Attempt to initialize a new heap for the allocator.
+    ///
+    /// Note:
+    /// * Each heap reserves a `usize` at the bottom as fixed overhead.
+    /// * Metadata will be placed into the bottom of the first successfully established heap.
+    /// It is currently ~1KiB on 64-bit systems (less on 32-bit). This is subject to change.
+    ///
+    /// # Return Values
+    /// The resulting [`Span`] is the actual heap extent, and may
+    /// be slightly smaller than requested. Use this to resize the heap.
+    /// Any memory outside the claimed heap is free to use.
+    ///
+    /// Returns [`Err`] where
+    /// * allocator metadata is not yet established, and there's insufficient memory to do so.
+    /// * allocator metadata is established, but the heap is too small
+    /// (less than around `4 * usize` for now).
+    ///
+    /// # Safety
+    /// - The memory within the `memory` must be valid for reads and writes,
+    /// and memory therein (when not allocated to the user) must not be mutated
+    /// while the allocator is in use.
+    /// - `memory` should not overlap with any other active heap.
+    ///
+    /// # Panics
+    /// Panics if `memory` contains the null address.
+    pub unsafe fn claim(&mut self, memory: Span) -> Result<Span, ()> {
+        self.scan_for_errors();
+
+        const BIN_ARRAY_SIZE: usize = core::mem::size_of::<Bin>() * BIN_COUNT;
+
+        // create a new heap
+        // if bins is null, we will need to try put the metadata in this heap
+        // this metadata is allocated 'by hand' to be isomorphic with other chunks
+
+        assert!(!memory.contains(null_mut()), "heap covers the null address!");
+
+        let aligned_heap = memory.word_align_inward();
+
+        // if this fails, there's no space to work with
+        if let Some((base, acme)) = aligned_heap.get_base_acme() {
+            // check if the allocator has already successfully placed its metadata
+            if !self.bins.is_null() {
+                // check if there's enough space to establish a free chunk
+                if acme as usize - base as usize >= MIN_HEAP_SIZE {
+                    // write in the base tag
+                    Tag::write(base.cast(), null_mut(), true);
+
+                    // register the free memory
+                    let chunk_base = base.wrapping_add(TAG_SIZE);
+                    self.register_gap(chunk_base, acme);
+                    
+                    self.scan_for_errors();
+
+                    #[cfg(feature = "counters")]
+                    self.counters.account_claim(aligned_heap.size());
+
+                    return Ok(aligned_heap);
+                }
+            } else {
+                // check if there's enough space to allocate metadata and establish a free chunk
+                if acme as usize - base as usize >= TAG_SIZE + BIN_ARRAY_SIZE + TAG_SIZE {
+                    Tag::write(base.cast(), null_mut(), false);
+
+                    // align the metadata pointer against the base of the heap
+                    let metadata_ptr = base.add(TAG_SIZE);
+                    // align the tag pointer against the top of the metadata
+                    let post_metadata_ptr = metadata_ptr.add(BIN_ARRAY_SIZE);
+
+                    // initialize the bins to None
+                    for i in 0..BIN_COUNT {
+                        let bin_ptr = metadata_ptr.cast::<Bin>().add(i);
+                        bin_ptr.write(None);
+                    }
+
+                    self.bins = metadata_ptr.cast::<Bin>();
+
+                    // check whether there's enough room on top to free
+                    // add_chunk_to_record only depends on self.bins
+                    let metadata_chunk_acme = post_metadata_ptr.add(TAG_SIZE);
+                    if is_chunk_size(metadata_chunk_acme, acme) {
+                        self.register_gap(metadata_chunk_acme, acme);
+                        Tag::write(post_metadata_ptr.cast(), base, true);
+                    } else {
+                        let tag_ptr = acme.sub(TAG_SIZE).cast::<Tag>();
+
+                        if tag_ptr != post_metadata_ptr.cast() {
+                            post_metadata_ptr.cast::<*mut Tag>().write(tag_ptr);
+                        }
+                        Tag::write(tag_ptr, base, false);
+                    }
+
+                    self.scan_for_errors();
+
+                    #[cfg(feature = "counters")]
+                    self.counters.account_claim(aligned_heap.size());
+
+                    return Ok(aligned_heap);
+                }
+            }
+        }
+
+        // fallthrough from insufficient size
+
+        Err(())
+    }
+
+    /// Increase the extent of a heap. The new extent of the heap is returned,
+    /// and will be equal to or slightly smaller than requested.
+    ///
+    /// # Safety
+    /// - `old_heap` must be the return value of a heap-manipulation function
+    /// of this allocator instance.
+    /// - The entire `req_heap` memory but be readable and writable
+    /// and unmutated besides that which is allocated so long as the heap is in use.
+    ///
+    /// # Panics
+    /// This function panics if:
+    /// - `old_heap` is too small or heap metadata is not yet allocated
+    /// - `req_heap` doesn't contain `old_heap`
+    /// - `req_heap` contains the null address
+    ///
+    /// A recommended pattern for satisfying these criteria is:
+    /// ```rust
+    /// # use talc::*;
+    /// # let mut talc = Talc::new(ErrOnOom);
+    /// let mut heap = [0u8; 2000];
+    /// let old_heap = Span::from(&mut heap[300..1700]);
+    /// let old_heap = unsafe { talc.claim(old_heap).unwrap() };
+    ///
+    /// // compute the new heap span as an extension of the old span
+    /// let new_heap = old_heap.extend(250, 500).fit_within((&mut heap[..]).into());
+    ///
+    /// // SAFETY: be sure not to extend into memory we can't use
+    /// let new_heap = unsafe { talc.extend(old_heap, new_heap) };
+    /// ```
+    pub unsafe fn extend(&mut self, old_heap: Span, req_heap: Span) -> Span {
+        assert!(!self.bins.is_null());
+        assert!(old_heap.size() >= MIN_HEAP_SIZE);
+        assert!(req_heap.contains_span(old_heap), "new_heap must contain old_heap");
+        assert!(!req_heap.contains(null_mut()), "new_heap covers the null address!");
+
+        self.scan_for_errors();
+
+        let (old_base, old_acme) = old_heap.word_align_inward().get_base_acme().unwrap();
+        let (new_base, new_acme) = req_heap.word_align_inward().get_base_acme().unwrap();
+        let new_chunk_base = new_base.add(TAG_SIZE);
+        let mut ret_base = new_base;
+        let mut ret_acme = new_acme;
+
+        // if the top chunk is free, extend the block to cover the new extra area
+        // otherwise allocate above if possible
+        if is_gap_below(old_acme) {
+            let (top_base, top_size) = gap_acme_to_base_size(old_acme);
+            self.deregister_gap(top_base, bin_of_size(top_size));
+            self.register_gap(top_base, new_acme);
+        } else if is_chunk_size(old_acme, new_acme) {
+            self.register_gap(old_acme, new_acme);
+            Tag::set_above_free(old_acme.sub(TAG_SIZE).cast());
+        } else {
+            ret_acme = old_acme;
+        }
+
+        // extend the bottom chunk if it's free, else add free chunk below if possible
+        if is_gap_above_heap_base(old_base) {
+            let bottom_base = old_base.add(TAG_SIZE);
+            let bottom_size = gap_base_to_size(bottom_base).read();
+            self.deregister_gap(bottom_base, bin_of_size(bottom_size));
+            self.register_gap(new_chunk_base, bottom_base.add(bottom_size));
+            Tag::write(new_base.cast(), null_mut(), true);
+        } else if is_chunk_size(new_base, old_base) {
+            self.register_gap(new_base.add(TAG_SIZE), old_base.add(TAG_SIZE));
+            Tag::write(new_base.cast(), null_mut(), true);
+        } else {
+            ret_base = old_base;
+        }
+
+        let ret_heap = Span::new(ret_base, ret_acme);
+
+        #[cfg(feature = "counters")]
+        self.counters.account_extend(old_heap.size(), ret_heap.size());
+
+        ret_heap
+    }
+
+    /// Reduce the extent of a heap.
+    /// The new extent must encompass all current allocations. See below.
+    ///
+    /// The resultant heap is always equal to or slightly smaller than `req_heap`.
+    ///
+    /// Truncating to an empty [`Span`] is valid for heaps where no memory is
+    /// currently allocated within it.
+    ///
+    /// In all cases where the return value is empty, the heap no longer exists.
+    /// You may do what you like with the heap memory. The empty span should not be
+    /// used as input to [`truncate`](Talc::truncate), [`extend`](Talc::extend),
+    /// or [`get_allocated_span`](Talc::get_allocated_span).
+    ///
+    /// # Safety
+    /// `old_heap` must be the return value of a heap-manipulation function
+    /// of this allocator instance.
+    ///
+    /// # Panics:
+    /// This function panics if:
+    /// - `old_heap` doesn't contain `req_heap`
+    /// - `req_heap` doesn't contain all the allocated memory in `old_heap`
+    /// - the heap metadata is not yet allocated, see [`claim`](Talc::claim)
+    ///
+    /// # Usage
+    ///
+    /// A recommended pattern for satisfying these criteria is:
+    /// ```rust
+    /// # use talc::*;
+    /// # let mut talc = Talc::new(ErrOnOom);
+    /// let mut heap = [0u8; 2000];
+    /// let old_heap = Span::from(&mut heap[300..1700]);
+    /// let old_heap = unsafe { talc.claim(old_heap).unwrap() };
+    ///
+    /// // note: lock a `Talck` here otherwise a race condition may occur
+    /// // in between Talc::get_allocated_span and Talc::truncate
+    ///
+    /// // compute the new heap span as a truncation of the old span
+    /// let new_heap = old_heap
+    ///     .truncate(250, 300)
+    ///     .fit_over(unsafe { talc.get_allocated_span(old_heap) });
+    ///
+    /// // truncate the heap
+    /// unsafe { talc.truncate(old_heap, new_heap); }
+    /// ```
+    pub unsafe fn truncate(&mut self, old_heap: Span, req_heap: Span) -> Span {
+        assert!(!self.bins.is_null(), "no heaps have been successfully established!");
+
+        self.scan_for_errors();
+
+        let new_heap = req_heap.word_align_inward();
+
+        // check that the new_heap is valid
+        assert!(old_heap.contains_span(new_heap), "the old_heap must contain new_heap!");
+        assert!(
+            new_heap.contains_span(unsafe { self.get_allocated_span(old_heap) }),
+            "new_heap must contain all the heap's allocated memory! see `get_allocated_span`"
+        );
+
+        let (old_base, old_acme) = old_heap.get_base_acme().unwrap();
+        let old_chunk_base = old_base.add(TAG_SIZE);
+
+        // if the entire heap is decimated, just return an empty span
+        if new_heap.size() < MIN_HEAP_SIZE {
+            self.deregister_gap(
+                old_chunk_base,
+                bin_of_size(old_acme as usize - old_chunk_base as usize),
+            );
+
+            #[cfg(feature = "counters")]
+            self.counters.account_truncate(old_heap.size(), 0);
+
+            return Span::empty();
+        }
+
+        let (new_base, new_acme) = new_heap.get_base_acme().unwrap();
+        let new_chunk_base = new_base.add(TAG_SIZE);
+        let mut ret_base = new_base;
+        let mut ret_acme = new_acme;
+
+        // trim the top
+        if new_acme < old_acme {
+            let (top_base, top_size) = gap_acme_to_base_size(old_acme);
+            self.deregister_gap(top_base, bin_of_size(top_size));
+
+            if is_chunk_size(top_base, new_acme) {
+                self.register_gap(top_base, new_acme);
+            } else {
+                ret_acme = top_base;
+                Tag::clear_above_free(top_base.sub(TAG_SIZE).cast());
+            }
+        }
+
+        // no need to check if the entire heap vanished;
+        // we eliminated this possibility earlier
+
+        // trim the bottom
+        if old_base < new_base {
+            debug_assert!(is_gap_above_heap_base(old_base));
+
+            let (bottom_acme, bottom_size) = gap_base_to_acme_size(old_chunk_base);
+            self.deregister_gap(old_chunk_base, bin_of_size(bottom_size));
+
+            if is_chunk_size(new_chunk_base, bottom_acme) {
+                self.register_gap(new_chunk_base, bottom_acme);
+                Tag::write(new_base.cast(), null_mut(), true);
+            } else {
+                ret_base = bottom_acme.sub(TAG_SIZE);
+                Tag::write(ret_base.cast(), null_mut(), false);
+            }
+        }
+
+        let ret_heap = Span::new(ret_base, ret_acme);
+
+        #[cfg(feature = "counters")]
+        self.counters.account_truncate(old_heap.size(), ret_heap.size());
+
+        ret_heap
+    }
+
+    #[cfg(not(debug_assertions))]
+    fn scan_for_errors(&self) {}
+
+    #[cfg(debug_assertions)]
+    /// Debugging function for checking various assumptions.
+    fn scan_for_errors(&self) {
+        #[cfg(any(test, fuzzing))]
+        let mut vec = std::vec::Vec::<Span>::new();
+
+        if !self.bins.is_null() {
+            for b in 0..BIN_COUNT {
+                let mut any = false;
+                unsafe {
+                    for node in LlistNode::iter_mut(*self.get_bin_ptr(b)) {
+                        any = true;
+                        if b < WORD_BITS {
+                            assert!(self.availability_low & 1 << b != 0);
+                        } else {
+                            assert!(self.availability_high & 1 << (b - WORD_BITS) != 0);
+                        }
+
+                        let base = gap_node_to_base(node);
+                        let (acme, size) = gap_base_to_acme_size(base);
+                        let low_size = gap_acme_to_size(acme).read();
+                        assert!(low_size == size);
+
+                        let lower_tag = base.sub(TAG_SIZE).cast::<Tag>().read();
+                        assert!(lower_tag.is_allocated());
+                        assert!(lower_tag.is_above_free());
+
+                        #[cfg(any(test, fuzzing))]
+                        {
+                            let span = Span::new(base, acme);
+                            //dbg!(span);
+                            for other in &vec {
+                                assert!(!span.overlaps(*other), "{} intersects {}", span, other);
+                            }
+                            vec.push(span);
+                        }
+                    }
+                }
+
+                if !any {
+                    if b < WORD_BITS {
+                        assert!(self.availability_low & 1 << b == 0);
+                    } else {
+                        assert!(self.availability_high & 1 << (b - WORD_BITS) == 0);
+                    }
+                }
+            }
+        } else {
+            assert!(self.availability_low == 0);
+            assert!(self.availability_high == 0);
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn alignment_assumptions_hold() {
+        // claim assumes this
+        assert!(ALIGN == std::mem::align_of::<Bin>() && ALIGN == std::mem::size_of::<Bin>());
+    }
+
+    #[test]
+    fn alloc_dealloc_test() {
+        const ARENA_SIZE: usize = 10000000;
+
+        let arena = Box::leak(vec![0u8; ARENA_SIZE].into_boxed_slice()) as *mut [_];
+
+        let mut talc = Talc::new(crate::ErrOnOom);
+
+        unsafe {
+            talc.claim(arena.as_mut().unwrap().into()).unwrap();
+        }
+
+        let layout = Layout::from_size_align(1243, 8).unwrap();
+
+        let a = unsafe { talc.malloc(layout) };
+        assert!(a.is_ok());
+        unsafe {
+            a.unwrap().as_ptr().write_bytes(255, layout.size());
+        }
+
+        let mut x = vec![NonNull::dangling(); 100];
+
+        for _ in 0..1 {
+            for i in 0..100 {
+                let allocation = unsafe { talc.malloc(layout) };
+                assert!(allocation.is_ok());
+                unsafe {
+                    allocation.unwrap().as_ptr().write_bytes(0xab, layout.size());
+                }
+                x[i] = allocation.unwrap();
+            }
+
+            for i in 0..50 {
+                unsafe {
+                    talc.free(x[i], layout);
+                }
+            }
+            for i in (50..100).rev() {
+                unsafe {
+                    talc.free(x[i], layout);
+                }
+            }
+        }
+
+        unsafe {
+            talc.free(a.unwrap(), layout);
+        }
+
+        unsafe {
+            drop(Box::from_raw(arena));
+        }
+    }
+
+    #[test]
+    fn claim_truncate_extend_test() {
+        // not big enough to fit the metadata
+        let mut tiny_heap = [0u8; BIN_COUNT * WORD_SIZE / 2];
+        let tiny_heap_span: Span = Span::from(&mut tiny_heap);
+
+        // big enough with plenty of extra
+        let big_heap = Box::leak(vec![0u8; BIN_COUNT * WORD_SIZE + 100000].into_boxed_slice());
+        let big_heap_span = Span::from(big_heap.as_mut());
+
+        let mut talc = Talc::new(crate::ErrOnOom);
+
+        unsafe {
+            talc.claim(tiny_heap_span).unwrap_err();
+        }
+
+        assert!(talc.bins.is_null());
+        assert!(talc.availability_low == 0 && talc.availability_high == 0);
+
+        let alloc_big_heap = unsafe { talc.claim(big_heap_span).unwrap() };
+
+        assert!(!talc.bins.is_null());
+
+        let alloc_big_heap = unsafe {
+            talc.truncate(
+                alloc_big_heap,
+                alloc_big_heap.truncate(500, 500).fit_over(talc.get_allocated_span(alloc_big_heap)),
+            )
+        };
+
+        let _alloc_tiny_heap = unsafe { talc.claim(tiny_heap_span).unwrap() };
+
+        let allocation = unsafe {
+            let allocation = talc.malloc(Layout::new::<u128>()).unwrap();
+            allocation.as_ptr().write_bytes(0, Layout::new::<u128>().size());
+            allocation
+        };
+
+        let alloc_big_heap = unsafe {
+            talc.truncate(
+                alloc_big_heap,
+                alloc_big_heap
+                    .truncate(100000, 100000)
+                    .fit_over(talc.get_allocated_span(alloc_big_heap)),
+            )
+        };
+
+        unsafe {
+            talc.extend(
+                alloc_big_heap,
+                alloc_big_heap.extend(10000, 10000).fit_within(big_heap_span),
+            );
+        }
+
+        unsafe {
+            talc.free(allocation, Layout::new::<u128>());
+        }
+
+        unsafe {
+            drop(Box::from_raw(big_heap));
+        }
+    }
+}
diff --git a/crates/talc/src/talc/counters.rs b/crates/talc/src/talc/counters.rs
new file mode 100644
index 0000000..0685402
--- /dev/null
+++ b/crates/talc/src/talc/counters.rs
@@ -0,0 +1,210 @@
+//! Track allocation counters for Talc.
+
+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Default)]
+pub struct Counters {
+    /// Number of active allocations.
+    pub allocation_count: usize,
+    /// Total number of allocations.
+    pub total_allocation_count: u64,
+
+    /// Sum of active allocations' layouts' size.
+    pub allocated_bytes: usize,
+    /// Sum of all allocations' layouts' maximum size.
+    /// 
+    /// In-place reallocations's unchanged bytes are not recounted.
+    pub total_allocated_bytes: u64,
+
+    /// Number of bytes available for allocation.
+    pub available_bytes: usize,
+    /// Number of holes/gaps between allocations.
+    pub fragment_count: usize,
+
+    /// Number of active established heaps.
+    pub heap_count: usize,
+    /// Total number of established heaps.
+    pub total_heap_count: u64,
+
+    /// Sum of bytes actively claimed.
+    pub claimed_bytes: usize,
+    /// Sum of bytes ever claimed. Reclaimed bytes included.
+    pub total_claimed_bytes: u64,
+}
+
+impl Counters {
+    pub const fn new() -> Self {
+        Self {
+            allocation_count: 0,
+            total_allocation_count: 0,
+            allocated_bytes: 0,
+            total_allocated_bytes: 0,
+            available_bytes: 0,
+            fragment_count: 0,
+            heap_count: 0,
+            total_heap_count: 0,
+            claimed_bytes: 0,
+            total_claimed_bytes: 0,
+        }
+    }
+
+    /// Returns the number of bytes unavailable due to padding/metadata/etc.
+    pub const fn overhead_bytes(&self) -> usize {
+        self.claimed_bytes - self.available_bytes - self.allocated_bytes
+    }
+
+    /// Returns the total number of allocated bytes freed.
+    pub const fn total_freed_bytes(&self) -> u64 {
+        self.total_allocated_bytes - self.allocated_bytes as u64
+    }
+
+    /// Returns the total number of claimed bytes released.
+    pub const fn total_released_bytes(&self) -> u64 {
+        self.total_claimed_bytes - self.claimed_bytes as u64
+    }
+
+    pub(crate) fn account_register_gap(&mut self, size: usize) {
+        self.available_bytes += size;
+        self.fragment_count += 1;
+    }
+    pub(crate) fn account_deregister_gap(&mut self, size: usize) {
+        self.available_bytes -= size;
+        self.fragment_count -= 1;
+    }
+
+    pub(crate) fn account_alloc(&mut self, alloc_size: usize) {
+        self.allocation_count += 1;
+        self.allocated_bytes += alloc_size;
+
+        self.total_allocation_count += 1;
+        self.total_allocated_bytes += alloc_size as u64;
+    }
+
+    pub(crate) fn account_dealloc(&mut self, alloc_size: usize) {
+        self.allocation_count -= 1;
+        self.allocated_bytes -= alloc_size;
+    }
+
+    pub(crate) fn account_grow_in_place(&mut self, old_alloc_size: usize, new_alloc_size: usize) {
+        self.allocated_bytes += new_alloc_size - old_alloc_size;
+        self.total_allocated_bytes += (new_alloc_size - old_alloc_size) as u64;
+    }
+
+    pub(crate) fn account_shrink_in_place(&mut self, old_alloc_size: usize, new_alloc_size: usize) {
+        self.allocated_bytes -= old_alloc_size - new_alloc_size;
+        self.total_allocated_bytes -= (old_alloc_size - new_alloc_size) as u64;
+    }
+
+    pub(crate) fn account_claim(&mut self, claimed_size: usize) {
+        self.heap_count += 1;
+        self.claimed_bytes += claimed_size;
+
+        self.total_heap_count += 1;
+        self.total_claimed_bytes += claimed_size as u64;
+    }
+
+    pub(crate) fn account_extend(&mut self, old_claimed_size: usize, new_claimed_size: usize) {
+        self.claimed_bytes += new_claimed_size - old_claimed_size;
+        self.total_claimed_bytes += (new_claimed_size - old_claimed_size) as u64;
+    }
+
+    pub(crate) fn account_truncate(&mut self, old_claimed_size: usize, new_claimed_size: usize) {
+        if old_claimed_size != 0 && new_claimed_size == 0 {
+            self.heap_count -= 1;
+        }
+
+        self.claimed_bytes -= old_claimed_size - new_claimed_size;
+    }
+}
+
+impl<O: super::OomHandler> super::Talc<O> {
+    pub fn get_counters(&self) -> &Counters {
+        &self.counters
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use core::alloc::Layout;
+
+    use ptr_utils::{WORD_BITS, WORD_SIZE};
+
+    use crate::{*, talc::TAG_SIZE};
+
+    #[test]
+    fn test_claim_alloc_free_truncate() {
+        let mut arena = [0u8; 1000000];
+
+        let mut talc = Talc::new(ErrOnOom);
+
+        let low = 99;
+        let high = 10001;
+        let heap1 = unsafe {
+            talc.claim(arena.get_mut(low..high).unwrap().into()).unwrap()
+        };
+
+        let pre_alloc_claimed_bytes = talc.get_counters().claimed_bytes;
+        assert!(talc.get_counters().claimed_bytes == heap1.size());
+        assert!(talc.get_counters().claimed_bytes <= high - low);
+        assert!(talc.get_counters().claimed_bytes >= high - low - 16);
+        assert!(talc.get_counters().claimed_bytes == talc.get_counters().total_claimed_bytes as _);
+
+        let pre_alloc_avl_bytes = talc.get_counters().available_bytes;
+        dbg!(pre_alloc_avl_bytes);
+        assert!(talc.get_counters().available_bytes < high - low - WORD_SIZE * WORD_BITS * 2);
+        assert!(talc.get_counters().available_bytes >= high - low - WORD_SIZE * WORD_BITS * 2 - 64);
+
+        assert!(talc.get_counters().allocated_bytes == 0);
+        assert!(talc.get_counters().total_allocated_bytes == 0);
+
+        assert!(talc.get_counters().allocation_count == 0);
+        assert!(talc.get_counters().total_allocation_count == 0);
+        assert!(talc.get_counters().fragment_count == 1);
+        assert!(talc.get_counters().overhead_bytes() >= TAG_SIZE + WORD_SIZE * WORD_BITS * 2);
+        assert!(talc.get_counters().overhead_bytes() <= TAG_SIZE + WORD_SIZE * WORD_BITS * 2 + 64);
+
+        let alloc_layout = Layout::new::<[u128; 3]>();
+        let alloc = unsafe {
+            talc.malloc(alloc_layout).unwrap()
+        };
+
+        assert!(talc.get_counters().claimed_bytes == pre_alloc_claimed_bytes);
+        assert!(talc.get_counters().available_bytes < pre_alloc_avl_bytes - alloc_layout.size());
+        assert!(talc.get_counters().available_bytes < pre_alloc_avl_bytes - alloc_layout.size());
+        assert!(talc.get_counters().allocated_bytes == alloc_layout.size());
+        assert!(talc.get_counters().total_allocated_bytes == alloc_layout.size() as _);
+        assert!(talc.get_counters().allocation_count == 1);
+        assert!(talc.get_counters().total_allocation_count == 1);
+        dbg!(talc.get_counters().fragment_count);
+        assert!(matches!(talc.get_counters().fragment_count, 1..=2));
+
+        assert!(talc.get_counters().overhead_bytes() >= 2 * TAG_SIZE);
+        
+        unsafe {
+            talc.free(alloc, alloc_layout);
+        }
+
+        assert!(talc.get_counters().claimed_bytes == pre_alloc_claimed_bytes);
+        assert!(talc.get_counters().total_claimed_bytes == pre_alloc_claimed_bytes as _);
+        assert!(talc.get_counters().available_bytes == pre_alloc_avl_bytes);
+        assert!(talc.get_counters().allocated_bytes == 0);
+        assert!(talc.get_counters().total_allocated_bytes == alloc_layout.size() as _);
+        assert!(talc.get_counters().allocation_count == 0);
+        assert!(talc.get_counters().total_allocation_count == 1);
+        assert!(talc.get_counters().fragment_count == 1);
+
+        let heap1 = unsafe {
+            talc.truncate(heap1, talc.get_allocated_span(heap1))
+        };
+
+        assert!(heap1.size() <= TAG_SIZE + WORD_SIZE * WORD_BITS * 2 + 64);
+
+        assert!(talc.get_counters().claimed_bytes == heap1.size());
+        assert!(talc.get_counters().overhead_bytes() == talc.get_counters().claimed_bytes);
+        assert!(talc.get_counters().total_claimed_bytes == pre_alloc_claimed_bytes as _);
+        assert!(talc.get_counters().available_bytes == 0);
+        assert!(talc.get_counters().allocated_bytes == 0);
+        assert!(talc.get_counters().total_allocated_bytes == alloc_layout.size() as _);
+        assert!(talc.get_counters().allocation_count == 0);
+        assert!(talc.get_counters().total_allocation_count == 1);
+        assert!(talc.get_counters().fragment_count == 0);
+    }
+}
diff --git a/crates/talc/src/talc/llist.rs b/crates/talc/src/talc/llist.rs
new file mode 100644
index 0000000..42f2c2e
--- /dev/null
+++ b/crates/talc/src/talc/llist.rs
@@ -0,0 +1,156 @@
+use core::ptr::NonNull;
+
+/// Describes a linked list node.
+///
+/// # Safety:
+/// `LlistNode`s are inherently unsafe due to the referencial dependency between nodes. This requires
+/// that `LlistNode`s are never moved manually, otherwise using the list becomes memory
+/// unsafe and may lead to undefined behaviour.
+///
+/// This data structure is not thread-safe, use mutexes/locks to mutually exclude data access.
+#[derive(Debug)]
+#[repr(C)]
+pub struct LlistNode {
+    pub next: Option<NonNull<LlistNode>>,
+    pub next_of_prev: *mut Option<NonNull<LlistNode>>,
+}
+
+impl LlistNode {
+    #[inline]
+    pub fn next_ptr(ptr: *mut Self) -> *mut Option<NonNull<LlistNode>> {
+        ptr.cast() /* .cast::<u8>().wrapping_add(core::mem::offset_of!(LlistNode, next)) */
+    }
+
+    /// Create a new node as a member of an existing linked list at `node`.
+    ///
+    /// Warning: This will not call `remove` on `node`, regardless of initialization.
+    /// It is your responsibility to make sure `node` gets `remove`d if necessary.
+    /// Failing to do so when is not undefined behaviour or memory unsafe, but
+    /// may cause unexpected linkages.
+    ///
+    /// # Safety
+    /// * `node` must be `ptr::write`-able.
+    /// * `next_of_prev` must be dereferencable and valid.
+    pub unsafe fn insert(
+        node: *mut Self,
+        next_of_prev: *mut Option<NonNull<LlistNode>>,
+        next: Option<NonNull<LlistNode>>,
+    ) {
+        debug_assert!(!node.is_null());
+        debug_assert!(!next_of_prev.is_null());
+
+        node.write(Self { next_of_prev, next });
+
+        *next_of_prev = Some(NonNull::new_unchecked(node));
+
+        if let Some(next) = next {
+            (*next.as_ptr()).next_of_prev = Self::next_ptr(node);
+        }
+    }
+
+    /// Remove `node` from it's linked list.
+    ///
+    /// Note that this does not modify `node`; it should be considered invalid.
+    ///
+    /// # Safety
+    /// * `self` must be dereferencable and valid.
+    pub unsafe fn remove(node: *mut Self) {
+        debug_assert!(!node.is_null());
+        let LlistNode { next, next_of_prev } = node.read();
+
+        debug_assert!(!next_of_prev.is_null());
+        *next_of_prev = next;
+
+        if let Some(next) = next {
+            (*next.as_ptr()).next_of_prev = next_of_prev;
+        }
+    }
+
+    /// Creates an iterator over the circular linked list, exclusive of
+    /// the sentinel.
+    /// # Safety
+    /// `start`'s linked list must remain in a valid state during iteration.
+    /// Modifying `LlistNode`s already returned by the iterator is okay.
+    pub unsafe fn iter_mut(first: Option<NonNull<Self>>) -> IterMut {
+        IterMut::new(first)
+    }
+}
+
+/// An iterator over the circular linked list `LlistNode`s, excluding the 'head'.
+///
+/// This `struct` is created by `LlistNode::iter_mut`. See its documentation for more.
+#[derive(Debug, Clone, Copy)]
+#[must_use = "iterators are lazy and do nothing unless consumed"]
+pub struct IterMut(Option<NonNull<LlistNode>>);
+
+impl IterMut {
+    /// Create a new iterator over the linked list from `first`.
+    pub unsafe fn new(first: Option<NonNull<LlistNode>>) -> Self {
+        Self(first)
+    }
+}
+
+impl Iterator for IterMut {
+    type Item = NonNull<LlistNode>;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        let current = self.0?;
+        self.0 = unsafe { (*current.as_ptr()).next };
+        Some(current)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::ptr::null_mut;
+
+    use super::*;
+
+    #[test]
+    fn test_llist() {
+        unsafe {
+            let x = Box::into_raw(Box::new(LlistNode { next: None, next_of_prev: null_mut() }));
+            let y = Box::into_raw(Box::new(LlistNode { next: None, next_of_prev: null_mut() }));
+            let z = Box::into_raw(Box::new(LlistNode { next: None, next_of_prev: null_mut() }));
+
+            LlistNode::insert(y, LlistNode::next_ptr(x), None);
+            LlistNode::insert(z, LlistNode::next_ptr(x), Some(NonNull::new(y).unwrap()));
+
+            let mut iter = LlistNode::iter_mut(Some(NonNull::new(x)).unwrap());
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == x));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == z));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == y));
+            assert!(iter.next().is_none());
+
+            let mut iter = LlistNode::iter_mut(Some(NonNull::new(y).unwrap()));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == y));
+            assert!(iter.next().is_none());
+
+            LlistNode::remove(z);
+
+            let mut iter = LlistNode::iter_mut(Some(NonNull::new(x).unwrap()));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == x));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == y));
+            assert!(iter.next().is_none());
+
+            LlistNode::insert(z, LlistNode::next_ptr(x), Some(NonNull::new(y).unwrap()));
+
+            let mut iter = LlistNode::iter_mut(Some(NonNull::new(x).unwrap()));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == x));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == z));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == y));
+            assert!(iter.next().is_none());
+
+            LlistNode::remove(z);
+            LlistNode::remove(y);
+
+            let mut iter = LlistNode::iter_mut(Some(NonNull::new(x).unwrap()));
+            assert!(iter.next().is_some_and(|n| n.as_ptr() == x));
+            assert!(iter.next().is_none());
+
+            drop(Box::from_raw(x));
+            drop(Box::from_raw(y));
+            drop(Box::from_raw(z));
+        }
+    }
+}
diff --git a/crates/talc/src/talc/tag.rs b/crates/talc/src/talc/tag.rs
new file mode 100644
index 0000000..501febc
--- /dev/null
+++ b/crates/talc/src/talc/tag.rs
@@ -0,0 +1,68 @@
+//! A `Tag` is a size with flags in the least significant
+//! bits and most significant bit for allocated chunks.
+
+// const UNUSED_BITS: usize = 2; //crate::ALIGN.ilog2();
+// on 64 bit machines we have unused 3 bits to work with but
+// let's keep it more portable for now.
+
+use crate::ptr_utils::ALIGN;
+
+/// Tag for allocated chunk metadata.
+#[derive(Clone, Copy, PartialEq, Eq)]
+#[repr(transparent)]
+pub struct Tag(pub *mut u8);
+
+impl core::fmt::Debug for Tag {
+    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
+        f.debug_struct("Tag")
+            .field("is_allocated", &self.is_allocated())
+            .field("is_above_free", &self.is_above_free())
+            .field("base_ptr", &format_args!("{:p}", self.chunk_base()))
+            .finish()
+    }
+}
+
+impl Tag {
+    pub const ALLOCATED_FLAG: usize = 1 << 0; // pointers are always aligned to 4 bytes at least
+    pub const IS_ABOVE_FREE_FLAG: usize = 1 << 1; // pointers are always aligned to 4 bytes at least
+
+    const BASE: usize = !(Self::ALLOCATED_FLAG | Self::IS_ABOVE_FREE_FLAG);
+
+    pub unsafe fn write(chunk_tag: *mut Tag, chunk_base: *mut u8, is_above_free: bool) {
+        debug_assert!(chunk_base as usize & !Self::BASE == 0);
+
+        chunk_tag.write(if is_above_free {
+            Self(chunk_base.wrapping_add(Self::IS_ABOVE_FREE_FLAG | Self::ALLOCATED_FLAG))
+        } else {
+            Self(chunk_base.wrapping_add(Self::ALLOCATED_FLAG))
+        })
+    }
+
+    pub fn chunk_base(self) -> *mut u8 {
+        self.0.wrapping_sub(self.0 as usize % ALIGN)
+    }
+
+    pub fn is_above_free(self) -> bool {
+        self.0 as usize & Self::IS_ABOVE_FREE_FLAG != 0
+    }
+
+    pub fn is_allocated(self) -> bool {
+        self.0 as usize & Self::ALLOCATED_FLAG != 0
+    }
+
+    pub unsafe fn set_above_free(ptr: *mut Self) {
+        let mut tag = ptr.read();
+        debug_assert!(!tag.is_above_free());
+        tag = Self(tag.0.wrapping_add(Self::IS_ABOVE_FREE_FLAG));
+        debug_assert!(tag.is_above_free());
+        ptr.write(tag);
+    }
+
+    pub unsafe fn clear_above_free(ptr: *mut Self) {
+        let mut tag = ptr.read();
+        debug_assert!(tag.is_above_free());
+        tag = Self(tag.0.wrapping_sub(Self::IS_ABOVE_FREE_FLAG));
+        debug_assert!(!tag.is_above_free());
+        ptr.write(tag);
+    }
+}
diff --git a/crates/talc/src/talck.rs b/crates/talc/src/talck.rs
new file mode 100644
index 0000000..196fbf4
--- /dev/null
+++ b/crates/talc/src/talck.rs
@@ -0,0 +1,257 @@
+//! Home of Talck, a mutex-locked wrapper of Talc.
+
+use crate::{talc::Talc, OomHandler};
+
+use core::{
+    alloc::{GlobalAlloc, Layout},
+    cmp::Ordering,
+    ptr::{NonNull, null_mut},
+};
+
+#[cfg(feature = "allocator")]
+use core::alloc::AllocError;
+
+#[cfg(feature = "allocator")]
+pub(crate) fn is_aligned_to(ptr: *mut u8, align: usize) -> bool {
+    (ptr as usize).trailing_zeros() >= align.trailing_zeros()
+}
+
+const RELEASE_LOCK_ON_REALLOC_LIMIT: usize = 0x10000;
+
+/// Talc lock, contains a mutex-locked [`Talc`].
+///
+/// # Example
+/// ```rust
+/// # use talc::*;
+/// let talc = Talc::new(ErrOnOom);
+/// let talck = talc.lock::<spin::Mutex<()>>();
+/// ```
+#[derive(Debug)]
+pub struct Talck<R: lock_api::RawMutex, O: OomHandler> {
+    mutex: lock_api::Mutex<R, Talc<O>>
+}
+
+impl<R: lock_api::RawMutex, O: OomHandler> Talck<R, O> {
+    /// Create a new `Talck`.
+    pub const fn new(talc: Talc<O>) -> Self {
+        Self {
+            mutex: lock_api::Mutex::new(talc),
+        }
+    }
+
+    /// Lock the mutex and access the inner `Talc`.
+    pub fn lock(&self) -> lock_api::MutexGuard<R, Talc<O>> {
+        self.mutex.lock()
+    }
+
+    /// Try to lock the mutex and access the inner `Talc`.
+    pub fn try_lock(&self) -> Option<lock_api::MutexGuard<R, Talc<O>>> {
+        self.mutex.try_lock()
+    }
+
+    /// Retrieve the inner `Talc`.
+    pub fn into_inner(self) -> Talc<O> {
+        self.mutex.into_inner()
+    }
+}
+
+unsafe impl<R: lock_api::RawMutex, O: OomHandler> GlobalAlloc for Talck<R, O> {
+    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
+        self.lock().malloc(layout).map_or(null_mut(), |nn| nn.as_ptr())
+    }
+
+    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
+        self.lock().free(NonNull::new_unchecked(ptr), layout)
+    }
+
+    unsafe fn realloc(&self, ptr: *mut u8, old_layout: Layout, new_size: usize) -> *mut u8 {
+        let nn_ptr = NonNull::new_unchecked(ptr);
+
+        match new_size.cmp(&old_layout.size()) {
+            Ordering::Greater => {
+                // first try to grow in-place before manually re-allocating
+
+                if let Ok(nn) = self.lock().grow_in_place(nn_ptr, old_layout, new_size) {
+                    return nn.as_ptr();
+                }
+
+                // grow in-place failed, reallocate manually
+
+                let new_layout = Layout::from_size_align_unchecked(new_size, old_layout.align());
+
+                let mut lock = self.lock();
+                let allocation = match lock.malloc(new_layout) {
+                    Ok(ptr) => ptr,
+                    Err(_) => return null_mut(),
+                };
+                
+                if old_layout.size() > RELEASE_LOCK_ON_REALLOC_LIMIT {
+                    drop(lock);
+                    allocation.as_ptr().copy_from_nonoverlapping(ptr, old_layout.size());
+                    lock = self.lock();
+                } else {
+                    allocation.as_ptr().copy_from_nonoverlapping(ptr, old_layout.size());
+                }
+
+                lock.free(nn_ptr, old_layout);
+                allocation.as_ptr()
+            }
+
+            Ordering::Less => {
+                self.lock().shrink(NonNull::new_unchecked(ptr), old_layout, new_size);
+                ptr
+            }
+
+            Ordering::Equal => ptr,
+        }
+    }
+}
+
+#[cfg(feature = "allocator")]
+unsafe impl<R: lock_api::RawMutex, O: OomHandler> core::alloc::Allocator for Talck<R, O> {
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, core::alloc::AllocError> {
+        if layout.size() == 0 {
+            return Ok(NonNull::slice_from_raw_parts(NonNull::dangling(), 0));
+        }
+
+        unsafe { self.lock().malloc(layout) }
+            .map(|nn| NonNull::slice_from_raw_parts(nn, layout.size()))
+            .map_err(|_| AllocError)
+    }
+
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
+        if layout.size() != 0 {
+            self.lock().free(ptr, layout);
+        }
+    }
+
+    unsafe fn grow(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, core::alloc::AllocError> {
+        debug_assert!(new_layout.size() >= old_layout.size());
+
+        if old_layout.size() == 0 {
+            return self.allocate(new_layout);
+        } else if is_aligned_to(ptr.as_ptr(), new_layout.align()) {
+            // alignment is fine, try to allocate in-place
+            if let Ok(nn) = self.lock().grow_in_place(ptr, old_layout, new_layout.size()) {
+                return Ok(NonNull::slice_from_raw_parts(nn, new_layout.size()));
+            }
+        }
+
+        // can't grow in place, reallocate manually
+
+        let mut lock = self.lock();
+        let allocation = lock.malloc(new_layout).map_err(|_| AllocError)?;
+
+        if old_layout.size() > RELEASE_LOCK_ON_REALLOC_LIMIT {
+            drop(lock);
+            allocation.as_ptr().copy_from_nonoverlapping(ptr.as_ptr(), old_layout.size());
+            lock = self.lock();
+        } else {
+            allocation.as_ptr().copy_from_nonoverlapping(ptr.as_ptr(), old_layout.size());
+        }
+
+        lock.free(ptr, old_layout);
+
+        Ok(NonNull::slice_from_raw_parts(allocation, new_layout.size()))
+    }
+
+    unsafe fn grow_zeroed(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, core::alloc::AllocError> {
+        let res = self.grow(ptr, old_layout, new_layout);
+
+        if let Ok(allocation) = res {
+            allocation
+                .as_ptr()
+                .cast::<u8>()
+                .add(old_layout.size())
+                .write_bytes(0, new_layout.size() - old_layout.size());
+        }
+
+        res
+    }
+
+    unsafe fn shrink(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, core::alloc::AllocError> {
+        debug_assert!(new_layout.size() <= old_layout.size());
+
+        if new_layout.size() == 0 {
+            if old_layout.size() > 0 {
+                self.lock().free(ptr, old_layout);
+            }
+
+            return Ok(NonNull::slice_from_raw_parts(NonNull::dangling(), 0));
+        }
+
+        if !is_aligned_to(ptr.as_ptr(), new_layout.align()) {
+            let mut lock = self.lock();
+            let allocation = lock.malloc(new_layout).map_err(|_| AllocError)?;
+
+            if new_layout.size() > RELEASE_LOCK_ON_REALLOC_LIMIT {
+                drop(lock);
+                allocation.as_ptr().copy_from_nonoverlapping(ptr.as_ptr(), new_layout.size());
+                lock = self.lock();
+            } else {
+                allocation.as_ptr().copy_from_nonoverlapping(ptr.as_ptr(), new_layout.size());
+            }
+
+            lock.free(ptr, old_layout);
+            return Ok(NonNull::slice_from_raw_parts(allocation, new_layout.size()));
+        }
+
+        self.lock().shrink(ptr, old_layout, new_layout.size());
+
+        Ok(NonNull::slice_from_raw_parts(ptr, new_layout.size()))
+    }
+}
+
+impl<O: OomHandler> Talc<O> {
+    /// Wrap in `Talck`, a mutex-locked wrapper struct using [`lock_api`].
+    ///
+    /// This implements the [`GlobalAlloc`](core::alloc::GlobalAlloc) trait and provides
+    /// access to the [`Allocator`](core::alloc::Allocator) API.
+    ///
+    /// # Examples
+    /// ```
+    /// # use talc::*;
+    /// # use core::alloc::{GlobalAlloc, Layout};
+    /// use spin::Mutex;
+    /// let talc = Talc::new(ErrOnOom);
+    /// let talck = talc.lock::<Mutex<()>>();
+    ///
+    /// unsafe {
+    ///     talck.alloc(Layout::from_size_align_unchecked(32, 4));
+    /// }
+    /// ```
+    pub const fn lock<R: lock_api::RawMutex>(self) -> Talck<R, O> {
+        Talck::new(self)
+    }
+}
+
+#[cfg(all(target_family = "wasm"))]
+impl TalckWasm {
+    /// Create a [`Talck`] instance that takes control of WASM memory management.
+    ///
+    /// # Safety
+    /// The runtime environment must be single-threaded WASM.
+    ///
+    /// Note: calls to memory.grow during use of the allocator is allowed.
+    pub const unsafe fn new_global() -> Self {
+        Talc::new(crate::WasmHandler::new()).lock()
+    }
+}
+
+#[cfg(all(target_family = "wasm"))]
+pub type TalckWasm = Talck<crate::locking::AssumeUnlockable, crate::WasmHandler>;
diff --git a/crates/talc/wasm-perf.sh b/crates/talc/wasm-perf.sh
new file mode 100755
index 0000000..5c696b6
--- /dev/null
+++ b/crates/talc/wasm-perf.sh
@@ -0,0 +1,16 @@
+#!/bin/bash
+
+# This script runs a benchmark on global alloctors for WASM.
+# requires wasm-pack and deno
+
+cd wasm-perf
+
+ALLOCATORS="talc talc_arena dlmalloc lol_alloc"
+for ALLOCATOR in ${ALLOCATORS}; do
+    echo "${ALLOCATOR}"
+    wasm-pack --log-level warn build --release --quiet --target web --features ${ALLOCATOR}
+
+    if [[ $1 != "check" ]]; then
+        deno run --allow-read bench.js
+    fi
+done
diff --git a/crates/talc/wasm-perf/Cargo.toml b/crates/talc/wasm-perf/Cargo.toml
new file mode 100644
index 0000000..7d0ddf6
--- /dev/null
+++ b/crates/talc/wasm-perf/Cargo.toml
@@ -0,0 +1,34 @@
+[package]
+name = "wasm-perf"
+rust-version = "1.67.1"
+version = "0.0.0"
+authors = ["Shaun Beautement <sf.beautement@protonmail.com>"]
+edition = "2021"
+
+[lib]
+crate-type = ["cdylib", "rlib"]
+
+[dependencies]
+console_error_panic_hook = "0.1.7"
+wasm-bindgen = "0.2.84"
+fastrand = "2.0.0"
+web-sys = { version = "0.3.67", features = ["Window", "Performance"] }
+
+[dependencies.lol_alloc]
+version = "0.4.0"
+optional = true
+
+[dependencies.talc]
+path = ".."
+default-features = false
+features = ["lock_api"]
+optional = true
+
+[features]
+talc_arena = ["talc"]
+dlmalloc = [] # dummy feature for wasm-bench.sh
+
+# be realistic about the optimization configuration, even if it's a benchmark
+[profile.release]
+opt-level = "z"
+lto = "fat"
diff --git a/crates/talc/wasm-perf/bench.js b/crates/talc/wasm-perf/bench.js
new file mode 100644
index 0000000..463a21f
--- /dev/null
+++ b/crates/talc/wasm-perf/bench.js
@@ -0,0 +1,3 @@
+import init, {bench} from "./pkg/wasm_perf.js";
+await init(Deno.readFile('./pkg/wasm_perf_bg.wasm'));
+bench();
diff --git a/crates/talc/wasm-perf/src/lib.rs b/crates/talc/wasm-perf/src/lib.rs
new file mode 100644
index 0000000..62b6702
--- /dev/null
+++ b/crates/talc/wasm-perf/src/lib.rs
@@ -0,0 +1,114 @@
+use std::alloc::Layout;
+
+use wasm_bindgen::prelude::*;
+
+
+#[cfg(all(feature = "talc", not(feature = "talc_arena")))]
+#[global_allocator]
+static TALCK: talc::TalckWasm = unsafe { talc::TalckWasm::new_global() };
+
+
+#[cfg(feature = "talc_arena")]
+#[global_allocator]
+static ALLOCATOR: talc::Talck<talc::locking::AssumeUnlockable, talc::ClaimOnOom> = {
+    static mut MEMORY: [std::mem::MaybeUninit<u8>; 32 * 1024 * 1024]
+        = [std::mem::MaybeUninit::uninit(); 32 * 1024 * 1024];
+    let span = talc::Span::from_const_array(unsafe { std::ptr::addr_of!(MEMORY) });
+    talc::Talc::new(unsafe { talc::ClaimOnOom::new(span) }).lock()
+};
+
+#[cfg(feature = "lol_alloc")]
+#[global_allocator] static ALLOC: lol_alloc::AssumeSingleThreaded<lol_alloc::FreeListAllocator> = 
+    unsafe { lol_alloc::AssumeSingleThreaded::new(lol_alloc::FreeListAllocator::new()) };
+
+
+#[wasm_bindgen]
+extern "C" {
+    #[wasm_bindgen(js_namespace = console)]
+    fn log(s: &str);
+}
+
+const ACTIONS: usize = 100000;
+const ITERATIONS: usize = 100;
+
+#[wasm_bindgen]
+pub fn bench() {
+    console_error_panic_hook::set_once();
+
+    let timer = web_sys::window().unwrap().performance().unwrap();
+
+    // warm up
+    random_actions();
+
+    // go!
+    let start = timer.now();
+    for _ in 0..ITERATIONS { random_actions(); }
+    let end = timer.now();
+
+    // log durations
+    let total_ms = end - start;
+    let average_ms = total_ms / ITERATIONS as f64;
+    let apms = ACTIONS as f64 / average_ms / 1000.0;
+    log(format!("  total time: {} ms", total_ms).as_str());
+    log(format!("  average time: {} ms", average_ms).as_str());
+    log(format!("  average actions/s: {:.1}", apms).as_str());
+}
+
+fn random_actions() {
+    let mut score = 0;
+    let mut v = Vec::with_capacity(10000);
+
+    while score < 100000 {
+        let action = fastrand::usize(0..3);
+
+        match action {
+            0 => {
+                let size = fastrand::usize(100..=1000);
+                let align = 8 << fastrand::u16(..).trailing_zeros() / 2;
+                let layout = Layout::from_size_align(size, align).unwrap();
+
+                let allocation = unsafe { std::alloc::alloc(layout) };
+
+                if !allocation.is_null() {
+                    v.push((allocation, layout));
+                    score += 1;
+                }
+            }
+            1 => {
+                if !v.is_empty() {
+                    let index = fastrand::usize(0..v.len());
+                    let (ptr, layout) = v.swap_remove(index);
+
+                    unsafe {
+                        std::alloc::dealloc(ptr, layout);
+                    }
+
+                    score += 1;
+                }
+            }
+            2 => {
+                if !v.is_empty() {
+                    let index = fastrand::usize(0..v.len());
+                    if let Some((ptr, layout)) = v.get_mut(index) {
+                        let new_size = fastrand::usize(100..=10000);
+
+                        unsafe {
+                            let realloc = std::alloc::realloc(*ptr, *layout, new_size);
+
+                            if !realloc.is_null() {
+                                *ptr = realloc;
+                                *layout = Layout::from_size_align_unchecked(new_size, layout.align());
+                                score += 1;
+                            }
+                        }
+                    }
+                }
+            }
+            _ => unreachable!(),
+        }
+    }
+
+    for (ptr, layout) in v {
+        unsafe { std::alloc::dealloc(ptr, layout); }
+    }
+}
diff --git a/crates/talc/wasm-size.sh b/crates/talc/wasm-size.sh
new file mode 100755
index 0000000..a490917
--- /dev/null
+++ b/crates/talc/wasm-size.sh
@@ -0,0 +1,25 @@
+#!/bin/bash
+
+# This script calculates a weight heurisitic for WASM allocators.
+
+
+COMMAND=""
+if [[ $1 == "check" ]]; then
+    COMMAND="check"
+else
+    COMMAND="build"
+fi
+
+cd wasm-size
+
+ALLOCATORS="talc talc_arena dlmalloc lol_alloc"
+for ALLOCATOR in ${ALLOCATORS}; do
+    echo "${ALLOCATOR}"
+    cargo $COMMAND --quiet --release --target wasm32-unknown-unknown --features ${ALLOCATOR}
+
+    if [[ $1 != "check" ]]; then
+        wasm-opt -Oz -o target/wasm32-unknown-unknown/release/wasm_size_opt.wasm target/wasm32-unknown-unknown/release/wasm_size.wasm
+        echo -n "  "
+        wc -c ./target/wasm32-unknown-unknown/release/wasm_size_opt.wasm
+    fi
+done
diff --git a/crates/talc/wasm-size/Cargo.toml b/crates/talc/wasm-size/Cargo.toml
new file mode 100644
index 0000000..97ee41c
--- /dev/null
+++ b/crates/talc/wasm-size/Cargo.toml
@@ -0,0 +1,25 @@
+[package]
+name = "wasm-size"
+rust-version = "1.68"
+version = "0.0.0"
+edition = "2021"
+
+[lib]
+crate-type = ["cdylib"]
+
+[dependencies]
+lol_alloc = { version = "0.4.0", optional = true }
+dlmalloc = { version = "0.2.4", features = ["global"], optional = true }
+
+[dependencies.talc]
+path = ".."
+default-features = false
+features = ["lock_api"]
+optional = true
+
+[features]
+talc_arena = ["talc"]
+
+[profile.release]
+opt-level = "z"
+lto = "fat"
diff --git a/crates/talc/wasm-size/src/lib.rs b/crates/talc/wasm-size/src/lib.rs
new file mode 100644
index 0000000..a219e15
--- /dev/null
+++ b/crates/talc/wasm-size/src/lib.rs
@@ -0,0 +1,62 @@
+#![no_std]
+
+extern crate alloc;
+
+use core::alloc::{GlobalAlloc, Layout};
+
+#[cfg(not(target_family = "wasm"))]
+compile_error!("Requires targetting WASM");
+
+struct NoAlloc;
+unsafe impl GlobalAlloc for NoAlloc {
+    unsafe fn alloc(&self, _: Layout) -> *mut u8 { core::ptr::null_mut() }
+    unsafe fn dealloc(&self, _: *mut u8, _: Layout) { }
+}
+
+#[cfg(all(not(feature = "talc"), not(feature = "dlmalloc"), not(feature = "lol_alloc")))]
+#[global_allocator]
+static NOALLOC: NoAlloc = NoAlloc;
+
+#[cfg(all(feature = "talc", not(feature = "talc_arena")))]
+#[global_allocator]
+static TALC: talc::TalckWasm = unsafe { talc::TalckWasm::new_global() };
+
+#[cfg(all(feature = "talc", feature = "talc_arena"))]
+#[global_allocator]
+static ALLOCATOR: talc::Talck<talc::locking::AssumeUnlockable, talc::ClaimOnOom> = {
+    const MEMORY_SIZE: usize = 128 * 1024 * 1024;
+    static mut MEMORY: [core::mem::MaybeUninit<u8>; MEMORY_SIZE] =
+        [core::mem::MaybeUninit::uninit(); MEMORY_SIZE];
+    let span = talc::Span::from_base_size(unsafe { MEMORY.as_ptr() as *mut _ }, MEMORY_SIZE);
+    talc::Talc::new(unsafe { talc::ClaimOnOom::new(span) }).lock()
+};
+
+#[cfg(feature = "lol_alloc")]
+#[global_allocator] 
+static LOL_ALLOC: lol_alloc::AssumeSingleThreaded<lol_alloc::FreeListAllocator> = 
+    unsafe { lol_alloc::AssumeSingleThreaded::new(lol_alloc::FreeListAllocator::new()) };
+
+#[cfg(feature = "dlmalloc")]
+#[global_allocator]
+static DLMALLOC: dlmalloc::GlobalDlmalloc = dlmalloc::GlobalDlmalloc;
+
+// this is necessary, despite rust-analyzer's protests
+#[panic_handler]
+fn panic_handler(_: &core::panic::PanicInfo) -> ! {
+    loop { }
+}
+
+#[no_mangle]
+pub unsafe extern "C" fn alloc(size: usize) -> *mut u8 {
+    alloc::alloc::alloc(Layout::from_size_align_unchecked(size, 8))
+}
+
+#[no_mangle]
+pub unsafe extern "C" fn dealloc(ptr: *mut u8, size: usize) {
+    alloc::alloc::dealloc(ptr, Layout::from_size_align_unchecked(size, 8))
+}
+
+#[no_mangle]
+pub unsafe extern "C" fn realloc(ptr: *mut u8, old_size: usize, new_size: usize) -> *mut u8 {
+    alloc::alloc::realloc(ptr, Layout::from_size_align_unchecked(old_size, 8), new_size)
+}
diff --git a/modules/axalloc/Cargo.toml b/modules/axalloc/Cargo.toml
index 40eedf4..63472be 100644
--- a/modules/axalloc/Cargo.toml
+++ b/modules/axalloc/Cargo.toml
@@ -14,6 +14,7 @@ default = ["tlsf"]
 tlsf = ["allocator/tlsf"]
 slab = ["allocator/slab"]
 buddy = ["allocator/buddy"]
+new = ["allocator/new"]
 
 [dependencies]
 log = "0.4"
diff --git a/modules/axalloc/src/lib.rs b/modules/axalloc/src/lib.rs
index 096ce95..500aa0b 100644
--- a/modules/axalloc/src/lib.rs
+++ b/modules/axalloc/src/lib.rs
@@ -30,6 +30,8 @@ cfg_if::cfg_if! {
         use allocator::BuddyByteAllocator as DefaultByteAllocator;
     } else if #[cfg(feature = "tlsf")] {
         use allocator::TlsfByteAllocator as DefaultByteAllocator;
+    } else if #[cfg(feature = "new")] {
+        use allocator::YourNewAllocator as DefaultByteAllocator;
     }
 }
 
@@ -67,6 +69,10 @@ impl GlobalAllocator {
                 "buddy"
             } else if #[cfg(feature = "tlsf")] {
                 "TLSF"
+            } else if #[cfg(feature = "new")] {
+                "talc"
+            } else {
+                "TLSF"
             }
         }
     }
diff --git a/ulib/axstd/Cargo.toml b/ulib/axstd/Cargo.toml
index 83239c3..c164548 100644
--- a/ulib/axstd/Cargo.toml
+++ b/ulib/axstd/Cargo.toml
@@ -33,6 +33,7 @@ alloc = ["arceos_api/alloc", "axfeat/alloc", "axio/alloc"]
 alloc-tlsf = ["axfeat/alloc-tlsf"]
 alloc-slab = ["axfeat/alloc-slab"]
 alloc-buddy = ["axfeat/alloc-buddy"]
+alloc-new = ["axfeat/alloc-buddy"]
 paging = ["axfeat/paging"]
 tls = ["axfeat/tls"]
 
@@ -74,3 +75,4 @@ arceos_api = { path = "../../api/arceos_api" }
 axio = { path = "../../crates/axio" }
 axerrno = { path = "../../crates/axerrno" }
 spinlock = { path = "../../crates/spinlock" }
+hash32 = { path = "../../crates/hash32" }
diff --git a/ulib/axstd/src/lib.rs b/ulib/axstd/src/lib.rs
index d256cd5..997346f 100644
--- a/ulib/axstd/src/lib.rs
+++ b/ulib/axstd/src/lib.rs
@@ -49,13 +49,22 @@
 #![cfg_attr(all(not(test), not(doc)), no_std)]
 #![feature(doc_cfg)]
 #![feature(doc_auto_cfg)]
+// 作业里提到的宏
+// ulib/axstd/src/lib.rs
+#![feature(hashmap_internals)]
+#![feature(extend_one)]
+#![feature(hasher_prefixfree_extras)]
+#![feature(error_in_core)]
+#![feature(try_reserve_kind)]
+#![feature(thread_local)]
+#![feature(const_hash)]
 
-#[cfg(feature = "alloc")]
+// #[cfg(feature = "alloc")]
 extern crate alloc;
 
-#[cfg(feature = "alloc")]
+// #[cfg(feature = "alloc")]
 #[doc(no_inline)]
-pub use alloc::{boxed, collections, format, string, vec};
+pub use alloc::{boxed, format, string, vec};
 
 #[doc(no_inline)]
 pub use core::{arch, cell, cmp, hint, marker, mem, ops, ptr, slice, str};
@@ -75,3 +84,84 @@ pub mod time;
 pub mod fs;
 #[cfg(feature = "net")]
 pub mod net;
+
+pub mod collections {
+    use alloc::vec::Vec;
+    use core::{
+        hash::{self, Hasher},
+        mem,
+        ptr::eq,
+    };
+
+    pub struct HashMap<K, V> {
+        buckets: Vec<Vec<(K, V)>>,
+        count: usize,
+    }
+
+    impl<K: hash::Hash + Eq, V: Clone> HashMap<K, V> {
+        pub fn new() -> Self {
+            //初始为1
+            let mut buckets = Vec::new();
+            buckets.push(Vec::new());
+            Self { buckets, count: 0 }
+        }
+        pub fn resize(&mut self) {
+            //// TODO: 扩容
+            let mut new_buckets = Vec::<Vec<(K, V)>>::with_capacity(self.count * 2);
+            new_buckets.extend((0..self.count * 2).map(|_| Vec::new()));
+            for bucket in self.buckets.iter_mut() {
+                for (key, value) in bucket.drain(..) {
+                    use hash32::Murmur3Hasher;
+                    let mut fnv = Murmur3Hasher::default();
+                    key.hash(&mut fnv);
+                    let index = (fnv.finish() % new_buckets.len() as u64) as usize;
+                    new_buckets[index].push((key, value));
+                }
+            }
+            let _ = mem::replace(&mut self.buckets, new_buckets);
+        }
+        pub fn insert(&mut self, key: K, value: V) -> Option<V> {
+            if self.count >= self.buckets.len() {
+                self.resize();
+            }
+            use hash32::{FnvHasher, Hasher};
+            let mut fnv = FnvHasher::default();
+            key.hash(&mut fnv);
+            // println!(
+            //     "len: {}, capacity: {}",
+            //     self.buckets.len(),
+            //     self.buckets.capacity()
+            // );
+            let index = (fnv.finish32() % self.buckets.len() as u32) as usize;
+            let bucket = &mut self.buckets[index];
+            for (k, v) in bucket.iter_mut() {
+                if eq(k, &key) {
+                    return Some(mem::replace(v, value));
+                }
+            }
+            bucket.push((key, value));
+            self.count += 1;
+            None
+        }
+        pub fn iter(&self) -> HashMapIter<'_, K, V> {
+            let mut unvisited = Vec::with_capacity(self.count);
+            for bucket in self.buckets.iter() {
+                for item in bucket.iter() {
+                    unvisited.push(item);
+                }
+            }
+            HashMapIter { unvisited }
+        }
+    }
+
+    pub struct HashMapIter<'a, K, V> {
+        unvisited: Vec<&'a (K, V)>,
+    }
+
+    impl<'a, K, V> Iterator for HashMapIter<'a, K, V> {
+        type Item = &'a (K, V);
+        fn next(&mut self) -> Option<Self::Item> {
+            self.unvisited.pop()
+        }
+    }
+}
